/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/dataset.py:214: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'dataset/TUDataset/IMDB-BINARY/processed' first
  warnings.warn(
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:0', dataset='IMDB-BINARY', in_size=136, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=136, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=136, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=6, eta=0.5, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.008626 val_loss: 0.702914 val_acc: 0.550000 test_loss: 0.703783 test_acc: 0.510000
Epoch: 010 train_loss: 0.005817 val_loss: 0.648116 val_acc: 0.690000 test_loss: 0.656542 test_acc: 0.690000
Epoch: 020 train_loss: 0.005270 val_loss: 0.626219 val_acc: 0.740000 test_loss: 0.654583 test_acc: 0.700000
Epoch: 030 train_loss: 0.005173 val_loss: 0.614861 val_acc: 0.730000 test_loss: 0.642383 test_acc: 0.700000
Epoch: 040 train_loss: 0.005116 val_loss: 0.616426 val_acc: 0.730000 test_loss: 0.625361 test_acc: 0.710000
Epoch: 050 train_loss: 0.005030 val_loss: 0.613949 val_acc: 0.740000 test_loss: 0.661015 test_acc: 0.660000
Epoch: 060 train_loss: 0.004948 val_loss: 0.620227 val_acc: 0.740000 test_loss: 0.633255 test_acc: 0.670000
Epoch: 070 train_loss: 0.004847 val_loss: 0.622769 val_acc: 0.730000 test_loss: 0.656320 test_acc: 0.660000
Epoch: 080 train_loss: 0.004846 val_loss: 0.624800 val_acc: 0.710000 test_loss: 0.646150 test_acc: 0.680000
Epoch: 090 train_loss: 0.004626 val_loss: 0.658296 val_acc: 0.670000 test_loss: 0.677838 test_acc: 0.720000
Epoch: 100 train_loss: 0.004565 val_loss: 0.639807 val_acc: 0.730000 test_loss: 0.681233 test_acc: 0.700000
Epoch: 110 train_loss: 0.004634 val_loss: 0.653259 val_acc: 0.730000 test_loss: 0.692415 test_acc: 0.690000
Epoch: 120 train_loss: 0.004588 val_loss: 0.643688 val_acc: 0.700000 test_loss: 0.727462 test_acc: 0.650000
Epoch: 130 train_loss: 0.004502 val_loss: 0.627726 val_acc: 0.750000 test_loss: 0.734955 test_acc: 0.670000
Epoch: 140 train_loss: 0.005208 val_loss: 0.596988 val_acc: 0.750000 test_loss: 0.683324 test_acc: 0.690000
Epoch: 150 train_loss: 0.005030 val_loss: 0.621868 val_acc: 0.730000 test_loss: 0.658864 test_acc: 0.680000
Epoch: 160 train_loss: 0.004908 val_loss: 0.608694 val_acc: 0.750000 test_loss: 0.672299 test_acc: 0.710000
Epoch: 170 train_loss: 0.004654 val_loss: 0.628801 val_acc: 0.740000 test_loss: 0.706476 test_acc: 0.640000
Epoch: 180 train_loss: 0.004711 val_loss: 0.640819 val_acc: 0.730000 test_loss: 0.719919 test_acc: 0.680000
Epoch: 190 train_loss: 0.004713 val_loss: 0.610883 val_acc: 0.730000 test_loss: 0.666298 test_acc: 0.720000
Epoch: 200 train_loss: 0.004545 val_loss: 0.604307 val_acc: 0.750000 test_loss: 0.699430 test_acc: 0.680000
Epoch: 210 train_loss: 0.004807 val_loss: 0.617154 val_acc: 0.730000 test_loss: 0.702223 test_acc: 0.650000
Epoch: 220 train_loss: 0.004530 val_loss: 0.620533 val_acc: 0.760000 test_loss: 0.733351 test_acc: 0.660000
Epoch: 230 train_loss: 0.004603 val_loss: 0.604637 val_acc: 0.770000 test_loss: 0.717220 test_acc: 0.700000
Epoch: 240 train_loss: 0.004488 val_loss: 0.621943 val_acc: 0.750000 test_loss: 0.719525 test_acc: 0.660000
Epoch: 250 train_loss: 0.004361 val_loss: 0.627046 val_acc: 0.740000 test_loss: 0.734218 test_acc: 0.680000
Epoch: 260 train_loss: 0.004559 val_loss: 0.618792 val_acc: 0.760000 test_loss: 0.705051 test_acc: 0.710000
Epoch: 270 train_loss: 0.004400 val_loss: 0.642937 val_acc: 0.740000 test_loss: 0.724977 test_acc: 0.690000
Epoch: 280 train_loss: 0.004612 val_loss: 0.615706 val_acc: 0.750000 test_loss: 0.687569 test_acc: 0.700000
Epoch: 290 train_loss: 0.004518 val_loss: 0.635981 val_acc: 0.730000 test_loss: 0.692749 test_acc: 0.680000
Epoch: 300 train_loss: 0.004412 val_loss: 0.648920 val_acc: 0.710000 test_loss: 0.738997 test_acc: 0.680000
Epoch: 310 train_loss: 0.004372 val_loss: 0.648771 val_acc: 0.730000 test_loss: 0.723961 test_acc: 0.690000
Epoch: 320 train_loss: 0.004324 val_loss: 0.645693 val_acc: 0.730000 test_loss: 0.703275 test_acc: 0.670000
Epoch: 330 train_loss: 0.004574 val_loss: 0.635689 val_acc: 0.720000 test_loss: 0.714660 test_acc: 0.720000
Epoch: 340 train_loss: 0.004324 val_loss: 0.635242 val_acc: 0.740000 test_loss: 0.734775 test_acc: 0.690000
Epoch: 350 train_loss: 0.004351 val_loss: 0.638713 val_acc: 0.720000 test_loss: 0.720399 test_acc: 0.690000
Epoch: 360 train_loss: 0.004312 val_loss: 0.638801 val_acc: 0.730000 test_loss: 0.735511 test_acc: 0.690000
Epoch: 370 train_loss: 0.004352 val_loss: 0.648656 val_acc: 0.720000 test_loss: 0.730639 test_acc: 0.700000
Epoch: 380 train_loss: 0.004340 val_loss: 0.641894 val_acc: 0.730000 test_loss: 0.721926 test_acc: 0.690000
Epoch: 390 train_loss: 0.004423 val_loss: 0.654290 val_acc: 0.730000 test_loss: 0.734527 test_acc: 0.670000
Epoch: 400 train_loss: 0.004316 val_loss: 0.644994 val_acc: 0.730000 test_loss: 0.725753 test_acc: 0.690000
Epoch: 410 train_loss: 0.004339 val_loss: 0.653137 val_acc: 0.700000 test_loss: 0.715911 test_acc: 0.700000
Epoch: 420 train_loss: 0.004306 val_loss: 0.639168 val_acc: 0.720000 test_loss: 0.704991 test_acc: 0.700000
Epoch: 430 train_loss: 0.004297 val_loss: 0.644658 val_acc: 0.720000 test_loss: 0.728515 test_acc: 0.690000
Epoch: 440 train_loss: 0.004402 val_loss: 0.632379 val_acc: 0.740000 test_loss: 0.739892 test_acc: 0.680000
Epoch: 450 train_loss: 0.004696 val_loss: 0.628840 val_acc: 0.710000 test_loss: 0.731840 test_acc: 0.680000
Epoch: 460 train_loss: 0.004403 val_loss: 0.640388 val_acc: 0.730000 test_loss: 0.702731 test_acc: 0.680000
Epoch: 470 train_loss: 0.004372 val_loss: 0.627664 val_acc: 0.740000 test_loss: 0.728475 test_acc: 0.690000
Epoch: 480 train_loss: 0.004316 val_loss: 0.642896 val_acc: 0.730000 test_loss: 0.745386 test_acc: 0.690000
Epoch: 490 train_loss: 0.004311 val_loss: 0.644382 val_acc: 0.730000 test_loss: 0.726620 test_acc: 0.680000
Epoch: 500 train_loss: 0.004340 val_loss: 0.647973 val_acc: 0.730000 test_loss: 0.744203 test_acc: 0.690000
Epoch: 510 train_loss: 0.004298 val_loss: 0.646653 val_acc: 0.740000 test_loss: 0.743258 test_acc: 0.680000
Epoch: 520 train_loss: 0.004322 val_loss: 0.652946 val_acc: 0.720000 test_loss: 0.760745 test_acc: 0.690000
Epoch: 530 train_loss: 0.004311 val_loss: 0.643094 val_acc: 0.730000 test_loss: 0.715044 test_acc: 0.700000
Epoch: 540 train_loss: 0.004297 val_loss: 0.645651 val_acc: 0.700000 test_loss: 0.730463 test_acc: 0.700000
Epoch: 550 train_loss: 0.004630 val_loss: 0.614018 val_acc: 0.750000 test_loss: 0.676310 test_acc: 0.700000
Epoch: 560 train_loss: 0.005366 val_loss: 0.624322 val_acc: 0.730000 test_loss: 0.630178 test_acc: 0.710000
Epoch: 570 train_loss: 0.004799 val_loss: 0.615528 val_acc: 0.740000 test_loss: 0.660698 test_acc: 0.680000
Epoch: 580 train_loss: 0.004971 val_loss: 0.621612 val_acc: 0.750000 test_loss: 0.641749 test_acc: 0.720000
Epoch: 590 train_loss: 0.004561 val_loss: 0.612970 val_acc: 0.760000 test_loss: 0.707010 test_acc: 0.660000
Epoch: 600 train_loss: 0.004452 val_loss: 0.613163 val_acc: 0.770000 test_loss: 0.725651 test_acc: 0.690000
Epoch: 610 train_loss: 0.004601 val_loss: 0.614872 val_acc: 0.780000 test_loss: 0.710062 test_acc: 0.690000
Epoch: 620 train_loss: 0.004789 val_loss: 0.591907 val_acc: 0.780000 test_loss: 0.710592 test_acc: 0.650000
Epoch: 630 train_loss: 0.004607 val_loss: 0.636621 val_acc: 0.740000 test_loss: 0.701153 test_acc: 0.720000
Epoch: 640 train_loss: 0.004382 val_loss: 0.627823 val_acc: 0.740000 test_loss: 0.728647 test_acc: 0.690000
Epoch: 650 train_loss: 0.004331 val_loss: 0.618459 val_acc: 0.740000 test_loss: 0.718171 test_acc: 0.690000
Epoch: 660 train_loss: 0.004344 val_loss: 0.605238 val_acc: 0.780000 test_loss: 0.724528 test_acc: 0.680000
Epoch: 670 train_loss: 0.004577 val_loss: 0.648452 val_acc: 0.720000 test_loss: 0.697206 test_acc: 0.710000
Epoch: 680 train_loss: 0.004609 val_loss: 0.633406 val_acc: 0.740000 test_loss: 0.675615 test_acc: 0.670000
Epoch: 690 train_loss: 0.004386 val_loss: 0.632900 val_acc: 0.730000 test_loss: 0.714909 test_acc: 0.660000
Epoch: 700 train_loss: 0.004379 val_loss: 0.617201 val_acc: 0.760000 test_loss: 0.712957 test_acc: 0.690000
Epoch: 710 train_loss: 0.004445 val_loss: 0.604354 val_acc: 0.770000 test_loss: 0.744098 test_acc: 0.680000
Epoch: 720 train_loss: 0.004306 val_loss: 0.633335 val_acc: 0.750000 test_loss: 0.726707 test_acc: 0.680000
Epoch: 730 train_loss: 0.004292 val_loss: 0.634860 val_acc: 0.730000 test_loss: 0.749436 test_acc: 0.670000
Epoch: 740 train_loss: 0.004327 val_loss: 0.634844 val_acc: 0.740000 test_loss: 0.749604 test_acc: 0.680000
Epoch: 750 train_loss: 0.004322 val_loss: 0.633011 val_acc: 0.720000 test_loss: 0.733050 test_acc: 0.690000
Epoch: 760 train_loss: 0.004330 val_loss: 0.637596 val_acc: 0.720000 test_loss: 0.747462 test_acc: 0.670000
Epoch: 770 train_loss: 0.004319 val_loss: 0.629712 val_acc: 0.730000 test_loss: 0.740154 test_acc: 0.670000
Epoch: 780 train_loss: 0.004258 val_loss: 0.640011 val_acc: 0.720000 test_loss: 0.762985 test_acc: 0.670000
Epoch: 790 train_loss: 0.004274 val_loss: 0.630226 val_acc: 0.730000 test_loss: 0.741325 test_acc: 0.670000
------------- 0 val_acc: 0.8000 test_acc: 0.6800 best_test: 0.7500 -----------------
Fold: 1
Epoch: 000 train_loss: 0.006601 val_loss: 0.697822 val_acc: 0.430000 test_loss: 0.700707 test_acc: 0.390000
Epoch: 010 train_loss: 0.005401 val_loss: 0.643303 val_acc: 0.690000 test_loss: 0.661374 test_acc: 0.690000
Epoch: 020 train_loss: 0.005195 val_loss: 0.674905 val_acc: 0.660000 test_loss: 0.694416 test_acc: 0.710000
Epoch: 030 train_loss: 0.005096 val_loss: 0.648727 val_acc: 0.650000 test_loss: 0.637436 test_acc: 0.720000
Epoch: 040 train_loss: 0.005001 val_loss: 0.706436 val_acc: 0.670000 test_loss: 0.696546 test_acc: 0.680000
Epoch: 050 train_loss: 0.004883 val_loss: 0.684810 val_acc: 0.630000 test_loss: 0.659001 test_acc: 0.660000
Epoch: 060 train_loss: 0.004813 val_loss: 0.670848 val_acc: 0.640000 test_loss: 0.653805 test_acc: 0.670000
Epoch: 070 train_loss: 0.005050 val_loss: 0.684045 val_acc: 0.660000 test_loss: 0.655757 test_acc: 0.670000
Epoch: 080 train_loss: 0.004924 val_loss: 0.676000 val_acc: 0.680000 test_loss: 0.685992 test_acc: 0.710000
Epoch: 090 train_loss: 0.004738 val_loss: 0.682131 val_acc: 0.710000 test_loss: 0.656579 test_acc: 0.730000
Epoch: 100 train_loss: 0.004622 val_loss: 0.711730 val_acc: 0.650000 test_loss: 0.700536 test_acc: 0.710000
Epoch: 110 train_loss: 0.004714 val_loss: 0.683400 val_acc: 0.710000 test_loss: 0.654087 test_acc: 0.710000
Epoch: 120 train_loss: 0.004844 val_loss: 0.670749 val_acc: 0.640000 test_loss: 0.665964 test_acc: 0.700000
Epoch: 130 train_loss: 0.004653 val_loss: 0.667036 val_acc: 0.710000 test_loss: 0.669159 test_acc: 0.680000
Epoch: 140 train_loss: 0.004775 val_loss: 0.703201 val_acc: 0.650000 test_loss: 0.719822 test_acc: 0.650000
Epoch: 150 train_loss: 0.004912 val_loss: 0.652150 val_acc: 0.680000 test_loss: 0.617856 test_acc: 0.710000
Epoch: 160 train_loss: 0.004516 val_loss: 0.711981 val_acc: 0.690000 test_loss: 0.683352 test_acc: 0.700000
Epoch: 170 train_loss: 0.004583 val_loss: 0.706664 val_acc: 0.680000 test_loss: 0.653348 test_acc: 0.720000
Epoch: 180 train_loss: 0.004512 val_loss: 0.707946 val_acc: 0.690000 test_loss: 0.662025 test_acc: 0.710000
Epoch: 190 train_loss: 0.004467 val_loss: 0.741166 val_acc: 0.710000 test_loss: 0.707970 test_acc: 0.700000
Epoch: 200 train_loss: 0.004646 val_loss: 0.670258 val_acc: 0.670000 test_loss: 0.646499 test_acc: 0.700000
Epoch: 210 train_loss: 0.004676 val_loss: 0.725642 val_acc: 0.680000 test_loss: 0.723383 test_acc: 0.690000
Epoch: 220 train_loss: 0.004421 val_loss: 0.720681 val_acc: 0.680000 test_loss: 0.692514 test_acc: 0.710000
Epoch: 230 train_loss: 0.004498 val_loss: 0.712484 val_acc: 0.710000 test_loss: 0.705183 test_acc: 0.700000
Epoch: 240 train_loss: 0.004636 val_loss: 0.698561 val_acc: 0.690000 test_loss: 0.697624 test_acc: 0.710000
Epoch: 250 train_loss: 0.004505 val_loss: 0.685079 val_acc: 0.700000 test_loss: 0.651592 test_acc: 0.730000
Epoch: 260 train_loss: 0.004495 val_loss: 0.720152 val_acc: 0.670000 test_loss: 0.668347 test_acc: 0.730000
Epoch: 270 train_loss: 0.004627 val_loss: 0.720971 val_acc: 0.650000 test_loss: 0.704150 test_acc: 0.690000
Epoch: 280 train_loss: 0.004644 val_loss: 0.669758 val_acc: 0.660000 test_loss: 0.653736 test_acc: 0.710000
Epoch: 290 train_loss: 0.004463 val_loss: 0.734115 val_acc: 0.680000 test_loss: 0.714677 test_acc: 0.680000
Epoch: 300 train_loss: 0.004410 val_loss: 0.719485 val_acc: 0.710000 test_loss: 0.713549 test_acc: 0.700000
Epoch: 310 train_loss: 0.004747 val_loss: 0.671016 val_acc: 0.710000 test_loss: 0.632346 test_acc: 0.760000
Epoch: 320 train_loss: 0.004679 val_loss: 0.659112 val_acc: 0.690000 test_loss: 0.656459 test_acc: 0.690000
Epoch: 330 train_loss: 0.004406 val_loss: 0.688254 val_acc: 0.720000 test_loss: 0.672397 test_acc: 0.720000
Epoch: 340 train_loss: 0.004382 val_loss: 0.706521 val_acc: 0.720000 test_loss: 0.697083 test_acc: 0.710000
Epoch: 350 train_loss: 0.004397 val_loss: 0.706248 val_acc: 0.710000 test_loss: 0.682030 test_acc: 0.720000
Epoch: 360 train_loss: 0.004425 val_loss: 0.727656 val_acc: 0.710000 test_loss: 0.695814 test_acc: 0.730000
Epoch: 370 train_loss: 0.004344 val_loss: 0.721585 val_acc: 0.700000 test_loss: 0.695008 test_acc: 0.720000
Epoch: 380 train_loss: 0.005049 val_loss: 0.667406 val_acc: 0.640000 test_loss: 0.642069 test_acc: 0.720000
Epoch: 390 train_loss: 0.004719 val_loss: 0.660246 val_acc: 0.740000 test_loss: 0.665102 test_acc: 0.740000
Epoch: 400 train_loss: 0.004608 val_loss: 0.676606 val_acc: 0.670000 test_loss: 0.667975 test_acc: 0.710000
Epoch: 410 train_loss: 0.004536 val_loss: 0.707648 val_acc: 0.650000 test_loss: 0.644817 test_acc: 0.720000
Epoch: 420 train_loss: 0.004407 val_loss: 0.710277 val_acc: 0.690000 test_loss: 0.700148 test_acc: 0.690000
Epoch: 430 train_loss: 0.004384 val_loss: 0.712326 val_acc: 0.700000 test_loss: 0.685791 test_acc: 0.720000
Epoch: 440 train_loss: 0.004321 val_loss: 0.709647 val_acc: 0.690000 test_loss: 0.675147 test_acc: 0.730000
Epoch: 450 train_loss: 0.004382 val_loss: 0.724045 val_acc: 0.690000 test_loss: 0.676879 test_acc: 0.720000
Epoch: 460 train_loss: 0.004373 val_loss: 0.688445 val_acc: 0.720000 test_loss: 0.699644 test_acc: 0.710000
