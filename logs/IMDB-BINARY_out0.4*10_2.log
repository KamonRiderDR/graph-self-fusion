/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/dataset.py:214: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'dataset/TUDataset/IMDB-BINARY/processed' first
  warnings.warn(
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:0', dataset='IMDB-BINARY', in_size=136, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=136, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=136, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=6, eta=0.5, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.008054 val_loss: 0.705374 val_acc: 0.500000 test_loss: 0.700840 test_acc: 0.530000
Epoch: 010 train_loss: 0.005627 val_loss: 0.611405 val_acc: 0.790000 test_loss: 0.651125 test_acc: 0.670000
Epoch: 020 train_loss: 0.005244 val_loss: 0.619386 val_acc: 0.740000 test_loss: 0.664713 test_acc: 0.690000
Epoch: 030 train_loss: 0.005191 val_loss: 0.617256 val_acc: 0.760000 test_loss: 0.649919 test_acc: 0.680000
Epoch: 040 train_loss: 0.005062 val_loss: 0.604538 val_acc: 0.780000 test_loss: 0.634343 test_acc: 0.700000
Epoch: 050 train_loss: 0.005007 val_loss: 0.597718 val_acc: 0.780000 test_loss: 0.666443 test_acc: 0.670000
Epoch: 060 train_loss: 0.004926 val_loss: 0.601545 val_acc: 0.780000 test_loss: 0.647206 test_acc: 0.660000
Epoch: 070 train_loss: 0.004847 val_loss: 0.603603 val_acc: 0.760000 test_loss: 0.658707 test_acc: 0.680000
Epoch: 080 train_loss: 0.004795 val_loss: 0.624864 val_acc: 0.730000 test_loss: 0.664027 test_acc: 0.660000
Epoch: 090 train_loss: 0.004692 val_loss: 0.628654 val_acc: 0.750000 test_loss: 0.683448 test_acc: 0.680000
Epoch: 100 train_loss: 0.004903 val_loss: 0.626306 val_acc: 0.750000 test_loss: 0.680333 test_acc: 0.700000
Epoch: 110 train_loss: 0.004637 val_loss: 0.611552 val_acc: 0.740000 test_loss: 0.686667 test_acc: 0.650000
Epoch: 120 train_loss: 0.004579 val_loss: 0.615603 val_acc: 0.740000 test_loss: 0.668956 test_acc: 0.720000
Epoch: 130 train_loss: 0.004694 val_loss: 0.622193 val_acc: 0.700000 test_loss: 0.684622 test_acc: 0.670000
Epoch: 140 train_loss: 0.004872 val_loss: 0.634206 val_acc: 0.730000 test_loss: 0.680435 test_acc: 0.640000
Epoch: 150 train_loss: 0.004933 val_loss: 0.647045 val_acc: 0.670000 test_loss: 0.679740 test_acc: 0.630000
Epoch: 160 train_loss: 0.004680 val_loss: 0.611735 val_acc: 0.750000 test_loss: 0.678016 test_acc: 0.650000
Epoch: 170 train_loss: 0.004457 val_loss: 0.607182 val_acc: 0.750000 test_loss: 0.679968 test_acc: 0.700000
Epoch: 180 train_loss: 0.004417 val_loss: 0.625892 val_acc: 0.750000 test_loss: 0.722101 test_acc: 0.680000
Epoch: 190 train_loss: 0.004563 val_loss: 0.632748 val_acc: 0.720000 test_loss: 0.717235 test_acc: 0.640000
Epoch: 200 train_loss: 0.004421 val_loss: 0.621292 val_acc: 0.730000 test_loss: 0.713160 test_acc: 0.680000
Epoch: 210 train_loss: 0.004496 val_loss: 0.643099 val_acc: 0.730000 test_loss: 0.732560 test_acc: 0.670000
Epoch: 220 train_loss: 0.004467 val_loss: 0.627552 val_acc: 0.730000 test_loss: 0.708821 test_acc: 0.680000
Epoch: 230 train_loss: 0.004795 val_loss: 0.613403 val_acc: 0.760000 test_loss: 0.679594 test_acc: 0.650000
Epoch: 240 train_loss: 0.004495 val_loss: 0.629175 val_acc: 0.720000 test_loss: 0.712426 test_acc: 0.680000
Epoch: 250 train_loss: 0.004356 val_loss: 0.640063 val_acc: 0.730000 test_loss: 0.733264 test_acc: 0.690000
Epoch: 260 train_loss: 0.004698 val_loss: 0.612274 val_acc: 0.710000 test_loss: 0.678098 test_acc: 0.690000
Epoch: 270 train_loss: 0.004393 val_loss: 0.631918 val_acc: 0.730000 test_loss: 0.717101 test_acc: 0.690000
Epoch: 280 train_loss: 0.004356 val_loss: 0.641409 val_acc: 0.710000 test_loss: 0.726123 test_acc: 0.690000
Epoch: 290 train_loss: 0.004364 val_loss: 0.639004 val_acc: 0.740000 test_loss: 0.708377 test_acc: 0.680000
Epoch: 300 train_loss: 0.004304 val_loss: 0.640095 val_acc: 0.720000 test_loss: 0.726051 test_acc: 0.680000
Epoch: 310 train_loss: 0.004316 val_loss: 0.650551 val_acc: 0.730000 test_loss: 0.742424 test_acc: 0.670000
Epoch: 320 train_loss: 0.004285 val_loss: 0.645326 val_acc: 0.710000 test_loss: 0.714126 test_acc: 0.680000
Epoch: 330 train_loss: 0.004307 val_loss: 0.640007 val_acc: 0.730000 test_loss: 0.718116 test_acc: 0.680000
Epoch: 340 train_loss: 0.004544 val_loss: 0.638928 val_acc: 0.720000 test_loss: 0.686212 test_acc: 0.680000
Epoch: 350 train_loss: 0.004485 val_loss: 0.656578 val_acc: 0.730000 test_loss: 0.679232 test_acc: 0.690000
Epoch: 360 train_loss: 0.004386 val_loss: 0.632769 val_acc: 0.750000 test_loss: 0.720611 test_acc: 0.700000
Epoch: 370 train_loss: 0.004685 val_loss: 0.622606 val_acc: 0.740000 test_loss: 0.703667 test_acc: 0.660000
Epoch: 380 train_loss: 0.004586 val_loss: 0.623459 val_acc: 0.740000 test_loss: 0.701074 test_acc: 0.660000
Epoch: 390 train_loss: 0.004642 val_loss: 0.615293 val_acc: 0.760000 test_loss: 0.714539 test_acc: 0.650000
Epoch: 400 train_loss: 0.004379 val_loss: 0.606479 val_acc: 0.760000 test_loss: 0.704167 test_acc: 0.680000
Epoch: 410 train_loss: 0.004369 val_loss: 0.601257 val_acc: 0.750000 test_loss: 0.723900 test_acc: 0.690000
Epoch: 420 train_loss: 0.004514 val_loss: 0.629189 val_acc: 0.730000 test_loss: 0.734621 test_acc: 0.660000
Epoch: 430 train_loss: 0.004815 val_loss: 0.615818 val_acc: 0.730000 test_loss: 0.719852 test_acc: 0.630000
Epoch: 440 train_loss: 0.004579 val_loss: 0.621835 val_acc: 0.730000 test_loss: 0.696737 test_acc: 0.680000
Epoch: 450 train_loss: 0.004434 val_loss: 0.611128 val_acc: 0.770000 test_loss: 0.699096 test_acc: 0.680000
Epoch: 460 train_loss: 0.004352 val_loss: 0.612488 val_acc: 0.760000 test_loss: 0.710839 test_acc: 0.680000
Epoch: 470 train_loss: 0.004377 val_loss: 0.615483 val_acc: 0.740000 test_loss: 0.718080 test_acc: 0.680000
Epoch: 480 train_loss: 0.004331 val_loss: 0.625561 val_acc: 0.740000 test_loss: 0.728045 test_acc: 0.690000
Epoch: 490 train_loss: 0.004307 val_loss: 0.633089 val_acc: 0.730000 test_loss: 0.725499 test_acc: 0.680000
Epoch: 500 train_loss: 0.004562 val_loss: 0.631398 val_acc: 0.730000 test_loss: 0.719516 test_acc: 0.640000
Epoch: 510 train_loss: 0.004366 val_loss: 0.642712 val_acc: 0.730000 test_loss: 0.737301 test_acc: 0.660000
Epoch: 520 train_loss: 0.004467 val_loss: 0.630611 val_acc: 0.740000 test_loss: 0.739649 test_acc: 0.660000
Epoch: 530 train_loss: 0.004316 val_loss: 0.625099 val_acc: 0.740000 test_loss: 0.705699 test_acc: 0.690000
Epoch: 540 train_loss: 0.004300 val_loss: 0.629151 val_acc: 0.710000 test_loss: 0.718846 test_acc: 0.690000
Epoch: 550 train_loss: 0.004312 val_loss: 0.632615 val_acc: 0.730000 test_loss: 0.713687 test_acc: 0.690000
Epoch: 560 train_loss: 0.004304 val_loss: 0.635082 val_acc: 0.720000 test_loss: 0.726696 test_acc: 0.690000
Epoch: 570 train_loss: 0.004265 val_loss: 0.632036 val_acc: 0.720000 test_loss: 0.724889 test_acc: 0.690000
Epoch: 580 train_loss: 0.004314 val_loss: 0.637532 val_acc: 0.730000 test_loss: 0.727266 test_acc: 0.690000
Epoch: 590 train_loss: 0.004311 val_loss: 0.638131 val_acc: 0.710000 test_loss: 0.725115 test_acc: 0.700000
Epoch: 600 train_loss: 0.004285 val_loss: 0.632713 val_acc: 0.740000 test_loss: 0.718784 test_acc: 0.670000
Epoch: 610 train_loss: 0.004329 val_loss: 0.640611 val_acc: 0.750000 test_loss: 0.728949 test_acc: 0.680000
Epoch: 620 train_loss: 0.004298 val_loss: 0.629997 val_acc: 0.740000 test_loss: 0.707166 test_acc: 0.680000
Epoch: 630 train_loss: 0.004299 val_loss: 0.627985 val_acc: 0.730000 test_loss: 0.715275 test_acc: 0.690000
Epoch: 640 train_loss: 0.004278 val_loss: 0.634077 val_acc: 0.740000 test_loss: 0.743373 test_acc: 0.670000
Epoch: 650 train_loss: 0.004256 val_loss: 0.651088 val_acc: 0.730000 test_loss: 0.735615 test_acc: 0.670000
Epoch: 660 train_loss: 0.004264 val_loss: 0.632053 val_acc: 0.740000 test_loss: 0.723391 test_acc: 0.670000
Epoch: 670 train_loss: 0.004308 val_loss: 0.651584 val_acc: 0.720000 test_loss: 0.732969 test_acc: 0.690000
Epoch: 680 train_loss: 0.004317 val_loss: 0.631533 val_acc: 0.720000 test_loss: 0.716196 test_acc: 0.690000
Epoch: 690 train_loss: 0.004311 val_loss: 0.650780 val_acc: 0.720000 test_loss: 0.738523 test_acc: 0.690000
Epoch: 700 train_loss: 0.005562 val_loss: 0.649434 val_acc: 0.710000 test_loss: 0.660198 test_acc: 0.640000
Epoch: 710 train_loss: 0.004895 val_loss: 0.626606 val_acc: 0.710000 test_loss: 0.694836 test_acc: 0.640000
Epoch: 720 train_loss: 0.004718 val_loss: 0.613103 val_acc: 0.750000 test_loss: 0.688342 test_acc: 0.640000
Epoch: 730 train_loss: 0.004406 val_loss: 0.618614 val_acc: 0.760000 test_loss: 0.697387 test_acc: 0.690000
Epoch: 740 train_loss: 0.004368 val_loss: 0.618958 val_acc: 0.770000 test_loss: 0.714322 test_acc: 0.670000
Epoch: 750 train_loss: 0.004577 val_loss: 0.635628 val_acc: 0.690000 test_loss: 0.693343 test_acc: 0.670000
Epoch: 760 train_loss: 0.004465 val_loss: 0.625237 val_acc: 0.750000 test_loss: 0.730338 test_acc: 0.670000
Epoch: 770 train_loss: 0.004617 val_loss: 0.621107 val_acc: 0.740000 test_loss: 0.703226 test_acc: 0.680000
Epoch: 780 train_loss: 0.004384 val_loss: 0.656244 val_acc: 0.710000 test_loss: 0.731906 test_acc: 0.670000
Epoch: 790 train_loss: 0.004440 val_loss: 0.631788 val_acc: 0.740000 test_loss: 0.759892 test_acc: 0.610000
------------- 0 val_acc: 0.8100 test_acc: 0.6800 best_test: 0.7200 -----------------
Fold: 1
Epoch: 000 train_loss: 0.006818 val_loss: 0.710590 val_acc: 0.520000 test_loss: 0.714782 test_acc: 0.600000
Epoch: 010 train_loss: 0.005418 val_loss: 0.663900 val_acc: 0.670000 test_loss: 0.675294 test_acc: 0.710000
Epoch: 020 train_loss: 0.005232 val_loss: 0.673505 val_acc: 0.670000 test_loss: 0.678485 test_acc: 0.690000
Epoch: 030 train_loss: 0.005071 val_loss: 0.685153 val_acc: 0.650000 test_loss: 0.685977 test_acc: 0.660000
Epoch: 040 train_loss: 0.005024 val_loss: 0.670895 val_acc: 0.660000 test_loss: 0.674245 test_acc: 0.670000
Epoch: 050 train_loss: 0.004945 val_loss: 0.649702 val_acc: 0.670000 test_loss: 0.649468 test_acc: 0.700000
Epoch: 060 train_loss: 0.005007 val_loss: 0.638424 val_acc: 0.690000 test_loss: 0.639283 test_acc: 0.720000
Epoch: 070 train_loss: 0.004866 val_loss: 0.676873 val_acc: 0.640000 test_loss: 0.649664 test_acc: 0.710000
Epoch: 080 train_loss: 0.004848 val_loss: 0.668445 val_acc: 0.690000 test_loss: 0.682027 test_acc: 0.690000
Epoch: 090 train_loss: 0.004718 val_loss: 0.668992 val_acc: 0.650000 test_loss: 0.660095 test_acc: 0.730000
Epoch: 100 train_loss: 0.004855 val_loss: 0.647515 val_acc: 0.700000 test_loss: 0.634621 test_acc: 0.710000
Epoch: 110 train_loss: 0.004837 val_loss: 0.702078 val_acc: 0.670000 test_loss: 0.718627 test_acc: 0.710000
Epoch: 120 train_loss: 0.004526 val_loss: 0.690212 val_acc: 0.670000 test_loss: 0.651526 test_acc: 0.700000
Epoch: 130 train_loss: 0.004534 val_loss: 0.705238 val_acc: 0.690000 test_loss: 0.738397 test_acc: 0.680000
Epoch: 140 train_loss: 0.004811 val_loss: 0.644962 val_acc: 0.710000 test_loss: 0.655366 test_acc: 0.720000
Epoch: 150 train_loss: 0.004688 val_loss: 0.658741 val_acc: 0.710000 test_loss: 0.690682 test_acc: 0.710000
Epoch: 160 train_loss: 0.004540 val_loss: 0.698491 val_acc: 0.640000 test_loss: 0.676636 test_acc: 0.740000
Epoch: 170 train_loss: 0.004564 val_loss: 0.701419 val_acc: 0.690000 test_loss: 0.697637 test_acc: 0.700000
Epoch: 180 train_loss: 0.004956 val_loss: 0.633433 val_acc: 0.680000 test_loss: 0.640885 test_acc: 0.720000
Epoch: 190 train_loss: 0.004664 val_loss: 0.681335 val_acc: 0.670000 test_loss: 0.635421 test_acc: 0.740000
Epoch: 200 train_loss: 0.004639 val_loss: 0.683569 val_acc: 0.690000 test_loss: 0.702330 test_acc: 0.710000
Epoch: 210 train_loss: 0.004599 val_loss: 0.682986 val_acc: 0.690000 test_loss: 0.682927 test_acc: 0.720000
Epoch: 220 train_loss: 0.004446 val_loss: 0.703067 val_acc: 0.670000 test_loss: 0.664169 test_acc: 0.710000
Epoch: 230 train_loss: 0.004413 val_loss: 0.717995 val_acc: 0.700000 test_loss: 0.676949 test_acc: 0.710000
Epoch: 240 train_loss: 0.004401 val_loss: 0.709962 val_acc: 0.700000 test_loss: 0.687550 test_acc: 0.700000
Epoch: 250 train_loss: 0.004476 val_loss: 0.714567 val_acc: 0.690000 test_loss: 0.686788 test_acc: 0.740000
Epoch: 260 train_loss: 0.004866 val_loss: 0.674479 val_acc: 0.660000 test_loss: 0.666365 test_acc: 0.680000
Epoch: 270 train_loss: 0.004770 val_loss: 0.676230 val_acc: 0.710000 test_loss: 0.670646 test_acc: 0.700000
Epoch: 280 train_loss: 0.004466 val_loss: 0.680688 val_acc: 0.690000 test_loss: 0.679788 test_acc: 0.730000
Epoch: 290 train_loss: 0.004531 val_loss: 0.663884 val_acc: 0.700000 test_loss: 0.668132 test_acc: 0.680000
Epoch: 300 train_loss: 0.004724 val_loss: 0.634474 val_acc: 0.720000 test_loss: 0.628940 test_acc: 0.740000
Epoch: 310 train_loss: 0.004649 val_loss: 0.655520 val_acc: 0.730000 test_loss: 0.641988 test_acc: 0.710000
Epoch: 320 train_loss: 0.004708 val_loss: 0.708515 val_acc: 0.670000 test_loss: 0.660283 test_acc: 0.670000
Epoch: 330 train_loss: 0.004434 val_loss: 0.682068 val_acc: 0.700000 test_loss: 0.707682 test_acc: 0.660000
Epoch: 340 train_loss: 0.004483 val_loss: 0.685155 val_acc: 0.700000 test_loss: 0.688203 test_acc: 0.710000
Epoch: 350 train_loss: 0.004405 val_loss: 0.706084 val_acc: 0.690000 test_loss: 0.694594 test_acc: 0.690000
Epoch: 360 train_loss: 0.004423 val_loss: 0.708152 val_acc: 0.700000 test_loss: 0.698752 test_acc: 0.700000
Epoch: 370 train_loss: 0.004338 val_loss: 0.705396 val_acc: 0.700000 test_loss: 0.709791 test_acc: 0.700000
Epoch: 380 train_loss: 0.004386 val_loss: 0.719650 val_acc: 0.690000 test_loss: 0.736396 test_acc: 0.700000
Epoch: 390 train_loss: 0.004362 val_loss: 0.722670 val_acc: 0.710000 test_loss: 0.741636 test_acc: 0.690000
Epoch: 400 train_loss: 0.004409 val_loss: 0.726072 val_acc: 0.710000 test_loss: 0.731572 test_acc: 0.700000
Epoch: 410 train_loss: 0.004344 val_loss: 0.707195 val_acc: 0.680000 test_loss: 0.694554 test_acc: 0.740000
Epoch: 420 train_loss: 0.004365 val_loss: 0.696622 val_acc: 0.720000 test_loss: 0.683521 test_acc: 0.710000
Epoch: 430 train_loss: 0.004334 val_loss: 0.703436 val_acc: 0.700000 test_loss: 0.688871 test_acc: 0.710000
