/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/dataset.py:214: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'dataset/TUDataset/IMDB-BINARY/processed' first
  warnings.warn(
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:0', seed=2023, dataset='IMDB-BINARY', in_size=136, num_classes=2, model_name='fusion_tm', fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=136, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=136, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.6, num_fusion_layers=6, eta=0.5, ffn_dim=128, num_trans_layers=3, lam1=0.2, lam2=0.2, theta1=0.15, theta2=0.4, theta3=0.4, gamma=0.7, patience=500, loss_log=2, folds=10, lr=1e-05, weight_decay=5e-05, batch_size=128, epoches=1000, output_dim=2)
Running 0 times
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.008905 val_loss: 0.763736 val_acc: 0.530000 test_loss: 0.801355 test_acc: 0.490000
Epoch: 010 train_loss: 0.005661 val_loss: 0.642534 val_acc: 0.680000 test_loss: 0.645849 test_acc: 0.670000
Epoch: 020 train_loss: 0.005177 val_loss: 0.622732 val_acc: 0.660000 test_loss: 0.630879 test_acc: 0.660000
Epoch: 030 train_loss: 0.005039 val_loss: 0.606857 val_acc: 0.720000 test_loss: 0.632966 test_acc: 0.680000
Epoch: 040 train_loss: 0.004892 val_loss: 0.615969 val_acc: 0.710000 test_loss: 0.631942 test_acc: 0.680000
Epoch: 050 train_loss: 0.004832 val_loss: 0.636739 val_acc: 0.730000 test_loss: 0.642595 test_acc: 0.670000
Epoch: 060 train_loss: 0.004743 val_loss: 0.644990 val_acc: 0.650000 test_loss: 0.643641 test_acc: 0.710000
Epoch: 070 train_loss: 0.004611 val_loss: 0.616850 val_acc: 0.720000 test_loss: 0.669089 test_acc: 0.650000
Epoch: 080 train_loss: 0.004613 val_loss: 0.633613 val_acc: 0.660000 test_loss: 0.637455 test_acc: 0.710000
Epoch: 090 train_loss: 0.004589 val_loss: 0.636835 val_acc: 0.670000 test_loss: 0.651119 test_acc: 0.700000
Epoch: 100 train_loss: 0.004517 val_loss: 0.651898 val_acc: 0.650000 test_loss: 0.654403 test_acc: 0.710000
Epoch: 110 train_loss: 0.004468 val_loss: 0.632516 val_acc: 0.700000 test_loss: 0.656464 test_acc: 0.690000
Epoch: 120 train_loss: 0.004388 val_loss: 0.655964 val_acc: 0.680000 test_loss: 0.662868 test_acc: 0.690000
Epoch: 130 train_loss: 0.004360 val_loss: 0.661659 val_acc: 0.680000 test_loss: 0.664977 test_acc: 0.690000
Epoch: 140 train_loss: 0.004407 val_loss: 0.652439 val_acc: 0.690000 test_loss: 0.668269 test_acc: 0.710000
Epoch: 150 train_loss: 0.004219 val_loss: 0.662296 val_acc: 0.680000 test_loss: 0.670885 test_acc: 0.690000
Epoch: 160 train_loss: 0.004278 val_loss: 0.647380 val_acc: 0.680000 test_loss: 0.690411 test_acc: 0.700000
Epoch: 170 train_loss: 0.004289 val_loss: 0.645337 val_acc: 0.730000 test_loss: 0.698824 test_acc: 0.700000
Epoch: 180 train_loss: 0.004282 val_loss: 0.633487 val_acc: 0.690000 test_loss: 0.692834 test_acc: 0.690000
Epoch: 190 train_loss: 0.004248 val_loss: 0.672196 val_acc: 0.690000 test_loss: 0.708165 test_acc: 0.690000
Epoch: 200 train_loss: 0.004276 val_loss: 0.662225 val_acc: 0.690000 test_loss: 0.709242 test_acc: 0.700000
Epoch: 210 train_loss: 0.004195 val_loss: 0.659786 val_acc: 0.690000 test_loss: 0.705999 test_acc: 0.700000
Epoch: 220 train_loss: 0.004223 val_loss: 0.683860 val_acc: 0.710000 test_loss: 0.713349 test_acc: 0.700000
Epoch: 230 train_loss: 0.004226 val_loss: 0.645826 val_acc: 0.690000 test_loss: 0.724187 test_acc: 0.700000
Epoch: 240 train_loss: 0.004176 val_loss: 0.662102 val_acc: 0.710000 test_loss: 0.726615 test_acc: 0.700000
Epoch: 250 train_loss: 0.004180 val_loss: 0.657387 val_acc: 0.720000 test_loss: 0.713624 test_acc: 0.690000
Epoch: 260 train_loss: 0.004116 val_loss: 0.663996 val_acc: 0.700000 test_loss: 0.728027 test_acc: 0.700000
Epoch: 270 train_loss: 0.004225 val_loss: 0.658349 val_acc: 0.720000 test_loss: 0.718925 test_acc: 0.690000
Epoch: 280 train_loss: 0.004134 val_loss: 0.662663 val_acc: 0.670000 test_loss: 0.723580 test_acc: 0.690000
Epoch: 290 train_loss: 0.004146 val_loss: 0.664071 val_acc: 0.700000 test_loss: 0.733256 test_acc: 0.700000
Epoch: 300 train_loss: 0.004123 val_loss: 0.665520 val_acc: 0.700000 test_loss: 0.731788 test_acc: 0.690000
Epoch: 310 train_loss: 0.004071 val_loss: 0.683826 val_acc: 0.690000 test_loss: 0.725049 test_acc: 0.700000
Epoch: 320 train_loss: 0.004110 val_loss: 0.667376 val_acc: 0.700000 test_loss: 0.737678 test_acc: 0.700000
Epoch: 330 train_loss: 0.004106 val_loss: 0.663653 val_acc: 0.700000 test_loss: 0.744842 test_acc: 0.690000
Epoch: 340 train_loss: 0.004053 val_loss: 0.689097 val_acc: 0.710000 test_loss: 0.720010 test_acc: 0.690000
Epoch: 350 train_loss: 0.004091 val_loss: 0.684188 val_acc: 0.680000 test_loss: 0.743450 test_acc: 0.670000
Epoch: 360 train_loss: 0.004099 val_loss: 0.662457 val_acc: 0.710000 test_loss: 0.741757 test_acc: 0.680000
Epoch: 370 train_loss: 0.004092 val_loss: 0.675018 val_acc: 0.700000 test_loss: 0.743569 test_acc: 0.690000
Epoch: 380 train_loss: 0.004039 val_loss: 0.665744 val_acc: 0.710000 test_loss: 0.744987 test_acc: 0.670000
Epoch: 390 train_loss: 0.004003 val_loss: 0.671920 val_acc: 0.690000 test_loss: 0.741844 test_acc: 0.690000
Epoch: 400 train_loss: 0.004027 val_loss: 0.681835 val_acc: 0.700000 test_loss: 0.752301 test_acc: 0.710000
Epoch: 410 train_loss: 0.004045 val_loss: 0.668559 val_acc: 0.700000 test_loss: 0.742306 test_acc: 0.680000
Epoch: 420 train_loss: 0.004008 val_loss: 0.663505 val_acc: 0.710000 test_loss: 0.754411 test_acc: 0.690000
Epoch: 430 train_loss: 0.004103 val_loss: 0.664921 val_acc: 0.700000 test_loss: 0.752114 test_acc: 0.690000
Epoch: 440 train_loss: 0.004064 val_loss: 0.660089 val_acc: 0.710000 test_loss: 0.754439 test_acc: 0.700000
Epoch: 450 train_loss: 0.004010 val_loss: 0.669937 val_acc: 0.690000 test_loss: 0.744610 test_acc: 0.680000
Epoch: 460 train_loss: 0.004019 val_loss: 0.665959 val_acc: 0.730000 test_loss: 0.746878 test_acc: 0.690000
Epoch: 470 train_loss: 0.004033 val_loss: 0.665825 val_acc: 0.710000 test_loss: 0.751593 test_acc: 0.690000
Epoch: 480 train_loss: 0.004004 val_loss: 0.668117 val_acc: 0.700000 test_loss: 0.753076 test_acc: 0.690000
Epoch: 490 train_loss: 0.004040 val_loss: 0.664491 val_acc: 0.720000 test_loss: 0.749836 test_acc: 0.680000
Epoch: 500 train_loss: 0.004025 val_loss: 0.666148 val_acc: 0.700000 test_loss: 0.749987 test_acc: 0.680000
Epoch: 510 train_loss: 0.004067 val_loss: 0.674713 val_acc: 0.710000 test_loss: 0.746505 test_acc: 0.680000
Epoch: 520 train_loss: 0.004020 val_loss: 0.660806 val_acc: 0.720000 test_loss: 0.751471 test_acc: 0.690000
Epoch: 530 train_loss: 0.004012 val_loss: 0.666872 val_acc: 0.710000 test_loss: 0.751642 test_acc: 0.680000
Early stop at epoch 537.
------------- 0 val_acc: 0.7500 test_acc: 0.6700 best_test: 0.7300 -----------------
Fold: 1
Epoch: 000 train_loss: 0.007215 val_loss: 0.717835 val_acc: 0.580000 test_loss: 0.716804 test_acc: 0.560000
Epoch: 010 train_loss: 0.005340 val_loss: 0.664770 val_acc: 0.660000 test_loss: 0.667364 test_acc: 0.660000
Epoch: 020 train_loss: 0.005014 val_loss: 0.670808 val_acc: 0.640000 test_loss: 0.649728 test_acc: 0.670000
Epoch: 030 train_loss: 0.004871 val_loss: 0.674371 val_acc: 0.650000 test_loss: 0.639207 test_acc: 0.700000
Epoch: 040 train_loss: 0.004690 val_loss: 0.696788 val_acc: 0.670000 test_loss: 0.650394 test_acc: 0.680000
Epoch: 050 train_loss: 0.004683 val_loss: 0.686388 val_acc: 0.680000 test_loss: 0.648525 test_acc: 0.700000
Epoch: 060 train_loss: 0.004561 val_loss: 0.687949 val_acc: 0.650000 test_loss: 0.646787 test_acc: 0.670000
Epoch: 070 train_loss: 0.004508 val_loss: 0.701381 val_acc: 0.650000 test_loss: 0.658789 test_acc: 0.660000
Epoch: 080 train_loss: 0.004488 val_loss: 0.713774 val_acc: 0.650000 test_loss: 0.668048 test_acc: 0.670000
Epoch: 090 train_loss: 0.004476 val_loss: 0.711666 val_acc: 0.650000 test_loss: 0.682442 test_acc: 0.650000
Epoch: 100 train_loss: 0.004637 val_loss: 0.681308 val_acc: 0.700000 test_loss: 0.710447 test_acc: 0.700000
Epoch: 110 train_loss: 0.004359 val_loss: 0.697737 val_acc: 0.690000 test_loss: 0.708391 test_acc: 0.710000
Epoch: 120 train_loss: 0.004303 val_loss: 0.719261 val_acc: 0.660000 test_loss: 0.674166 test_acc: 0.680000
Epoch: 130 train_loss: 0.004385 val_loss: 0.729607 val_acc: 0.660000 test_loss: 0.673912 test_acc: 0.660000
Epoch: 140 train_loss: 0.004368 val_loss: 0.713994 val_acc: 0.670000 test_loss: 0.701635 test_acc: 0.680000
Epoch: 150 train_loss: 0.004246 val_loss: 0.708552 val_acc: 0.650000 test_loss: 0.678409 test_acc: 0.720000
Epoch: 160 train_loss: 0.004228 val_loss: 0.727393 val_acc: 0.670000 test_loss: 0.697085 test_acc: 0.700000
Epoch: 170 train_loss: 0.004199 val_loss: 0.712898 val_acc: 0.680000 test_loss: 0.693443 test_acc: 0.680000
Epoch: 180 train_loss: 0.004246 val_loss: 0.729256 val_acc: 0.680000 test_loss: 0.687344 test_acc: 0.670000
Epoch: 190 train_loss: 0.004212 val_loss: 0.739617 val_acc: 0.670000 test_loss: 0.705836 test_acc: 0.680000
Epoch: 200 train_loss: 0.004149 val_loss: 0.730074 val_acc: 0.680000 test_loss: 0.726476 test_acc: 0.680000
Epoch: 210 train_loss: 0.004281 val_loss: 0.710283 val_acc: 0.690000 test_loss: 0.763027 test_acc: 0.660000
Epoch: 220 train_loss: 0.004215 val_loss: 0.729013 val_acc: 0.660000 test_loss: 0.731662 test_acc: 0.680000
Epoch: 230 train_loss: 0.004137 val_loss: 0.728693 val_acc: 0.660000 test_loss: 0.707184 test_acc: 0.670000
Epoch: 240 train_loss: 0.004178 val_loss: 0.734885 val_acc: 0.670000 test_loss: 0.724551 test_acc: 0.650000
Epoch: 250 train_loss: 0.004166 val_loss: 0.733438 val_acc: 0.670000 test_loss: 0.746877 test_acc: 0.660000
Epoch: 260 train_loss: 0.004139 val_loss: 0.726204 val_acc: 0.670000 test_loss: 0.742453 test_acc: 0.680000
Epoch: 270 train_loss: 0.004107 val_loss: 0.719717 val_acc: 0.670000 test_loss: 0.750818 test_acc: 0.670000
Epoch: 280 train_loss: 0.004160 val_loss: 0.728561 val_acc: 0.670000 test_loss: 0.734365 test_acc: 0.660000
Epoch: 290 train_loss: 0.004096 val_loss: 0.730179 val_acc: 0.660000 test_loss: 0.732006 test_acc: 0.690000
Epoch: 300 train_loss: 0.004011 val_loss: 0.747792 val_acc: 0.660000 test_loss: 0.737210 test_acc: 0.680000
Epoch: 310 train_loss: 0.004169 val_loss: 0.735001 val_acc: 0.670000 test_loss: 0.715619 test_acc: 0.680000
Epoch: 320 train_loss: 0.004126 val_loss: 0.730173 val_acc: 0.680000 test_loss: 0.726867 test_acc: 0.690000
Epoch: 330 train_loss: 0.004128 val_loss: 0.723848 val_acc: 0.690000 test_loss: 0.738289 test_acc: 0.690000
Epoch: 340 train_loss: 0.004077 val_loss: 0.733589 val_acc: 0.660000 test_loss: 0.739360 test_acc: 0.680000
Epoch: 350 train_loss: 0.004081 val_loss: 0.730261 val_acc: 0.680000 test_loss: 0.723993 test_acc: 0.700000
Epoch: 360 train_loss: 0.004108 val_loss: 0.728610 val_acc: 0.690000 test_loss: 0.724383 test_acc: 0.690000
Epoch: 370 train_loss: 0.004033 val_loss: 0.734853 val_acc: 0.680000 test_loss: 0.738252 test_acc: 0.690000
Epoch: 380 train_loss: 0.004058 val_loss: 0.743530 val_acc: 0.670000 test_loss: 0.750910 test_acc: 0.690000
Epoch: 390 train_loss: 0.004049 val_loss: 0.725493 val_acc: 0.680000 test_loss: 0.730909 test_acc: 0.670000
Epoch: 400 train_loss: 0.004033 val_loss: 0.735073 val_acc: 0.660000 test_loss: 0.732597 test_acc: 0.680000
Epoch: 410 train_loss: 0.004090 val_loss: 0.737064 val_acc: 0.680000 test_loss: 0.729830 test_acc: 0.690000
Epoch: 420 train_loss: 0.004041 val_loss: 0.736377 val_acc: 0.670000 test_loss: 0.728111 test_acc: 0.690000
Epoch: 430 train_loss: 0.004073 val_loss: 0.736492 val_acc: 0.670000 test_loss: 0.735277 test_acc: 0.690000
Epoch: 440 train_loss: 0.004043 val_loss: 0.740168 val_acc: 0.670000 test_loss: 0.741948 test_acc: 0.680000
Epoch: 450 train_loss: 0.004200 val_loss: 0.736638 val_acc: 0.680000 test_loss: 0.741149 test_acc: 0.680000
Epoch: 460 train_loss: 0.004053 val_loss: 0.736322 val_acc: 0.670000 test_loss: 0.745631 test_acc: 0.670000
Epoch: 470 train_loss: 0.004053 val_loss: 0.730571 val_acc: 0.670000 test_loss: 0.742643 test_acc: 0.670000
Epoch: 480 train_loss: 0.004043 val_loss: 0.733742 val_acc: 0.670000 test_loss: 0.748447 test_acc: 0.680000
Epoch: 490 train_loss: 0.004151 val_loss: 0.728565 val_acc: 0.680000 test_loss: 0.727614 test_acc: 0.690000
Epoch: 500 train_loss: 0.004091 val_loss: 0.739530 val_acc: 0.670000 test_loss: 0.746979 test_acc: 0.680000
Epoch: 510 train_loss: 0.004002 val_loss: 0.737468 val_acc: 0.670000 test_loss: 0.751024 test_acc: 0.680000
Epoch: 520 train_loss: 0.004018 val_loss: 0.730465 val_acc: 0.680000 test_loss: 0.732712 test_acc: 0.680000
Epoch: 530 train_loss: 0.004115 val_loss: 0.731654 val_acc: 0.670000 test_loss: 0.745325 test_acc: 0.680000
Epoch: 540 train_loss: 0.004035 val_loss: 0.733546 val_acc: 0.660000 test_loss: 0.750772 test_acc: 0.680000
Epoch: 550 train_loss: 0.004022 val_loss: 0.732199 val_acc: 0.670000 test_loss: 0.741720 test_acc: 0.670000
Epoch: 560 train_loss: 0.004079 val_loss: 0.740133 val_acc: 0.660000 test_loss: 0.746834 test_acc: 0.680000
Epoch: 570 train_loss: 0.004058 val_loss: 0.743463 val_acc: 0.680000 test_loss: 0.745108 test_acc: 0.700000
Epoch: 580 train_loss: 0.004041 val_loss: 0.737163 val_acc: 0.680000 test_loss: 0.740222 test_acc: 0.680000
Epoch: 590 train_loss: 0.004051 val_loss: 0.732314 val_acc: 0.660000 test_loss: 0.740097 test_acc: 0.670000
Epoch: 600 train_loss: 0.004010 val_loss: 0.735723 val_acc: 0.670000 test_loss: 0.745043 test_acc: 0.660000
Epoch: 610 train_loss: 0.003986 val_loss: 0.740890 val_acc: 0.680000 test_loss: 0.744143 test_acc: 0.680000
Early stop at epoch 612.
------------- 1 val_acc: 0.7100 test_acc: 0.7000 best_test: 0.7100 -----------------
Fold: 2
Epoch: 000 train_loss: 0.007688 val_loss: 0.795597 val_acc: 0.610000 test_loss: 0.761748 test_acc: 0.530000
Epoch: 010 train_loss: 0.005244 val_loss: 0.639881 val_acc: 0.730000 test_loss: 0.611642 test_acc: 0.760000
Epoch: 020 train_loss: 0.004978 val_loss: 0.642836 val_acc: 0.700000 test_loss: 0.590657 test_acc: 0.780000
Epoch: 030 train_loss: 0.004918 val_loss: 0.640339 val_acc: 0.710000 test_loss: 0.583976 test_acc: 0.750000
Epoch: 040 train_loss: 0.004941 val_loss: 0.670863 val_acc: 0.670000 test_loss: 0.605646 test_acc: 0.760000
Epoch: 050 train_loss: 0.004760 val_loss: 0.658370 val_acc: 0.690000 test_loss: 0.586280 test_acc: 0.760000
Epoch: 060 train_loss: 0.004651 val_loss: 0.687597 val_acc: 0.670000 test_loss: 0.584726 test_acc: 0.750000
Epoch: 070 train_loss: 0.004633 val_loss: 0.707592 val_acc: 0.690000 test_loss: 0.601027 test_acc: 0.770000
Epoch: 080 train_loss: 0.004556 val_loss: 0.702389 val_acc: 0.660000 test_loss: 0.587713 test_acc: 0.790000
Epoch: 090 train_loss: 0.004557 val_loss: 0.668388 val_acc: 0.720000 test_loss: 0.592023 test_acc: 0.740000
Epoch: 100 train_loss: 0.004485 val_loss: 0.705960 val_acc: 0.690000 test_loss: 0.588559 test_acc: 0.790000
Epoch: 110 train_loss: 0.004430 val_loss: 0.694695 val_acc: 0.680000 test_loss: 0.580020 test_acc: 0.780000
Epoch: 120 train_loss: 0.004416 val_loss: 0.727070 val_acc: 0.710000 test_loss: 0.601221 test_acc: 0.760000
Epoch: 130 train_loss: 0.004385 val_loss: 0.708878 val_acc: 0.670000 test_loss: 0.594620 test_acc: 0.790000
Epoch: 140 train_loss: 0.004404 val_loss: 0.730598 val_acc: 0.690000 test_loss: 0.612530 test_acc: 0.750000
Epoch: 150 train_loss: 0.004349 val_loss: 0.724828 val_acc: 0.700000 test_loss: 0.605523 test_acc: 0.760000
Epoch: 160 train_loss: 0.004250 val_loss: 0.729231 val_acc: 0.670000 test_loss: 0.581235 test_acc: 0.770000
Epoch: 170 train_loss: 0.004290 val_loss: 0.737637 val_acc: 0.670000 test_loss: 0.614612 test_acc: 0.750000
Epoch: 180 train_loss: 0.004265 val_loss: 0.714890 val_acc: 0.690000 test_loss: 0.604850 test_acc: 0.770000
Epoch: 190 train_loss: 0.004308 val_loss: 0.705005 val_acc: 0.700000 test_loss: 0.611721 test_acc: 0.760000
Epoch: 200 train_loss: 0.004213 val_loss: 0.711624 val_acc: 0.660000 test_loss: 0.603515 test_acc: 0.780000
Epoch: 210 train_loss: 0.004192 val_loss: 0.714623 val_acc: 0.690000 test_loss: 0.585681 test_acc: 0.770000
Epoch: 220 train_loss: 0.004239 val_loss: 0.722530 val_acc: 0.660000 test_loss: 0.582739 test_acc: 0.790000
Epoch: 230 train_loss: 0.004140 val_loss: 0.762458 val_acc: 0.650000 test_loss: 0.607267 test_acc: 0.790000
Epoch: 240 train_loss: 0.004239 val_loss: 0.741397 val_acc: 0.660000 test_loss: 0.601958 test_acc: 0.790000
Epoch: 250 train_loss: 0.004221 val_loss: 0.745503 val_acc: 0.670000 test_loss: 0.593000 test_acc: 0.780000
Epoch: 260 train_loss: 0.004222 val_loss: 0.721990 val_acc: 0.640000 test_loss: 0.597131 test_acc: 0.780000
Epoch: 270 train_loss: 0.004267 val_loss: 0.728143 val_acc: 0.670000 test_loss: 0.591492 test_acc: 0.760000
Epoch: 280 train_loss: 0.004207 val_loss: 0.765512 val_acc: 0.660000 test_loss: 0.595176 test_acc: 0.780000
Epoch: 290 train_loss: 0.004190 val_loss: 0.763882 val_acc: 0.650000 test_loss: 0.593969 test_acc: 0.770000
Epoch: 300 train_loss: 0.004180 val_loss: 0.750103 val_acc: 0.650000 test_loss: 0.598276 test_acc: 0.780000
Epoch: 310 train_loss: 0.004118 val_loss: 0.731550 val_acc: 0.670000 test_loss: 0.590707 test_acc: 0.770000
Epoch: 320 train_loss: 0.004114 val_loss: 0.762555 val_acc: 0.630000 test_loss: 0.597082 test_acc: 0.800000
Epoch: 330 train_loss: 0.004150 val_loss: 0.733791 val_acc: 0.640000 test_loss: 0.591292 test_acc: 0.780000
Epoch: 340 train_loss: 0.004109 val_loss: 0.740520 val_acc: 0.660000 test_loss: 0.597901 test_acc: 0.770000
Epoch: 350 train_loss: 0.004105 val_loss: 0.747362 val_acc: 0.640000 test_loss: 0.599541 test_acc: 0.770000
Epoch: 360 train_loss: 0.004094 val_loss: 0.755319 val_acc: 0.630000 test_loss: 0.599244 test_acc: 0.780000
Epoch: 370 train_loss: 0.004087 val_loss: 0.741508 val_acc: 0.640000 test_loss: 0.596631 test_acc: 0.760000
Epoch: 380 train_loss: 0.004108 val_loss: 0.753021 val_acc: 0.620000 test_loss: 0.604409 test_acc: 0.780000
Epoch: 390 train_loss: 0.004085 val_loss: 0.781795 val_acc: 0.640000 test_loss: 0.608392 test_acc: 0.770000
Epoch: 400 train_loss: 0.004092 val_loss: 0.793014 val_acc: 0.620000 test_loss: 0.607612 test_acc: 0.770000
Epoch: 410 train_loss: 0.004115 val_loss: 0.788419 val_acc: 0.620000 test_loss: 0.600410 test_acc: 0.790000
Epoch: 420 train_loss: 0.004083 val_loss: 0.764233 val_acc: 0.610000 test_loss: 0.599565 test_acc: 0.770000
Epoch: 430 train_loss: 0.004071 val_loss: 0.822895 val_acc: 0.620000 test_loss: 0.617877 test_acc: 0.790000
Epoch: 440 train_loss: 0.004073 val_loss: 0.791667 val_acc: 0.620000 test_loss: 0.604708 test_acc: 0.780000
Epoch: 450 train_loss: 0.004113 val_loss: 0.751059 val_acc: 0.640000 test_loss: 0.597090 test_acc: 0.770000
Epoch: 460 train_loss: 0.004069 val_loss: 0.746671 val_acc: 0.640000 test_loss: 0.598136 test_acc: 0.780000
Epoch: 470 train_loss: 0.004152 val_loss: 0.743452 val_acc: 0.630000 test_loss: 0.597268 test_acc: 0.780000
Epoch: 480 train_loss: 0.004167 val_loss: 0.748059 val_acc: 0.630000 test_loss: 0.600027 test_acc: 0.780000
Epoch: 490 train_loss: 0.004072 val_loss: 0.765792 val_acc: 0.630000 test_loss: 0.601440 test_acc: 0.780000
Epoch: 500 train_loss: 0.004124 val_loss: 0.771042 val_acc: 0.630000 test_loss: 0.603747 test_acc: 0.760000
Epoch: 510 train_loss: 0.004083 val_loss: 0.772203 val_acc: 0.620000 test_loss: 0.591753 test_acc: 0.790000
Epoch: 520 train_loss: 0.004075 val_loss: 0.755120 val_acc: 0.630000 test_loss: 0.593302 test_acc: 0.780000
Early stop at epoch 523.
------------- 2 val_acc: 0.7400 test_acc: 0.7700 best_test: 0.7800 -----------------
Fold: 3
Epoch: 000 train_loss: 0.008523 val_loss: 0.728930 val_acc: 0.480000 test_loss: 0.734052 test_acc: 0.480000
Epoch: 010 train_loss: 0.005794 val_loss: 0.651038 val_acc: 0.730000 test_loss: 0.636757 test_acc: 0.780000
Epoch: 020 train_loss: 0.005202 val_loss: 0.596811 val_acc: 0.750000 test_loss: 0.580577 test_acc: 0.800000
Epoch: 030 train_loss: 0.005235 val_loss: 0.624091 val_acc: 0.740000 test_loss: 0.659055 test_acc: 0.690000
Epoch: 040 train_loss: 0.004969 val_loss: 0.593379 val_acc: 0.710000 test_loss: 0.588249 test_acc: 0.730000
Epoch: 050 train_loss: 0.004931 val_loss: 0.591508 val_acc: 0.710000 test_loss: 0.600890 test_acc: 0.710000
Epoch: 060 train_loss: 0.004832 val_loss: 0.578325 val_acc: 0.760000 test_loss: 0.552011 test_acc: 0.830000
Epoch: 070 train_loss: 0.004828 val_loss: 0.583821 val_acc: 0.740000 test_loss: 0.551855 test_acc: 0.840000
Epoch: 080 train_loss: 0.004749 val_loss: 0.583372 val_acc: 0.730000 test_loss: 0.550914 test_acc: 0.810000
Epoch: 090 train_loss: 0.004722 val_loss: 0.603308 val_acc: 0.700000 test_loss: 0.585229 test_acc: 0.750000
Epoch: 100 train_loss: 0.004634 val_loss: 0.590126 val_acc: 0.720000 test_loss: 0.545516 test_acc: 0.830000
Epoch: 110 train_loss: 0.004629 val_loss: 0.592520 val_acc: 0.750000 test_loss: 0.554008 test_acc: 0.800000
Epoch: 120 train_loss: 0.004601 val_loss: 0.584548 val_acc: 0.750000 test_loss: 0.557988 test_acc: 0.790000
Epoch: 130 train_loss: 0.004456 val_loss: 0.585294 val_acc: 0.760000 test_loss: 0.550128 test_acc: 0.820000
Epoch: 140 train_loss: 0.004503 val_loss: 0.592566 val_acc: 0.750000 test_loss: 0.581032 test_acc: 0.790000
Epoch: 150 train_loss: 0.004549 val_loss: 0.582309 val_acc: 0.770000 test_loss: 0.552356 test_acc: 0.820000
Epoch: 160 train_loss: 0.004446 val_loss: 0.585276 val_acc: 0.770000 test_loss: 0.547328 test_acc: 0.840000
Epoch: 170 train_loss: 0.004418 val_loss: 0.592520 val_acc: 0.770000 test_loss: 0.564958 test_acc: 0.820000
