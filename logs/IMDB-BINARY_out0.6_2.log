/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/dataset.py:214: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'dataset/TUDataset/IMDB-BINARY/processed' first
  warnings.warn(
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:1', dataset='IMDB-BINARY', in_size=136, num_classes=2, model_name='fusion_tm', fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=136, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=136, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.6, num_fusion_layers=6, eta=0.5, ffn_dim=128, num_trans_layers=3, lam1=0.2, lam2=0.2, theta1=0.15, theta2=0.4, theta3=0.4, gamma=0.7, patience=500, loss_log=2, folds=10, lr=1e-05, weight_decay=5e-05, batch_size=128, epoches=1000, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.005878 val_loss: 0.514131 val_acc: 0.520000 test_loss: 0.547107 test_acc: 0.490000
Epoch: 010 train_loss: 0.003694 val_loss: 0.433091 val_acc: 0.680000 test_loss: 0.443236 test_acc: 0.690000
Epoch: 020 train_loss: 0.003434 val_loss: 0.429928 val_acc: 0.670000 test_loss: 0.433103 test_acc: 0.680000
Epoch: 030 train_loss: 0.003314 val_loss: 0.432948 val_acc: 0.710000 test_loss: 0.434559 test_acc: 0.690000
Epoch: 040 train_loss: 0.003231 val_loss: 0.432072 val_acc: 0.680000 test_loss: 0.438312 test_acc: 0.710000
Epoch: 050 train_loss: 0.003174 val_loss: 0.448649 val_acc: 0.660000 test_loss: 0.438665 test_acc: 0.700000
Epoch: 060 train_loss: 0.003096 val_loss: 0.462349 val_acc: 0.690000 test_loss: 0.448392 test_acc: 0.690000
Epoch: 070 train_loss: 0.002985 val_loss: 0.444869 val_acc: 0.680000 test_loss: 0.468163 test_acc: 0.660000
Epoch: 080 train_loss: 0.003003 val_loss: 0.435787 val_acc: 0.720000 test_loss: 0.460179 test_acc: 0.670000
Epoch: 090 train_loss: 0.003006 val_loss: 0.438219 val_acc: 0.720000 test_loss: 0.495454 test_acc: 0.670000
Epoch: 100 train_loss: 0.002864 val_loss: 0.442461 val_acc: 0.720000 test_loss: 0.487989 test_acc: 0.670000
Epoch: 110 train_loss: 0.002786 val_loss: 0.431268 val_acc: 0.730000 test_loss: 0.513874 test_acc: 0.650000
Epoch: 120 train_loss: 0.002788 val_loss: 0.429306 val_acc: 0.740000 test_loss: 0.521075 test_acc: 0.650000
Epoch: 130 train_loss: 0.002749 val_loss: 0.449159 val_acc: 0.670000 test_loss: 0.496171 test_acc: 0.670000
Epoch: 140 train_loss: 0.002747 val_loss: 0.436016 val_acc: 0.760000 test_loss: 0.516258 test_acc: 0.640000
Epoch: 150 train_loss: 0.002672 val_loss: 0.433193 val_acc: 0.740000 test_loss: 0.535397 test_acc: 0.650000
Epoch: 160 train_loss: 0.002666 val_loss: 0.442763 val_acc: 0.700000 test_loss: 0.520346 test_acc: 0.660000
Epoch: 170 train_loss: 0.002704 val_loss: 0.435409 val_acc: 0.760000 test_loss: 0.512242 test_acc: 0.650000
Epoch: 180 train_loss: 0.002702 val_loss: 0.434381 val_acc: 0.740000 test_loss: 0.544461 test_acc: 0.670000
Epoch: 190 train_loss: 0.002683 val_loss: 0.446799 val_acc: 0.710000 test_loss: 0.569369 test_acc: 0.640000
Epoch: 200 train_loss: 0.002661 val_loss: 0.448929 val_acc: 0.720000 test_loss: 0.531478 test_acc: 0.650000
Epoch: 210 train_loss: 0.002607 val_loss: 0.450959 val_acc: 0.720000 test_loss: 0.523760 test_acc: 0.660000
Epoch: 220 train_loss: 0.002621 val_loss: 0.444888 val_acc: 0.710000 test_loss: 0.551783 test_acc: 0.640000
Epoch: 230 train_loss: 0.002514 val_loss: 0.449939 val_acc: 0.740000 test_loss: 0.541904 test_acc: 0.660000
Epoch: 240 train_loss: 0.002611 val_loss: 0.443648 val_acc: 0.720000 test_loss: 0.516389 test_acc: 0.660000
Epoch: 250 train_loss: 0.002584 val_loss: 0.445527 val_acc: 0.720000 test_loss: 0.534689 test_acc: 0.670000
Epoch: 260 train_loss: 0.002575 val_loss: 0.443039 val_acc: 0.700000 test_loss: 0.566780 test_acc: 0.650000
Epoch: 270 train_loss: 0.002577 val_loss: 0.442141 val_acc: 0.710000 test_loss: 0.540377 test_acc: 0.660000
Epoch: 280 train_loss: 0.002559 val_loss: 0.440741 val_acc: 0.720000 test_loss: 0.559040 test_acc: 0.650000
Epoch: 290 train_loss: 0.002524 val_loss: 0.444259 val_acc: 0.720000 test_loss: 0.550695 test_acc: 0.640000
Epoch: 300 train_loss: 0.002559 val_loss: 0.445563 val_acc: 0.720000 test_loss: 0.566050 test_acc: 0.640000
Epoch: 310 train_loss: 0.002516 val_loss: 0.447501 val_acc: 0.720000 test_loss: 0.551789 test_acc: 0.630000
Epoch: 320 train_loss: 0.002539 val_loss: 0.441417 val_acc: 0.720000 test_loss: 0.532172 test_acc: 0.670000
Epoch: 330 train_loss: 0.002564 val_loss: 0.449605 val_acc: 0.720000 test_loss: 0.566789 test_acc: 0.640000
Epoch: 340 train_loss: 0.002523 val_loss: 0.464366 val_acc: 0.710000 test_loss: 0.550169 test_acc: 0.650000
Epoch: 350 train_loss: 0.002506 val_loss: 0.445211 val_acc: 0.730000 test_loss: 0.560475 test_acc: 0.650000
Epoch: 360 train_loss: 0.002524 val_loss: 0.450639 val_acc: 0.710000 test_loss: 0.564733 test_acc: 0.650000
Epoch: 370 train_loss: 0.002527 val_loss: 0.446042 val_acc: 0.710000 test_loss: 0.584844 test_acc: 0.630000
Epoch: 380 train_loss: 0.002473 val_loss: 0.445478 val_acc: 0.710000 test_loss: 0.556299 test_acc: 0.640000
Epoch: 390 train_loss: 0.002454 val_loss: 0.442348 val_acc: 0.730000 test_loss: 0.552050 test_acc: 0.630000
Epoch: 400 train_loss: 0.002464 val_loss: 0.441038 val_acc: 0.720000 test_loss: 0.575081 test_acc: 0.640000
Epoch: 410 train_loss: 0.002500 val_loss: 0.446863 val_acc: 0.730000 test_loss: 0.560408 test_acc: 0.640000
Epoch: 420 train_loss: 0.002471 val_loss: 0.450048 val_acc: 0.720000 test_loss: 0.575192 test_acc: 0.640000
Epoch: 430 train_loss: 0.002536 val_loss: 0.452386 val_acc: 0.710000 test_loss: 0.555910 test_acc: 0.640000
Epoch: 440 train_loss: 0.002506 val_loss: 0.446025 val_acc: 0.720000 test_loss: 0.586870 test_acc: 0.630000
Epoch: 450 train_loss: 0.002440 val_loss: 0.445851 val_acc: 0.730000 test_loss: 0.560748 test_acc: 0.640000
Epoch: 460 train_loss: 0.002489 val_loss: 0.445172 val_acc: 0.720000 test_loss: 0.561733 test_acc: 0.630000
Epoch: 470 train_loss: 0.002458 val_loss: 0.444894 val_acc: 0.730000 test_loss: 0.572833 test_acc: 0.630000
Epoch: 480 train_loss: 0.002477 val_loss: 0.446867 val_acc: 0.720000 test_loss: 0.564261 test_acc: 0.650000
Epoch: 490 train_loss: 0.002470 val_loss: 0.446491 val_acc: 0.730000 test_loss: 0.562086 test_acc: 0.640000
Epoch: 500 train_loss: 0.002490 val_loss: 0.444619 val_acc: 0.720000 test_loss: 0.565322 test_acc: 0.630000
Epoch: 510 train_loss: 0.002539 val_loss: 0.451382 val_acc: 0.710000 test_loss: 0.563728 test_acc: 0.640000
Epoch: 520 train_loss: 0.002465 val_loss: 0.446891 val_acc: 0.730000 test_loss: 0.573791 test_acc: 0.620000
Epoch: 530 train_loss: 0.002476 val_loss: 0.450181 val_acc: 0.720000 test_loss: 0.578218 test_acc: 0.620000
Epoch: 540 train_loss: 0.002494 val_loss: 0.449584 val_acc: 0.720000 test_loss: 0.561007 test_acc: 0.640000
Epoch: 550 train_loss: 0.002471 val_loss: 0.448944 val_acc: 0.730000 test_loss: 0.565467 test_acc: 0.630000
Epoch: 560 train_loss: 0.002490 val_loss: 0.445769 val_acc: 0.730000 test_loss: 0.573770 test_acc: 0.620000
Epoch: 570 train_loss: 0.002497 val_loss: 0.455332 val_acc: 0.720000 test_loss: 0.553488 test_acc: 0.640000
Epoch: 580 train_loss: 0.002470 val_loss: 0.451626 val_acc: 0.720000 test_loss: 0.554258 test_acc: 0.640000
Epoch: 590 train_loss: 0.002467 val_loss: 0.447837 val_acc: 0.730000 test_loss: 0.571457 test_acc: 0.620000
Epoch: 600 train_loss: 0.002483 val_loss: 0.449211 val_acc: 0.730000 test_loss: 0.569945 test_acc: 0.630000
Epoch: 610 train_loss: 0.002469 val_loss: 0.449973 val_acc: 0.730000 test_loss: 0.559969 test_acc: 0.650000
Epoch: 620 train_loss: 0.002458 val_loss: 0.449538 val_acc: 0.730000 test_loss: 0.564827 test_acc: 0.640000
Epoch: 630 train_loss: 0.002510 val_loss: 0.445209 val_acc: 0.730000 test_loss: 0.578565 test_acc: 0.630000
Epoch: 640 train_loss: 0.002472 val_loss: 0.451813 val_acc: 0.730000 test_loss: 0.561246 test_acc: 0.640000
Epoch: 650 train_loss: 0.002502 val_loss: 0.452593 val_acc: 0.710000 test_loss: 0.562833 test_acc: 0.640000
Epoch: 660 train_loss: 0.002476 val_loss: 0.449486 val_acc: 0.720000 test_loss: 0.566462 test_acc: 0.630000
Epoch: 670 train_loss: 0.002483 val_loss: 0.449404 val_acc: 0.720000 test_loss: 0.558185 test_acc: 0.640000
Early stop at epoch 679.
------------- 0 val_acc: 0.7700 test_acc: 0.6500 best_test: 0.7100 -----------------
Fold: 1
Epoch: 000 train_loss: 0.005402 val_loss: 0.524586 val_acc: 0.490000 test_loss: 0.528992 test_acc: 0.570000
Epoch: 010 train_loss: 0.003439 val_loss: 0.445259 val_acc: 0.630000 test_loss: 0.428716 test_acc: 0.740000
Epoch: 020 train_loss: 0.003258 val_loss: 0.457584 val_acc: 0.660000 test_loss: 0.439285 test_acc: 0.720000
Epoch: 030 train_loss: 0.003174 val_loss: 0.463284 val_acc: 0.660000 test_loss: 0.434803 test_acc: 0.710000
Epoch: 040 train_loss: 0.003111 val_loss: 0.464535 val_acc: 0.670000 test_loss: 0.436047 test_acc: 0.720000
Epoch: 050 train_loss: 0.002979 val_loss: 0.491135 val_acc: 0.660000 test_loss: 0.454574 test_acc: 0.740000
Epoch: 060 train_loss: 0.002950 val_loss: 0.469839 val_acc: 0.660000 test_loss: 0.436409 test_acc: 0.720000
Epoch: 070 train_loss: 0.002969 val_loss: 0.473617 val_acc: 0.650000 test_loss: 0.451491 test_acc: 0.680000
Epoch: 080 train_loss: 0.002942 val_loss: 0.474788 val_acc: 0.670000 test_loss: 0.460376 test_acc: 0.700000
Epoch: 090 train_loss: 0.003000 val_loss: 0.474442 val_acc: 0.700000 test_loss: 0.443825 test_acc: 0.740000
Epoch: 100 train_loss: 0.002919 val_loss: 0.478028 val_acc: 0.720000 test_loss: 0.443679 test_acc: 0.780000
Epoch: 110 train_loss: 0.002708 val_loss: 0.485168 val_acc: 0.680000 test_loss: 0.460835 test_acc: 0.730000
Epoch: 120 train_loss: 0.002719 val_loss: 0.507771 val_acc: 0.660000 test_loss: 0.449152 test_acc: 0.760000
Epoch: 130 train_loss: 0.002688 val_loss: 0.499588 val_acc: 0.660000 test_loss: 0.490111 test_acc: 0.730000
Epoch: 140 train_loss: 0.002681 val_loss: 0.520037 val_acc: 0.670000 test_loss: 0.495618 test_acc: 0.740000
Epoch: 150 train_loss: 0.002704 val_loss: 0.501208 val_acc: 0.700000 test_loss: 0.519561 test_acc: 0.720000
Epoch: 160 train_loss: 0.002633 val_loss: 0.524678 val_acc: 0.670000 test_loss: 0.483168 test_acc: 0.740000
