Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-MULTI.zip
Extracting dataset/TUDataset/IMDB-MULTI/IMDB-MULTI.zip
Processing...
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
  0%|          | 0/1500 [00:00<?, ?it/s]100%|██████████| 1500/1500 [00:00<00:00, 23657.52it/s]
Done!
Namespace(device='cuda:0', dataset='IMDB-MULTI', in_size=89, num_classes=3, fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=89, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=89, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=6, eta=0.5, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=3)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.009570 val_loss: 1.111341 val_acc: 0.306667 test_loss: 1.109384 test_acc: 0.286667
Epoch: 010 train_loss: 0.008566 val_loss: 1.061851 val_acc: 0.453333 test_loss: 1.076870 test_acc: 0.453333
Epoch: 020 train_loss: 0.008605 val_loss: 1.066486 val_acc: 0.453333 test_loss: 1.074464 test_acc: 0.473333
Epoch: 030 train_loss: 0.008294 val_loss: 1.073434 val_acc: 0.453333 test_loss: 1.084040 test_acc: 0.473333
Epoch: 040 train_loss: 0.008191 val_loss: 1.096783 val_acc: 0.400000 test_loss: 1.077844 test_acc: 0.446667
Epoch: 050 train_loss: 0.008217 val_loss: 1.085396 val_acc: 0.440000 test_loss: 1.091146 test_acc: 0.466667
Epoch: 060 train_loss: 0.008052 val_loss: 1.099899 val_acc: 0.446667 test_loss: 1.078148 test_acc: 0.453333
Epoch: 070 train_loss: 0.008038 val_loss: 1.095903 val_acc: 0.480000 test_loss: 1.098645 test_acc: 0.480000
Epoch: 080 train_loss: 0.008031 val_loss: 1.096153 val_acc: 0.446667 test_loss: 1.095462 test_acc: 0.460000
Epoch: 090 train_loss: 0.008137 val_loss: 1.085879 val_acc: 0.426667 test_loss: 1.084663 test_acc: 0.453333
Epoch: 100 train_loss: 0.007934 val_loss: 1.090796 val_acc: 0.440000 test_loss: 1.093021 test_acc: 0.453333
Epoch: 110 train_loss: 0.008048 val_loss: 1.100417 val_acc: 0.440000 test_loss: 1.134658 test_acc: 0.440000
Epoch: 120 train_loss: 0.007921 val_loss: 1.072268 val_acc: 0.460000 test_loss: 1.075266 test_acc: 0.493333
Epoch: 130 train_loss: 0.007936 val_loss: 1.070774 val_acc: 0.493333 test_loss: 1.144513 test_acc: 0.473333
Epoch: 140 train_loss: 0.007828 val_loss: 1.123907 val_acc: 0.420000 test_loss: 1.097951 test_acc: 0.460000
Epoch: 150 train_loss: 0.007776 val_loss: 1.103033 val_acc: 0.426667 test_loss: 1.090776 test_acc: 0.486667
Epoch: 160 train_loss: 0.007672 val_loss: 1.115314 val_acc: 0.433333 test_loss: 1.110105 test_acc: 0.480000
Epoch: 170 train_loss: 0.008028 val_loss: 1.116709 val_acc: 0.420000 test_loss: 1.128848 test_acc: 0.473333
Epoch: 180 train_loss: 0.007950 val_loss: 1.084220 val_acc: 0.453333 test_loss: 1.073746 test_acc: 0.473333
Epoch: 190 train_loss: 0.007819 val_loss: 1.086332 val_acc: 0.453333 test_loss: 1.079178 test_acc: 0.493333
Epoch: 200 train_loss: 0.007862 val_loss: 1.096843 val_acc: 0.433333 test_loss: 1.154977 test_acc: 0.433333
Epoch: 210 train_loss: 0.007764 val_loss: 1.099981 val_acc: 0.466667 test_loss: 1.124018 test_acc: 0.486667
Epoch: 220 train_loss: 0.007699 val_loss: 1.109621 val_acc: 0.440000 test_loss: 1.117164 test_acc: 0.486667
Epoch: 230 train_loss: 0.007936 val_loss: 1.079810 val_acc: 0.466667 test_loss: 1.074747 test_acc: 0.500000
Epoch: 240 train_loss: 0.007823 val_loss: 1.075771 val_acc: 0.460000 test_loss: 1.078546 test_acc: 0.493333
Epoch: 250 train_loss: 0.007733 val_loss: 1.096759 val_acc: 0.446667 test_loss: 1.101983 test_acc: 0.473333
Epoch: 260 train_loss: 0.007697 val_loss: 1.117822 val_acc: 0.433333 test_loss: 1.100887 test_acc: 0.480000
Epoch: 270 train_loss: 0.007698 val_loss: 1.107055 val_acc: 0.433333 test_loss: 1.090940 test_acc: 0.486667
Epoch: 280 train_loss: 0.007725 val_loss: 1.133030 val_acc: 0.433333 test_loss: 1.102193 test_acc: 0.480000
Epoch: 290 train_loss: 0.007868 val_loss: 1.092765 val_acc: 0.433333 test_loss: 1.063719 test_acc: 0.493333
Epoch: 300 train_loss: 0.007774 val_loss: 1.087633 val_acc: 0.446667 test_loss: 1.098666 test_acc: 0.473333
Epoch: 310 train_loss: 0.007917 val_loss: 1.085043 val_acc: 0.460000 test_loss: 1.092122 test_acc: 0.506667
Epoch: 320 train_loss: 0.007863 val_loss: 1.087617 val_acc: 0.466667 test_loss: 1.109394 test_acc: 0.473333
Epoch: 330 train_loss: 0.007703 val_loss: 1.099593 val_acc: 0.433333 test_loss: 1.111306 test_acc: 0.473333
Epoch: 340 train_loss: 0.007700 val_loss: 1.098092 val_acc: 0.446667 test_loss: 1.109209 test_acc: 0.473333
Epoch: 350 train_loss: 0.007705 val_loss: 1.096765 val_acc: 0.453333 test_loss: 1.134579 test_acc: 0.460000
Epoch: 360 train_loss: 0.007682 val_loss: 1.096457 val_acc: 0.440000 test_loss: 1.118839 test_acc: 0.460000
Epoch: 370 train_loss: 0.007647 val_loss: 1.102393 val_acc: 0.440000 test_loss: 1.116075 test_acc: 0.466667
Epoch: 380 train_loss: 0.007702 val_loss: 1.099825 val_acc: 0.433333 test_loss: 1.128942 test_acc: 0.473333
Epoch: 390 train_loss: 0.007614 val_loss: 1.113320 val_acc: 0.426667 test_loss: 1.141668 test_acc: 0.466667
Epoch: 400 train_loss: 0.007657 val_loss: 1.093776 val_acc: 0.440000 test_loss: 1.128907 test_acc: 0.473333
Epoch: 410 train_loss: 0.007647 val_loss: 1.102392 val_acc: 0.433333 test_loss: 1.123754 test_acc: 0.460000
Epoch: 420 train_loss: 0.007662 val_loss: 1.123966 val_acc: 0.433333 test_loss: 1.134335 test_acc: 0.460000
Epoch: 430 train_loss: 0.007668 val_loss: 1.110037 val_acc: 0.433333 test_loss: 1.158138 test_acc: 0.446667
Epoch: 440 train_loss: 0.007650 val_loss: 1.093683 val_acc: 0.440000 test_loss: 1.104169 test_acc: 0.480000
Epoch: 450 train_loss: 0.007661 val_loss: 1.104338 val_acc: 0.426667 test_loss: 1.130165 test_acc: 0.480000
Epoch: 460 train_loss: 0.007967 val_loss: 1.096543 val_acc: 0.440000 test_loss: 1.111514 test_acc: 0.466667
Epoch: 470 train_loss: 0.007954 val_loss: 1.089360 val_acc: 0.446667 test_loss: 1.049790 test_acc: 0.493333
Epoch: 480 train_loss: 0.007740 val_loss: 1.099008 val_acc: 0.446667 test_loss: 1.061646 test_acc: 0.513333
Epoch: 490 train_loss: 0.007804 val_loss: 1.075826 val_acc: 0.440000 test_loss: 1.076199 test_acc: 0.466667
Epoch: 500 train_loss: 0.007702 val_loss: 1.094427 val_acc: 0.440000 test_loss: 1.076668 test_acc: 0.493333
Epoch: 510 train_loss: 0.007665 val_loss: 1.099357 val_acc: 0.440000 test_loss: 1.099444 test_acc: 0.480000
Epoch: 520 train_loss: 0.007644 val_loss: 1.094516 val_acc: 0.446667 test_loss: 1.096819 test_acc: 0.486667
Epoch: 530 train_loss: 0.007710 val_loss: 1.105131 val_acc: 0.440000 test_loss: 1.105837 test_acc: 0.466667
Epoch: 540 train_loss: 0.007631 val_loss: 1.098964 val_acc: 0.433333 test_loss: 1.100498 test_acc: 0.480000
Epoch: 550 train_loss: 0.007628 val_loss: 1.101006 val_acc: 0.426667 test_loss: 1.101336 test_acc: 0.486667
Epoch: 560 train_loss: 0.007583 val_loss: 1.104141 val_acc: 0.440000 test_loss: 1.101231 test_acc: 0.480000
Epoch: 570 train_loss: 0.007639 val_loss: 1.096904 val_acc: 0.446667 test_loss: 1.108652 test_acc: 0.480000
Epoch: 580 train_loss: 0.007645 val_loss: 1.114771 val_acc: 0.433333 test_loss: 1.112457 test_acc: 0.486667
Epoch: 590 train_loss: 0.008002 val_loss: 1.089513 val_acc: 0.413333 test_loss: 1.062446 test_acc: 0.486667
Epoch: 600 train_loss: 0.007877 val_loss: 1.095205 val_acc: 0.433333 test_loss: 1.071557 test_acc: 0.500000
Epoch: 610 train_loss: 0.007825 val_loss: 1.075011 val_acc: 0.473333 test_loss: 1.054227 test_acc: 0.493333
Epoch: 620 train_loss: 0.007745 val_loss: 1.085675 val_acc: 0.440000 test_loss: 1.074627 test_acc: 0.486667
Epoch: 630 train_loss: 0.007680 val_loss: 1.100152 val_acc: 0.440000 test_loss: 1.081569 test_acc: 0.486667
Epoch: 640 train_loss: 0.007670 val_loss: 1.098782 val_acc: 0.446667 test_loss: 1.079561 test_acc: 0.466667
Epoch: 650 train_loss: 0.007622 val_loss: 1.093765 val_acc: 0.440000 test_loss: 1.084840 test_acc: 0.486667
Epoch: 660 train_loss: 0.007635 val_loss: 1.098519 val_acc: 0.446667 test_loss: 1.101067 test_acc: 0.460000
Epoch: 670 train_loss: 0.008284 val_loss: 1.092605 val_acc: 0.440000 test_loss: 1.066076 test_acc: 0.526667
Epoch: 680 train_loss: 0.007834 val_loss: 1.098928 val_acc: 0.420000 test_loss: 1.058238 test_acc: 0.473333
Epoch: 690 train_loss: 0.007865 val_loss: 1.110825 val_acc: 0.440000 test_loss: 1.083555 test_acc: 0.500000
Epoch: 700 train_loss: 0.007746 val_loss: 1.092970 val_acc: 0.446667 test_loss: 1.120668 test_acc: 0.440000
Epoch: 710 train_loss: 0.007641 val_loss: 1.103078 val_acc: 0.440000 test_loss: 1.101971 test_acc: 0.480000
Epoch: 720 train_loss: 0.007645 val_loss: 1.104399 val_acc: 0.453333 test_loss: 1.103316 test_acc: 0.473333
Epoch: 730 train_loss: 0.007624 val_loss: 1.107280 val_acc: 0.446667 test_loss: 1.110025 test_acc: 0.473333
Epoch: 740 train_loss: 0.007626 val_loss: 1.099948 val_acc: 0.440000 test_loss: 1.101005 test_acc: 0.473333
Epoch: 750 train_loss: 0.007616 val_loss: 1.113517 val_acc: 0.453333 test_loss: 1.111661 test_acc: 0.466667
Epoch: 760 train_loss: 0.007659 val_loss: 1.106553 val_acc: 0.440000 test_loss: 1.097631 test_acc: 0.480000
Epoch: 770 train_loss: 0.007756 val_loss: 1.120169 val_acc: 0.426667 test_loss: 1.136403 test_acc: 0.453333
Epoch: 780 train_loss: 0.007939 val_loss: 1.076194 val_acc: 0.453333 test_loss: 1.095075 test_acc: 0.440000
Epoch: 790 train_loss: 0.007792 val_loss: 1.125021 val_acc: 0.426667 test_loss: 1.099723 test_acc: 0.466667
------------- 0 val_acc: 0.5067 test_acc: 0.5267 best_test: 0.5400 -----------------
Fold: 1
Epoch: 000 train_loss: 0.009496 val_loss: 1.103589 val_acc: 0.293333 test_loss: 1.096718 test_acc: 0.406667
Epoch: 010 train_loss: 0.008662 val_loss: 1.056900 val_acc: 0.480000 test_loss: 1.044730 test_acc: 0.486667
Epoch: 020 train_loss: 0.008501 val_loss: 1.057869 val_acc: 0.480000 test_loss: 1.037149 test_acc: 0.466667
Epoch: 030 train_loss: 0.008510 val_loss: 1.069464 val_acc: 0.506667 test_loss: 1.023251 test_acc: 0.533333
Epoch: 040 train_loss: 0.008339 val_loss: 1.062734 val_acc: 0.466667 test_loss: 1.030158 test_acc: 0.480000
Epoch: 050 train_loss: 0.008339 val_loss: 1.040809 val_acc: 0.526667 test_loss: 1.025535 test_acc: 0.486667
Epoch: 060 train_loss: 0.008264 val_loss: 1.055580 val_acc: 0.506667 test_loss: 1.028077 test_acc: 0.486667
Epoch: 070 train_loss: 0.008296 val_loss: 1.056383 val_acc: 0.506667 test_loss: 1.048735 test_acc: 0.513333
Epoch: 080 train_loss: 0.008212 val_loss: 1.072749 val_acc: 0.460000 test_loss: 1.048380 test_acc: 0.466667
Epoch: 090 train_loss: 0.008072 val_loss: 1.076765 val_acc: 0.493333 test_loss: 1.062773 test_acc: 0.466667
Epoch: 100 train_loss: 0.008178 val_loss: 1.119676 val_acc: 0.460000 test_loss: 1.090838 test_acc: 0.433333
Epoch: 110 train_loss: 0.008044 val_loss: 1.090210 val_acc: 0.466667 test_loss: 1.093280 test_acc: 0.446667
Epoch: 120 train_loss: 0.007894 val_loss: 1.077321 val_acc: 0.500000 test_loss: 1.069869 test_acc: 0.486667
Epoch: 130 train_loss: 0.007914 val_loss: 1.107684 val_acc: 0.486667 test_loss: 1.093498 test_acc: 0.446667
Epoch: 140 train_loss: 0.007869 val_loss: 1.073092 val_acc: 0.520000 test_loss: 1.086276 test_acc: 0.460000
Epoch: 150 train_loss: 0.008094 val_loss: 1.124489 val_acc: 0.426667 test_loss: 1.062445 test_acc: 0.453333
Epoch: 160 train_loss: 0.007916 val_loss: 1.083553 val_acc: 0.506667 test_loss: 1.081928 test_acc: 0.506667
Epoch: 170 train_loss: 0.007918 val_loss: 1.095407 val_acc: 0.513333 test_loss: 1.087493 test_acc: 0.480000
Epoch: 180 train_loss: 0.007829 val_loss: 1.064131 val_acc: 0.520000 test_loss: 1.091021 test_acc: 0.486667
Epoch: 190 train_loss: 0.007876 val_loss: 1.056035 val_acc: 0.493333 test_loss: 1.083241 test_acc: 0.446667
Epoch: 200 train_loss: 0.007788 val_loss: 1.061172 val_acc: 0.506667 test_loss: 1.103531 test_acc: 0.453333
Epoch: 210 train_loss: 0.007901 val_loss: 1.064245 val_acc: 0.493333 test_loss: 1.115040 test_acc: 0.433333
Epoch: 220 train_loss: 0.007820 val_loss: 1.076093 val_acc: 0.513333 test_loss: 1.105840 test_acc: 0.473333
Epoch: 230 train_loss: 0.007751 val_loss: 1.068250 val_acc: 0.513333 test_loss: 1.129157 test_acc: 0.440000
Epoch: 240 train_loss: 0.007825 val_loss: 1.068725 val_acc: 0.520000 test_loss: 1.121454 test_acc: 0.460000
Epoch: 250 train_loss: 0.007734 val_loss: 1.070892 val_acc: 0.520000 test_loss: 1.117899 test_acc: 0.466667
Epoch: 260 train_loss: 0.007884 val_loss: 1.079384 val_acc: 0.513333 test_loss: 1.106152 test_acc: 0.466667
Epoch: 270 train_loss: 0.007878 val_loss: 1.061515 val_acc: 0.520000 test_loss: 1.070538 test_acc: 0.453333
Epoch: 280 train_loss: 0.007780 val_loss: 1.077574 val_acc: 0.506667 test_loss: 1.100799 test_acc: 0.460000
Epoch: 290 train_loss: 0.007791 val_loss: 1.086662 val_acc: 0.500000 test_loss: 1.111443 test_acc: 0.446667
Epoch: 300 train_loss: 0.007832 val_loss: 1.112437 val_acc: 0.500000 test_loss: 1.135512 test_acc: 0.433333
Epoch: 310 train_loss: 0.007903 val_loss: 1.110072 val_acc: 0.493333 test_loss: 1.099211 test_acc: 0.433333
Epoch: 320 train_loss: 0.007956 val_loss: 1.043633 val_acc: 0.513333 test_loss: 1.115075 test_acc: 0.433333
Epoch: 330 train_loss: 0.007851 val_loss: 1.065393 val_acc: 0.533333 test_loss: 1.105622 test_acc: 0.440000
Epoch: 340 train_loss: 0.007740 val_loss: 1.067727 val_acc: 0.533333 test_loss: 1.113989 test_acc: 0.460000
Epoch: 350 train_loss: 0.007769 val_loss: 1.079965 val_acc: 0.520000 test_loss: 1.119109 test_acc: 0.433333
Epoch: 360 train_loss: 0.007722 val_loss: 1.076994 val_acc: 0.526667 test_loss: 1.107280 test_acc: 0.460000
Epoch: 370 train_loss: 0.007689 val_loss: 1.078422 val_acc: 0.520000 test_loss: 1.111970 test_acc: 0.466667
Epoch: 380 train_loss: 0.007960 val_loss: 1.057679 val_acc: 0.506667 test_loss: 1.071884 test_acc: 0.426667
Epoch: 390 train_loss: 0.008123 val_loss: 1.078157 val_acc: 0.486667 test_loss: 1.089126 test_acc: 0.466667
Epoch: 400 train_loss: 0.008045 val_loss: 1.068226 val_acc: 0.533333 test_loss: 1.080582 test_acc: 0.460000
Epoch: 410 train_loss: 0.007762 val_loss: 1.085376 val_acc: 0.506667 test_loss: 1.091293 test_acc: 0.480000
Epoch: 420 train_loss: 0.007771 val_loss: 1.081547 val_acc: 0.506667 test_loss: 1.099173 test_acc: 0.473333
Epoch: 430 train_loss: 0.007722 val_loss: 1.112257 val_acc: 0.480000 test_loss: 1.105423 test_acc: 0.460000
Epoch: 440 train_loss: 0.007743 val_loss: 1.100094 val_acc: 0.486667 test_loss: 1.109847 test_acc: 0.453333
Epoch: 450 train_loss: 0.007784 val_loss: 1.070908 val_acc: 0.500000 test_loss: 1.088628 test_acc: 0.480000
Epoch: 460 train_loss: 0.007683 val_loss: 1.102026 val_acc: 0.500000 test_loss: 1.113180 test_acc: 0.446667
Epoch: 470 train_loss: 0.008393 val_loss: 1.055664 val_acc: 0.513333 test_loss: 1.064224 test_acc: 0.473333
Epoch: 480 train_loss: 0.008278 val_loss: 1.052180 val_acc: 0.480000 test_loss: 1.051144 test_acc: 0.500000
Epoch: 490 train_loss: 0.007849 val_loss: 1.108580 val_acc: 0.500000 test_loss: 1.110469 test_acc: 0.433333
Epoch: 500 train_loss: 0.007763 val_loss: 1.119722 val_acc: 0.500000 test_loss: 1.102844 test_acc: 0.440000
Epoch: 510 train_loss: 0.007754 val_loss: 1.088159 val_acc: 0.500000 test_loss: 1.107311 test_acc: 0.453333
Epoch: 520 train_loss: 0.007786 val_loss: 1.107628 val_acc: 0.506667 test_loss: 1.138621 test_acc: 0.460000
Epoch: 530 train_loss: 0.007687 val_loss: 1.099880 val_acc: 0.500000 test_loss: 1.118836 test_acc: 0.460000
Epoch: 540 train_loss: 0.007700 val_loss: 1.098840 val_acc: 0.500000 test_loss: 1.106696 test_acc: 0.453333
Epoch: 550 train_loss: 0.007690 val_loss: 1.087516 val_acc: 0.493333 test_loss: 1.108089 test_acc: 0.460000
Epoch: 560 train_loss: 0.007693 val_loss: 1.069161 val_acc: 0.506667 test_loss: 1.101962 test_acc: 0.446667
Epoch: 570 train_loss: 0.007702 val_loss: 1.104554 val_acc: 0.500000 test_loss: 1.135516 test_acc: 0.426667
Epoch: 580 train_loss: 0.007769 val_loss: 1.092219 val_acc: 0.506667 test_loss: 1.134106 test_acc: 0.446667
Epoch: 590 train_loss: 0.007695 val_loss: 1.076193 val_acc: 0.520000 test_loss: 1.103454 test_acc: 0.453333
Epoch: 600 train_loss: 0.007729 val_loss: 1.076727 val_acc: 0.513333 test_loss: 1.111457 test_acc: 0.460000
Epoch: 610 train_loss: 0.007689 val_loss: 1.088171 val_acc: 0.520000 test_loss: 1.108228 test_acc: 0.440000
Epoch: 620 train_loss: 0.007798 val_loss: 1.089732 val_acc: 0.500000 test_loss: 1.096112 test_acc: 0.480000
Epoch: 630 train_loss: 0.008285 val_loss: 1.046140 val_acc: 0.506667 test_loss: 1.052492 test_acc: 0.460000
Epoch: 640 train_loss: 0.008190 val_loss: 1.091441 val_acc: 0.500000 test_loss: 1.085777 test_acc: 0.460000
Epoch: 650 train_loss: 0.008186 val_loss: 1.097336 val_acc: 0.433333 test_loss: 1.076532 test_acc: 0.440000
Epoch: 660 train_loss: 0.007883 val_loss: 1.088893 val_acc: 0.513333 test_loss: 1.104312 test_acc: 0.440000
Epoch: 670 train_loss: 0.007709 val_loss: 1.056085 val_acc: 0.533333 test_loss: 1.087830 test_acc: 0.453333
Epoch: 680 train_loss: 0.007697 val_loss: 1.082466 val_acc: 0.526667 test_loss: 1.108290 test_acc: 0.473333
Epoch: 690 train_loss: 0.007717 val_loss: 1.073591 val_acc: 0.526667 test_loss: 1.094807 test_acc: 0.466667
Epoch: 700 train_loss: 0.007699 val_loss: 1.077247 val_acc: 0.526667 test_loss: 1.103548 test_acc: 0.480000
Epoch: 710 train_loss: 0.007707 val_loss: 1.072445 val_acc: 0.526667 test_loss: 1.095466 test_acc: 0.466667
Epoch: 720 train_loss: 0.007684 val_loss: 1.069147 val_acc: 0.533333 test_loss: 1.101637 test_acc: 0.466667
Epoch: 730 train_loss: 0.007683 val_loss: 1.070327 val_acc: 0.526667 test_loss: 1.089499 test_acc: 0.466667
Epoch: 740 train_loss: 0.007687 val_loss: 1.080541 val_acc: 0.520000 test_loss: 1.098723 test_acc: 0.460000
Epoch: 750 train_loss: 0.007692 val_loss: 1.072975 val_acc: 0.526667 test_loss: 1.091544 test_acc: 0.466667
Epoch: 760 train_loss: 0.007675 val_loss: 1.083293 val_acc: 0.520000 test_loss: 1.107545 test_acc: 0.460000
Epoch: 770 train_loss: 0.007972 val_loss: 1.086164 val_acc: 0.506667 test_loss: 1.109374 test_acc: 0.440000
Epoch: 780 train_loss: 0.007930 val_loss: 1.114488 val_acc: 0.446667 test_loss: 1.102623 test_acc: 0.413333
Epoch: 790 train_loss: 0.008054 val_loss: 1.071835 val_acc: 0.500000 test_loss: 1.086567 test_acc: 0.413333
------------- 1 val_acc: 0.5467 test_acc: 0.4800 best_test: 0.5467 -----------------
Fold: 2
Epoch: 000 train_loss: 0.009266 val_loss: 1.096526 val_acc: 0.400000 test_loss: 1.099890 test_acc: 0.400000
Epoch: 010 train_loss: 0.008709 val_loss: 1.039203 val_acc: 0.500000 test_loss: 1.057134 test_acc: 0.520000
Epoch: 020 train_loss: 0.008754 val_loss: 1.040836 val_acc: 0.486667 test_loss: 1.059864 test_acc: 0.513333
Epoch: 030 train_loss: 0.008434 val_loss: 1.040716 val_acc: 0.506667 test_loss: 1.043145 test_acc: 0.540000
Epoch: 040 train_loss: 0.008423 val_loss: 1.050745 val_acc: 0.466667 test_loss: 1.038187 test_acc: 0.520000
Epoch: 050 train_loss: 0.008350 val_loss: 1.055048 val_acc: 0.460000 test_loss: 1.065011 test_acc: 0.466667
Epoch: 060 train_loss: 0.008259 val_loss: 1.068337 val_acc: 0.466667 test_loss: 1.079369 test_acc: 0.520000
Epoch: 070 train_loss: 0.008281 val_loss: 1.063567 val_acc: 0.440000 test_loss: 1.029484 test_acc: 0.553333
Epoch: 080 train_loss: 0.008155 val_loss: 1.061589 val_acc: 0.466667 test_loss: 1.049498 test_acc: 0.513333
Epoch: 090 train_loss: 0.008140 val_loss: 1.074991 val_acc: 0.426667 test_loss: 1.052628 test_acc: 0.506667
Epoch: 100 train_loss: 0.008128 val_loss: 1.133265 val_acc: 0.420000 test_loss: 1.128244 test_acc: 0.493333
Epoch: 110 train_loss: 0.008168 val_loss: 1.083254 val_acc: 0.453333 test_loss: 1.083909 test_acc: 0.533333
Epoch: 120 train_loss: 0.008014 val_loss: 1.084044 val_acc: 0.433333 test_loss: 1.065137 test_acc: 0.526667
Epoch: 130 train_loss: 0.007995 val_loss: 1.089028 val_acc: 0.453333 test_loss: 1.058408 test_acc: 0.513333
Epoch: 140 train_loss: 0.008060 val_loss: 1.077826 val_acc: 0.480000 test_loss: 1.092647 test_acc: 0.540000
Epoch: 150 train_loss: 0.007932 val_loss: 1.088840 val_acc: 0.453333 test_loss: 1.100621 test_acc: 0.520000
Epoch: 160 train_loss: 0.007946 val_loss: 1.107621 val_acc: 0.433333 test_loss: 1.132069 test_acc: 0.506667
Epoch: 170 train_loss: 0.007832 val_loss: 1.110354 val_acc: 0.426667 test_loss: 1.125236 test_acc: 0.480000
Epoch: 180 train_loss: 0.007856 val_loss: 1.131886 val_acc: 0.453333 test_loss: 1.116823 test_acc: 0.520000
Epoch: 190 train_loss: 0.007989 val_loss: 1.067434 val_acc: 0.480000 test_loss: 1.056220 test_acc: 0.520000
Epoch: 200 train_loss: 0.007877 val_loss: 1.095853 val_acc: 0.493333 test_loss: 1.086643 test_acc: 0.540000
Epoch: 210 train_loss: 0.007972 val_loss: 1.083426 val_acc: 0.446667 test_loss: 1.110668 test_acc: 0.473333
Epoch: 220 train_loss: 0.007745 val_loss: 1.106895 val_acc: 0.460000 test_loss: 1.113330 test_acc: 0.506667
Epoch: 230 train_loss: 0.007781 val_loss: 1.117527 val_acc: 0.453333 test_loss: 1.109492 test_acc: 0.520000
Epoch: 240 train_loss: 0.007767 val_loss: 1.125795 val_acc: 0.433333 test_loss: 1.122789 test_acc: 0.513333
Epoch: 250 train_loss: 0.007723 val_loss: 1.127273 val_acc: 0.426667 test_loss: 1.127486 test_acc: 0.500000
Epoch: 260 train_loss: 0.007773 val_loss: 1.132943 val_acc: 0.446667 test_loss: 1.130405 test_acc: 0.500000
Epoch: 270 train_loss: 0.007735 val_loss: 1.109360 val_acc: 0.440000 test_loss: 1.091165 test_acc: 0.513333
Epoch: 280 train_loss: 0.007716 val_loss: 1.139980 val_acc: 0.433333 test_loss: 1.149845 test_acc: 0.486667
Epoch: 290 train_loss: 0.008089 val_loss: 1.081793 val_acc: 0.480000 test_loss: 1.069694 test_acc: 0.540000
Epoch: 300 train_loss: 0.007883 val_loss: 1.113776 val_acc: 0.446667 test_loss: 1.079115 test_acc: 0.500000
Epoch: 310 train_loss: 0.007883 val_loss: 1.127377 val_acc: 0.460000 test_loss: 1.089283 test_acc: 0.526667
Epoch: 320 train_loss: 0.007775 val_loss: 1.134667 val_acc: 0.413333 test_loss: 1.119162 test_acc: 0.493333
Epoch: 330 train_loss: 0.007741 val_loss: 1.126840 val_acc: 0.433333 test_loss: 1.101402 test_acc: 0.500000
Epoch: 340 train_loss: 0.007755 val_loss: 1.120765 val_acc: 0.440000 test_loss: 1.085964 test_acc: 0.520000
Epoch: 350 train_loss: 0.007711 val_loss: 1.123646 val_acc: 0.446667 test_loss: 1.086573 test_acc: 0.513333
Epoch: 360 train_loss: 0.007709 val_loss: 1.120534 val_acc: 0.446667 test_loss: 1.090756 test_acc: 0.520000
Epoch: 370 train_loss: 0.007736 val_loss: 1.137294 val_acc: 0.440000 test_loss: 1.093907 test_acc: 0.513333
Epoch: 380 train_loss: 0.007713 val_loss: 1.124876 val_acc: 0.426667 test_loss: 1.090737 test_acc: 0.513333
Epoch: 390 train_loss: 0.007742 val_loss: 1.129672 val_acc: 0.440000 test_loss: 1.096775 test_acc: 0.480000
Epoch: 400 train_loss: 0.007694 val_loss: 1.150839 val_acc: 0.440000 test_loss: 1.096033 test_acc: 0.533333
Epoch: 410 train_loss: 0.007701 val_loss: 1.141161 val_acc: 0.433333 test_loss: 1.099269 test_acc: 0.513333
Epoch: 420 train_loss: 0.007712 val_loss: 1.127518 val_acc: 0.453333 test_loss: 1.129164 test_acc: 0.473333
Epoch: 430 train_loss: 0.007903 val_loss: 1.103987 val_acc: 0.426667 test_loss: 1.076194 test_acc: 0.533333
Epoch: 440 train_loss: 0.008164 val_loss: 1.064492 val_acc: 0.460000 test_loss: 1.071145 test_acc: 0.506667
Epoch: 450 train_loss: 0.008049 val_loss: 1.096751 val_acc: 0.446667 test_loss: 1.070950 test_acc: 0.500000
Epoch: 460 train_loss: 0.007812 val_loss: 1.127340 val_acc: 0.433333 test_loss: 1.078899 test_acc: 0.520000
Epoch: 470 train_loss: 0.007754 val_loss: 1.117087 val_acc: 0.433333 test_loss: 1.094321 test_acc: 0.480000
Epoch: 480 train_loss: 0.007821 val_loss: 1.126495 val_acc: 0.440000 test_loss: 1.092893 test_acc: 0.533333
Epoch: 490 train_loss: 0.007834 val_loss: 1.129884 val_acc: 0.440000 test_loss: 1.089321 test_acc: 0.506667
Epoch: 500 train_loss: 0.007750 val_loss: 1.147063 val_acc: 0.433333 test_loss: 1.096655 test_acc: 0.500000
Epoch: 510 train_loss: 0.007735 val_loss: 1.132971 val_acc: 0.413333 test_loss: 1.087326 test_acc: 0.513333
Epoch: 520 train_loss: 0.007725 val_loss: 1.139840 val_acc: 0.453333 test_loss: 1.094464 test_acc: 0.506667
Epoch: 530 train_loss: 0.007693 val_loss: 1.150859 val_acc: 0.426667 test_loss: 1.093514 test_acc: 0.520000
Epoch: 540 train_loss: 0.007688 val_loss: 1.149241 val_acc: 0.426667 test_loss: 1.107368 test_acc: 0.500000
Epoch: 550 train_loss: 0.007710 val_loss: 1.137362 val_acc: 0.420000 test_loss: 1.090918 test_acc: 0.486667
Epoch: 560 train_loss: 0.007685 val_loss: 1.145631 val_acc: 0.426667 test_loss: 1.091615 test_acc: 0.493333
Epoch: 570 train_loss: 0.007717 val_loss: 1.146569 val_acc: 0.433333 test_loss: 1.099594 test_acc: 0.506667
Epoch: 580 train_loss: 0.007708 val_loss: 1.136151 val_acc: 0.440000 test_loss: 1.089454 test_acc: 0.513333
Epoch: 590 train_loss: 0.007717 val_loss: 1.156102 val_acc: 0.433333 test_loss: 1.103655 test_acc: 0.520000
Epoch: 600 train_loss: 0.007678 val_loss: 1.139463 val_acc: 0.420000 test_loss: 1.124015 test_acc: 0.486667
Epoch: 610 train_loss: 0.007700 val_loss: 1.122574 val_acc: 0.420000 test_loss: 1.082849 test_acc: 0.493333
Epoch: 620 train_loss: 0.007945 val_loss: 1.066212 val_acc: 0.473333 test_loss: 1.069776 test_acc: 0.500000
Epoch: 630 train_loss: 0.008109 val_loss: 1.093095 val_acc: 0.426667 test_loss: 1.049023 test_acc: 0.513333
Epoch: 640 train_loss: 0.007841 val_loss: 1.088356 val_acc: 0.453333 test_loss: 1.063517 test_acc: 0.540000
Epoch: 650 train_loss: 0.007807 val_loss: 1.148306 val_acc: 0.433333 test_loss: 1.122840 test_acc: 0.513333
Epoch: 660 train_loss: 0.007767 val_loss: 1.137055 val_acc: 0.460000 test_loss: 1.126951 test_acc: 0.500000
Epoch: 670 train_loss: 0.007680 val_loss: 1.131593 val_acc: 0.440000 test_loss: 1.121519 test_acc: 0.486667
Epoch: 680 train_loss: 0.007672 val_loss: 1.131208 val_acc: 0.440000 test_loss: 1.110687 test_acc: 0.473333
Epoch: 690 train_loss: 0.007675 val_loss: 1.152756 val_acc: 0.426667 test_loss: 1.120435 test_acc: 0.486667
Epoch: 700 train_loss: 0.007743 val_loss: 1.137512 val_acc: 0.446667 test_loss: 1.118877 test_acc: 0.486667
Epoch: 710 train_loss: 0.007685 val_loss: 1.140708 val_acc: 0.446667 test_loss: 1.108784 test_acc: 0.500000
Epoch: 720 train_loss: 0.007672 val_loss: 1.124944 val_acc: 0.433333 test_loss: 1.094631 test_acc: 0.493333
Epoch: 730 train_loss: 0.007722 val_loss: 1.132272 val_acc: 0.446667 test_loss: 1.106760 test_acc: 0.480000
Epoch: 740 train_loss: 0.007730 val_loss: 1.135384 val_acc: 0.440000 test_loss: 1.100976 test_acc: 0.493333
Epoch: 750 train_loss: 0.007689 val_loss: 1.137659 val_acc: 0.453333 test_loss: 1.116845 test_acc: 0.506667
Epoch: 760 train_loss: 0.007676 val_loss: 1.140656 val_acc: 0.453333 test_loss: 1.119095 test_acc: 0.493333
Epoch: 770 train_loss: 0.008042 val_loss: 1.096767 val_acc: 0.446667 test_loss: 1.057837 test_acc: 0.513333
Epoch: 780 train_loss: 0.007872 val_loss: 1.121390 val_acc: 0.440000 test_loss: 1.090235 test_acc: 0.533333
Epoch: 790 train_loss: 0.007724 val_loss: 1.117264 val_acc: 0.433333 test_loss: 1.104676 test_acc: 0.486667
------------- 2 val_acc: 0.5133 test_acc: 0.4800 best_test: 0.5667 -----------------
Fold: 3
Epoch: 000 train_loss: 0.009390 val_loss: 1.099885 val_acc: 0.420000 test_loss: 1.097606 test_acc: 0.366667
Epoch: 010 train_loss: 0.008692 val_loss: 1.075230 val_acc: 0.440000 test_loss: 1.073440 test_acc: 0.420000
Epoch: 020 train_loss: 0.008536 val_loss: 1.042721 val_acc: 0.533333 test_loss: 1.053790 test_acc: 0.526667
Epoch: 030 train_loss: 0.008463 val_loss: 1.037493 val_acc: 0.533333 test_loss: 1.057555 test_acc: 0.506667
Epoch: 040 train_loss: 0.008436 val_loss: 1.058960 val_acc: 0.540000 test_loss: 1.063423 test_acc: 0.526667
Epoch: 050 train_loss: 0.008274 val_loss: 1.031546 val_acc: 0.566667 test_loss: 1.070486 test_acc: 0.506667
Epoch: 060 train_loss: 0.008326 val_loss: 1.037247 val_acc: 0.566667 test_loss: 1.067996 test_acc: 0.473333
Epoch: 070 train_loss: 0.008342 val_loss: 1.052050 val_acc: 0.546667 test_loss: 1.064360 test_acc: 0.526667
Epoch: 080 train_loss: 0.008236 val_loss: 1.052093 val_acc: 0.546667 test_loss: 1.080077 test_acc: 0.506667
Epoch: 090 train_loss: 0.008101 val_loss: 1.043566 val_acc: 0.560000 test_loss: 1.112051 test_acc: 0.466667
Epoch: 100 train_loss: 0.008095 val_loss: 1.027809 val_acc: 0.546667 test_loss: 1.107943 test_acc: 0.493333
Epoch: 110 train_loss: 0.008059 val_loss: 1.100667 val_acc: 0.526667 test_loss: 1.126298 test_acc: 0.473333
Epoch: 120 train_loss: 0.008189 val_loss: 1.061204 val_acc: 0.526667 test_loss: 1.115298 test_acc: 0.473333
Epoch: 130 train_loss: 0.007983 val_loss: 1.066261 val_acc: 0.540000 test_loss: 1.129499 test_acc: 0.473333
Epoch: 140 train_loss: 0.007914 val_loss: 1.082572 val_acc: 0.540000 test_loss: 1.148712 test_acc: 0.500000
Epoch: 150 train_loss: 0.007976 val_loss: 1.075510 val_acc: 0.526667 test_loss: 1.118758 test_acc: 0.486667
Epoch: 160 train_loss: 0.007941 val_loss: 1.078231 val_acc: 0.533333 test_loss: 1.142193 test_acc: 0.466667
Epoch: 170 train_loss: 0.007973 val_loss: 1.120559 val_acc: 0.526667 test_loss: 1.144055 test_acc: 0.486667
Epoch: 180 train_loss: 0.007887 val_loss: 1.084950 val_acc: 0.546667 test_loss: 1.158159 test_acc: 0.460000
Epoch: 190 train_loss: 0.007845 val_loss: 1.083941 val_acc: 0.540000 test_loss: 1.181318 test_acc: 0.453333
Epoch: 200 train_loss: 0.007781 val_loss: 1.070859 val_acc: 0.500000 test_loss: 1.179327 test_acc: 0.480000
Epoch: 210 train_loss: 0.007833 val_loss: 1.114312 val_acc: 0.486667 test_loss: 1.140234 test_acc: 0.486667
Epoch: 220 train_loss: 0.007987 val_loss: 1.091003 val_acc: 0.526667 test_loss: 1.123514 test_acc: 0.440000
Epoch: 230 train_loss: 0.007784 val_loss: 1.114916 val_acc: 0.506667 test_loss: 1.140121 test_acc: 0.480000
Epoch: 240 train_loss: 0.007801 val_loss: 1.112488 val_acc: 0.500000 test_loss: 1.176836 test_acc: 0.446667
Epoch: 250 train_loss: 0.007914 val_loss: 1.082146 val_acc: 0.573333 test_loss: 1.139105 test_acc: 0.453333
Epoch: 260 train_loss: 0.007868 val_loss: 1.090976 val_acc: 0.480000 test_loss: 1.151363 test_acc: 0.433333
Epoch: 270 train_loss: 0.007756 val_loss: 1.107396 val_acc: 0.520000 test_loss: 1.182573 test_acc: 0.466667
Epoch: 280 train_loss: 0.007761 val_loss: 1.090846 val_acc: 0.493333 test_loss: 1.189318 test_acc: 0.473333
Epoch: 290 train_loss: 0.007738 val_loss: 1.081448 val_acc: 0.520000 test_loss: 1.175549 test_acc: 0.446667
Epoch: 300 train_loss: 0.007736 val_loss: 1.079501 val_acc: 0.533333 test_loss: 1.171911 test_acc: 0.460000
Epoch: 310 train_loss: 0.007772 val_loss: 1.074595 val_acc: 0.506667 test_loss: 1.167785 test_acc: 0.453333
Epoch: 320 train_loss: 0.007735 val_loss: 1.085935 val_acc: 0.506667 test_loss: 1.165368 test_acc: 0.460000
Epoch: 330 train_loss: 0.007843 val_loss: 1.117625 val_acc: 0.486667 test_loss: 1.168686 test_acc: 0.480000
Epoch: 340 train_loss: 0.007775 val_loss: 1.128224 val_acc: 0.520000 test_loss: 1.178204 test_acc: 0.460000
Epoch: 350 train_loss: 0.007732 val_loss: 1.108020 val_acc: 0.520000 test_loss: 1.179389 test_acc: 0.480000
Epoch: 360 train_loss: 0.007746 val_loss: 1.096031 val_acc: 0.506667 test_loss: 1.160432 test_acc: 0.453333
Epoch: 370 train_loss: 0.007794 val_loss: 1.109791 val_acc: 0.506667 test_loss: 1.164334 test_acc: 0.493333
Epoch: 380 train_loss: 0.008128 val_loss: 1.086384 val_acc: 0.440000 test_loss: 1.091358 test_acc: 0.493333
Epoch: 390 train_loss: 0.007901 val_loss: 1.093201 val_acc: 0.526667 test_loss: 1.138573 test_acc: 0.486667
Epoch: 400 train_loss: 0.007863 val_loss: 1.092300 val_acc: 0.480000 test_loss: 1.135878 test_acc: 0.453333
Epoch: 410 train_loss: 0.007793 val_loss: 1.122513 val_acc: 0.506667 test_loss: 1.172441 test_acc: 0.453333
Epoch: 420 train_loss: 0.007737 val_loss: 1.139617 val_acc: 0.486667 test_loss: 1.180928 test_acc: 0.466667
Epoch: 430 train_loss: 0.007759 val_loss: 1.112559 val_acc: 0.493333 test_loss: 1.159759 test_acc: 0.480000
Epoch: 440 train_loss: 0.007967 val_loss: 1.109730 val_acc: 0.513333 test_loss: 1.114163 test_acc: 0.500000
Epoch: 450 train_loss: 0.007831 val_loss: 1.098490 val_acc: 0.513333 test_loss: 1.127071 test_acc: 0.480000
Epoch: 460 train_loss: 0.007775 val_loss: 1.110626 val_acc: 0.500000 test_loss: 1.144127 test_acc: 0.500000
Epoch: 470 train_loss: 0.007740 val_loss: 1.110133 val_acc: 0.513333 test_loss: 1.154029 test_acc: 0.460000
Epoch: 480 train_loss: 0.007705 val_loss: 1.104075 val_acc: 0.526667 test_loss: 1.160472 test_acc: 0.480000
Epoch: 490 train_loss: 0.007736 val_loss: 1.097561 val_acc: 0.533333 test_loss: 1.151854 test_acc: 0.480000
Epoch: 500 train_loss: 0.008051 val_loss: 1.078833 val_acc: 0.520000 test_loss: 1.101097 test_acc: 0.506667
Epoch: 510 train_loss: 0.007827 val_loss: 1.116134 val_acc: 0.513333 test_loss: 1.125938 test_acc: 0.500000
Epoch: 520 train_loss: 0.008037 val_loss: 1.093813 val_acc: 0.520000 test_loss: 1.091316 test_acc: 0.540000
Epoch: 530 train_loss: 0.008239 val_loss: 1.093249 val_acc: 0.520000 test_loss: 1.084447 test_acc: 0.500000
Epoch: 540 train_loss: 0.007755 val_loss: 1.099244 val_acc: 0.513333 test_loss: 1.108540 test_acc: 0.466667
Epoch: 550 train_loss: 0.007738 val_loss: 1.098995 val_acc: 0.520000 test_loss: 1.126249 test_acc: 0.473333
Epoch: 560 train_loss: 0.007790 val_loss: 1.114600 val_acc: 0.500000 test_loss: 1.141779 test_acc: 0.480000
Epoch: 570 train_loss: 0.007746 val_loss: 1.100770 val_acc: 0.526667 test_loss: 1.138356 test_acc: 0.513333
Epoch: 580 train_loss: 0.007729 val_loss: 1.121189 val_acc: 0.520000 test_loss: 1.147106 test_acc: 0.480000
Epoch: 590 train_loss: 0.007719 val_loss: 1.104769 val_acc: 0.526667 test_loss: 1.136412 test_acc: 0.500000
Epoch: 600 train_loss: 0.007755 val_loss: 1.085978 val_acc: 0.533333 test_loss: 1.135254 test_acc: 0.486667
Epoch: 610 train_loss: 0.007743 val_loss: 1.094677 val_acc: 0.520000 test_loss: 1.123102 test_acc: 0.486667
Epoch: 620 train_loss: 0.007720 val_loss: 1.072168 val_acc: 0.526667 test_loss: 1.126268 test_acc: 0.513333
Epoch: 630 train_loss: 0.007710 val_loss: 1.083201 val_acc: 0.526667 test_loss: 1.124479 test_acc: 0.520000
Epoch: 640 train_loss: 0.007708 val_loss: 1.088974 val_acc: 0.533333 test_loss: 1.136982 test_acc: 0.493333
Epoch: 650 train_loss: 0.007725 val_loss: 1.078805 val_acc: 0.533333 test_loss: 1.134517 test_acc: 0.486667
Epoch: 660 train_loss: 0.007747 val_loss: 1.103126 val_acc: 0.513333 test_loss: 1.155041 test_acc: 0.513333
Epoch: 670 train_loss: 0.007684 val_loss: 1.076369 val_acc: 0.533333 test_loss: 1.138820 test_acc: 0.480000
Epoch: 680 train_loss: 0.007749 val_loss: 1.086803 val_acc: 0.520000 test_loss: 1.145047 test_acc: 0.500000
Epoch: 690 train_loss: 0.007704 val_loss: 1.079973 val_acc: 0.533333 test_loss: 1.137960 test_acc: 0.486667
Epoch: 700 train_loss: 0.007763 val_loss: 1.088203 val_acc: 0.520000 test_loss: 1.140159 test_acc: 0.486667
Epoch: 710 train_loss: 0.008051 val_loss: 1.068460 val_acc: 0.513333 test_loss: 1.086620 test_acc: 0.486667
Epoch: 720 train_loss: 0.008103 val_loss: 1.061032 val_acc: 0.480000 test_loss: 1.092655 test_acc: 0.466667
Epoch: 730 train_loss: 0.007795 val_loss: 1.084034 val_acc: 0.526667 test_loss: 1.150281 test_acc: 0.453333
