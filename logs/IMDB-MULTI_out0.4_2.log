/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/dataset.py:214: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete 'dataset/TUDataset/IMDB-MULTI/processed' first
  warnings.warn(
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:0', dataset='IMDB-MULTI', in_size=89, num_classes=3, fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=89, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=89, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=6, eta=0.5, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=3)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.009525 val_loss: 1.107787 val_acc: 0.306667 test_loss: 1.101652 test_acc: 0.286667
Epoch: 010 train_loss: 0.008654 val_loss: 1.064350 val_acc: 0.460000 test_loss: 1.076002 test_acc: 0.480000
Epoch: 020 train_loss: 0.008430 val_loss: 1.076437 val_acc: 0.446667 test_loss: 1.068159 test_acc: 0.486667
Epoch: 030 train_loss: 0.008272 val_loss: 1.079368 val_acc: 0.440000 test_loss: 1.073599 test_acc: 0.493333
Epoch: 040 train_loss: 0.008217 val_loss: 1.076657 val_acc: 0.440000 test_loss: 1.062953 test_acc: 0.500000
Epoch: 050 train_loss: 0.008113 val_loss: 1.073267 val_acc: 0.426667 test_loss: 1.066324 test_acc: 0.473333
Epoch: 060 train_loss: 0.008095 val_loss: 1.096267 val_acc: 0.406667 test_loss: 1.086005 test_acc: 0.446667
Epoch: 070 train_loss: 0.008169 val_loss: 1.113231 val_acc: 0.460000 test_loss: 1.091738 test_acc: 0.506667
Epoch: 080 train_loss: 0.008130 val_loss: 1.087707 val_acc: 0.453333 test_loss: 1.075958 test_acc: 0.506667
Epoch: 090 train_loss: 0.007896 val_loss: 1.101600 val_acc: 0.426667 test_loss: 1.100313 test_acc: 0.466667
Epoch: 100 train_loss: 0.007912 val_loss: 1.082287 val_acc: 0.473333 test_loss: 1.098676 test_acc: 0.493333
Epoch: 110 train_loss: 0.007885 val_loss: 1.096596 val_acc: 0.460000 test_loss: 1.141175 test_acc: 0.446667
Epoch: 120 train_loss: 0.007792 val_loss: 1.084860 val_acc: 0.460000 test_loss: 1.101057 test_acc: 0.480000
Epoch: 130 train_loss: 0.007995 val_loss: 1.096711 val_acc: 0.433333 test_loss: 1.098060 test_acc: 0.460000
Epoch: 140 train_loss: 0.007872 val_loss: 1.078358 val_acc: 0.440000 test_loss: 1.107899 test_acc: 0.446667
Epoch: 150 train_loss: 0.007830 val_loss: 1.113606 val_acc: 0.433333 test_loss: 1.127125 test_acc: 0.460000
Epoch: 160 train_loss: 0.007782 val_loss: 1.089303 val_acc: 0.473333 test_loss: 1.085893 test_acc: 0.480000
Epoch: 170 train_loss: 0.008009 val_loss: 1.087313 val_acc: 0.480000 test_loss: 1.074126 test_acc: 0.493333
Epoch: 180 train_loss: 0.007776 val_loss: 1.097263 val_acc: 0.453333 test_loss: 1.107114 test_acc: 0.466667
Epoch: 190 train_loss: 0.007725 val_loss: 1.103490 val_acc: 0.440000 test_loss: 1.103218 test_acc: 0.460000
Epoch: 200 train_loss: 0.007764 val_loss: 1.104533 val_acc: 0.440000 test_loss: 1.094854 test_acc: 0.473333
Epoch: 210 train_loss: 0.007792 val_loss: 1.117220 val_acc: 0.433333 test_loss: 1.121545 test_acc: 0.473333
Epoch: 220 train_loss: 0.007845 val_loss: 1.104956 val_acc: 0.413333 test_loss: 1.122442 test_acc: 0.420000
Epoch: 230 train_loss: 0.007704 val_loss: 1.090566 val_acc: 0.433333 test_loss: 1.087199 test_acc: 0.473333
Epoch: 240 train_loss: 0.007729 val_loss: 1.101488 val_acc: 0.446667 test_loss: 1.104534 test_acc: 0.466667
Epoch: 250 train_loss: 0.007705 val_loss: 1.110751 val_acc: 0.433333 test_loss: 1.097513 test_acc: 0.493333
Epoch: 260 train_loss: 0.007695 val_loss: 1.102774 val_acc: 0.440000 test_loss: 1.105595 test_acc: 0.473333
Epoch: 270 train_loss: 0.007692 val_loss: 1.100598 val_acc: 0.433333 test_loss: 1.116834 test_acc: 0.446667
Epoch: 280 train_loss: 0.008012 val_loss: 1.097491 val_acc: 0.440000 test_loss: 1.107935 test_acc: 0.460000
Epoch: 290 train_loss: 0.007780 val_loss: 1.099572 val_acc: 0.433333 test_loss: 1.088265 test_acc: 0.466667
Epoch: 300 train_loss: 0.007713 val_loss: 1.103734 val_acc: 0.453333 test_loss: 1.103590 test_acc: 0.486667
Epoch: 310 train_loss: 0.007716 val_loss: 1.101971 val_acc: 0.433333 test_loss: 1.088167 test_acc: 0.486667
Epoch: 320 train_loss: 0.007652 val_loss: 1.116915 val_acc: 0.433333 test_loss: 1.086838 test_acc: 0.473333
Epoch: 330 train_loss: 0.007658 val_loss: 1.106401 val_acc: 0.433333 test_loss: 1.083765 test_acc: 0.486667
Epoch: 340 train_loss: 0.007684 val_loss: 1.115947 val_acc: 0.446667 test_loss: 1.095770 test_acc: 0.453333
Epoch: 350 train_loss: 0.007693 val_loss: 1.107728 val_acc: 0.440000 test_loss: 1.075344 test_acc: 0.500000
Epoch: 360 train_loss: 0.007749 val_loss: 1.080095 val_acc: 0.466667 test_loss: 1.080822 test_acc: 0.506667
Epoch: 370 train_loss: 0.008082 val_loss: 1.080967 val_acc: 0.466667 test_loss: 1.083189 test_acc: 0.480000
Epoch: 380 train_loss: 0.008079 val_loss: 1.071270 val_acc: 0.466667 test_loss: 1.077476 test_acc: 0.486667
Epoch: 390 train_loss: 0.007662 val_loss: 1.122531 val_acc: 0.433333 test_loss: 1.118329 test_acc: 0.473333
Epoch: 400 train_loss: 0.007669 val_loss: 1.110851 val_acc: 0.446667 test_loss: 1.119215 test_acc: 0.453333
Epoch: 410 train_loss: 0.007659 val_loss: 1.109770 val_acc: 0.433333 test_loss: 1.116540 test_acc: 0.453333
Epoch: 420 train_loss: 0.007663 val_loss: 1.109815 val_acc: 0.446667 test_loss: 1.114884 test_acc: 0.466667
Epoch: 430 train_loss: 0.008141 val_loss: 1.099805 val_acc: 0.440000 test_loss: 1.105730 test_acc: 0.453333
Epoch: 440 train_loss: 0.008202 val_loss: 1.068450 val_acc: 0.466667 test_loss: 1.088308 test_acc: 0.473333
Epoch: 450 train_loss: 0.007881 val_loss: 1.087085 val_acc: 0.466667 test_loss: 1.110892 test_acc: 0.460000
Epoch: 460 train_loss: 0.007689 val_loss: 1.088196 val_acc: 0.466667 test_loss: 1.117474 test_acc: 0.453333
Epoch: 470 train_loss: 0.007676 val_loss: 1.101254 val_acc: 0.433333 test_loss: 1.113366 test_acc: 0.480000
Epoch: 480 train_loss: 0.007665 val_loss: 1.094660 val_acc: 0.440000 test_loss: 1.098920 test_acc: 0.486667
Epoch: 490 train_loss: 0.007650 val_loss: 1.093022 val_acc: 0.466667 test_loss: 1.106776 test_acc: 0.486667
Epoch: 500 train_loss: 0.007625 val_loss: 1.095123 val_acc: 0.453333 test_loss: 1.119151 test_acc: 0.460000
Epoch: 510 train_loss: 0.007666 val_loss: 1.085677 val_acc: 0.453333 test_loss: 1.114274 test_acc: 0.460000
Epoch: 520 train_loss: 0.007611 val_loss: 1.095116 val_acc: 0.466667 test_loss: 1.116336 test_acc: 0.480000
Epoch: 530 train_loss: 0.007701 val_loss: 1.090876 val_acc: 0.460000 test_loss: 1.111515 test_acc: 0.466667
Epoch: 540 train_loss: 0.008127 val_loss: 1.074052 val_acc: 0.466667 test_loss: 1.088303 test_acc: 0.480000
Epoch: 550 train_loss: 0.007863 val_loss: 1.096074 val_acc: 0.440000 test_loss: 1.077322 test_acc: 0.480000
Epoch: 560 train_loss: 0.007700 val_loss: 1.093368 val_acc: 0.453333 test_loss: 1.103621 test_acc: 0.460000
Epoch: 570 train_loss: 0.007690 val_loss: 1.098327 val_acc: 0.440000 test_loss: 1.118373 test_acc: 0.460000
Epoch: 580 train_loss: 0.007772 val_loss: 1.098384 val_acc: 0.453333 test_loss: 1.129470 test_acc: 0.453333
Epoch: 590 train_loss: 0.007973 val_loss: 1.089991 val_acc: 0.433333 test_loss: 1.059282 test_acc: 0.480000
Epoch: 600 train_loss: 0.007872 val_loss: 1.110218 val_acc: 0.386667 test_loss: 1.061876 test_acc: 0.466667
Epoch: 610 train_loss: 0.007706 val_loss: 1.105555 val_acc: 0.433333 test_loss: 1.118924 test_acc: 0.460000
Epoch: 620 train_loss: 0.007695 val_loss: 1.107717 val_acc: 0.426667 test_loss: 1.107917 test_acc: 0.480000
Epoch: 630 train_loss: 0.007681 val_loss: 1.108260 val_acc: 0.440000 test_loss: 1.091268 test_acc: 0.486667
Epoch: 640 train_loss: 0.007668 val_loss: 1.118512 val_acc: 0.440000 test_loss: 1.111199 test_acc: 0.453333
Epoch: 650 train_loss: 0.007638 val_loss: 1.118962 val_acc: 0.433333 test_loss: 1.117081 test_acc: 0.460000
Epoch: 660 train_loss: 0.007652 val_loss: 1.113353 val_acc: 0.440000 test_loss: 1.111779 test_acc: 0.453333
Epoch: 670 train_loss: 0.007640 val_loss: 1.123777 val_acc: 0.440000 test_loss: 1.114841 test_acc: 0.466667
Epoch: 680 train_loss: 0.007993 val_loss: 1.082958 val_acc: 0.433333 test_loss: 1.088483 test_acc: 0.453333
Epoch: 690 train_loss: 0.007773 val_loss: 1.099264 val_acc: 0.453333 test_loss: 1.133568 test_acc: 0.433333
Epoch: 700 train_loss: 0.007673 val_loss: 1.103930 val_acc: 0.433333 test_loss: 1.093669 test_acc: 0.486667
Epoch: 710 train_loss: 0.007626 val_loss: 1.103985 val_acc: 0.426667 test_loss: 1.093647 test_acc: 0.473333
Epoch: 720 train_loss: 0.007641 val_loss: 1.096434 val_acc: 0.446667 test_loss: 1.099034 test_acc: 0.473333
Epoch: 730 train_loss: 0.007616 val_loss: 1.109613 val_acc: 0.433333 test_loss: 1.102358 test_acc: 0.473333
Epoch: 740 train_loss: 0.007626 val_loss: 1.107197 val_acc: 0.440000 test_loss: 1.115560 test_acc: 0.480000
Epoch: 750 train_loss: 0.007624 val_loss: 1.116056 val_acc: 0.453333 test_loss: 1.111969 test_acc: 0.480000
Epoch: 760 train_loss: 0.007658 val_loss: 1.114084 val_acc: 0.440000 test_loss: 1.104846 test_acc: 0.486667
Epoch: 770 train_loss: 0.007619 val_loss: 1.106817 val_acc: 0.446667 test_loss: 1.108473 test_acc: 0.473333
Epoch: 780 train_loss: 0.007627 val_loss: 1.097255 val_acc: 0.446667 test_loss: 1.096414 test_acc: 0.473333
Epoch: 790 train_loss: 0.007835 val_loss: 1.078197 val_acc: 0.466667 test_loss: 1.066896 test_acc: 0.500000
------------- 0 val_acc: 0.4867 test_acc: 0.4933 best_test: 0.5333 -----------------
Fold: 1
Epoch: 000 train_loss: 0.009664 val_loss: 1.104791 val_acc: 0.320000 test_loss: 1.090719 test_acc: 0.440000
Epoch: 010 train_loss: 0.008674 val_loss: 1.059333 val_acc: 0.486667 test_loss: 1.040177 test_acc: 0.453333
Epoch: 020 train_loss: 0.008489 val_loss: 1.054597 val_acc: 0.480000 test_loss: 1.035523 test_acc: 0.486667
Epoch: 030 train_loss: 0.008357 val_loss: 1.060844 val_acc: 0.500000 test_loss: 1.053500 test_acc: 0.486667
Epoch: 040 train_loss: 0.008298 val_loss: 1.066568 val_acc: 0.486667 test_loss: 1.037983 test_acc: 0.460000
Epoch: 050 train_loss: 0.008183 val_loss: 1.063276 val_acc: 0.500000 test_loss: 1.038316 test_acc: 0.480000
Epoch: 060 train_loss: 0.008112 val_loss: 1.058170 val_acc: 0.480000 test_loss: 1.052080 test_acc: 0.533333
Epoch: 070 train_loss: 0.008177 val_loss: 1.067572 val_acc: 0.513333 test_loss: 1.046084 test_acc: 0.473333
Epoch: 080 train_loss: 0.008009 val_loss: 1.099418 val_acc: 0.473333 test_loss: 1.108934 test_acc: 0.440000
Epoch: 090 train_loss: 0.007926 val_loss: 1.097713 val_acc: 0.500000 test_loss: 1.090818 test_acc: 0.446667
Epoch: 100 train_loss: 0.007919 val_loss: 1.098088 val_acc: 0.500000 test_loss: 1.077810 test_acc: 0.520000
Epoch: 110 train_loss: 0.007910 val_loss: 1.079532 val_acc: 0.506667 test_loss: 1.094925 test_acc: 0.480000
Epoch: 120 train_loss: 0.007815 val_loss: 1.126065 val_acc: 0.500000 test_loss: 1.112882 test_acc: 0.426667
Epoch: 130 train_loss: 0.008004 val_loss: 1.127479 val_acc: 0.466667 test_loss: 1.145590 test_acc: 0.420000
Epoch: 140 train_loss: 0.007838 val_loss: 1.073279 val_acc: 0.520000 test_loss: 1.093772 test_acc: 0.453333
Epoch: 150 train_loss: 0.007796 val_loss: 1.090769 val_acc: 0.506667 test_loss: 1.107839 test_acc: 0.460000
Epoch: 160 train_loss: 0.007932 val_loss: 1.081096 val_acc: 0.520000 test_loss: 1.108952 test_acc: 0.453333
Epoch: 170 train_loss: 0.007814 val_loss: 1.106922 val_acc: 0.473333 test_loss: 1.110944 test_acc: 0.473333
Epoch: 180 train_loss: 0.007810 val_loss: 1.110422 val_acc: 0.486667 test_loss: 1.094396 test_acc: 0.453333
Epoch: 190 train_loss: 0.007769 val_loss: 1.115461 val_acc: 0.486667 test_loss: 1.109618 test_acc: 0.473333
Epoch: 200 train_loss: 0.007726 val_loss: 1.098824 val_acc: 0.493333 test_loss: 1.090936 test_acc: 0.460000
Epoch: 210 train_loss: 0.007804 val_loss: 1.080185 val_acc: 0.506667 test_loss: 1.103780 test_acc: 0.440000
Epoch: 220 train_loss: 0.007749 val_loss: 1.091487 val_acc: 0.500000 test_loss: 1.100430 test_acc: 0.446667
Epoch: 230 train_loss: 0.007719 val_loss: 1.090482 val_acc: 0.500000 test_loss: 1.110070 test_acc: 0.446667
Epoch: 240 train_loss: 0.007778 val_loss: 1.104365 val_acc: 0.513333 test_loss: 1.119666 test_acc: 0.446667
Epoch: 250 train_loss: 0.007827 val_loss: 1.106877 val_acc: 0.500000 test_loss: 1.123769 test_acc: 0.473333
Epoch: 260 train_loss: 0.008158 val_loss: 1.065581 val_acc: 0.500000 test_loss: 1.088173 test_acc: 0.466667
Epoch: 270 train_loss: 0.007996 val_loss: 1.080108 val_acc: 0.500000 test_loss: 1.090356 test_acc: 0.480000
Epoch: 280 train_loss: 0.007806 val_loss: 1.096867 val_acc: 0.480000 test_loss: 1.107404 test_acc: 0.440000
Epoch: 290 train_loss: 0.007755 val_loss: 1.082322 val_acc: 0.513333 test_loss: 1.096057 test_acc: 0.446667
Epoch: 300 train_loss: 0.007721 val_loss: 1.076201 val_acc: 0.520000 test_loss: 1.114584 test_acc: 0.440000
Epoch: 310 train_loss: 0.008212 val_loss: 1.143507 val_acc: 0.466667 test_loss: 1.074505 test_acc: 0.506667
Epoch: 320 train_loss: 0.007931 val_loss: 1.145229 val_acc: 0.486667 test_loss: 1.114973 test_acc: 0.426667
Epoch: 330 train_loss: 0.007966 val_loss: 1.070302 val_acc: 0.506667 test_loss: 1.103347 test_acc: 0.440000
Epoch: 340 train_loss: 0.007751 val_loss: 1.090455 val_acc: 0.500000 test_loss: 1.105532 test_acc: 0.473333
Epoch: 350 train_loss: 0.007818 val_loss: 1.103825 val_acc: 0.513333 test_loss: 1.108942 test_acc: 0.433333
Epoch: 360 train_loss: 0.007721 val_loss: 1.096152 val_acc: 0.500000 test_loss: 1.102362 test_acc: 0.466667
Epoch: 370 train_loss: 0.007693 val_loss: 1.086465 val_acc: 0.493333 test_loss: 1.095874 test_acc: 0.460000
Epoch: 380 train_loss: 0.007881 val_loss: 1.075387 val_acc: 0.493333 test_loss: 1.094273 test_acc: 0.446667
Epoch: 390 train_loss: 0.007832 val_loss: 1.093793 val_acc: 0.466667 test_loss: 1.103408 test_acc: 0.453333
Epoch: 400 train_loss: 0.007720 val_loss: 1.093016 val_acc: 0.493333 test_loss: 1.114124 test_acc: 0.433333
Epoch: 410 train_loss: 0.007695 val_loss: 1.082384 val_acc: 0.500000 test_loss: 1.117569 test_acc: 0.440000
Epoch: 420 train_loss: 0.007702 val_loss: 1.080774 val_acc: 0.500000 test_loss: 1.119998 test_acc: 0.440000
Epoch: 430 train_loss: 0.007682 val_loss: 1.087074 val_acc: 0.506667 test_loss: 1.115885 test_acc: 0.460000
Epoch: 440 train_loss: 0.007703 val_loss: 1.080224 val_acc: 0.520000 test_loss: 1.108938 test_acc: 0.446667
Epoch: 450 train_loss: 0.007706 val_loss: 1.074616 val_acc: 0.520000 test_loss: 1.096622 test_acc: 0.466667
Epoch: 460 train_loss: 0.007679 val_loss: 1.088324 val_acc: 0.506667 test_loss: 1.112928 test_acc: 0.440000
Epoch: 470 train_loss: 0.007729 val_loss: 1.088161 val_acc: 0.506667 test_loss: 1.105580 test_acc: 0.460000
Epoch: 480 train_loss: 0.007726 val_loss: 1.074679 val_acc: 0.526667 test_loss: 1.103058 test_acc: 0.460000
Epoch: 490 train_loss: 0.007732 val_loss: 1.084633 val_acc: 0.493333 test_loss: 1.122794 test_acc: 0.446667
Epoch: 500 train_loss: 0.008464 val_loss: 1.073982 val_acc: 0.480000 test_loss: 1.047569 test_acc: 0.473333
Epoch: 510 train_loss: 0.008017 val_loss: 1.065478 val_acc: 0.500000 test_loss: 1.070323 test_acc: 0.460000
Epoch: 520 train_loss: 0.008000 val_loss: 1.088301 val_acc: 0.486667 test_loss: 1.085515 test_acc: 0.433333
Epoch: 530 train_loss: 0.007789 val_loss: 1.071220 val_acc: 0.526667 test_loss: 1.102354 test_acc: 0.433333
Epoch: 540 train_loss: 0.007715 val_loss: 1.079499 val_acc: 0.506667 test_loss: 1.129266 test_acc: 0.433333
Epoch: 550 train_loss: 0.007727 val_loss: 1.092654 val_acc: 0.493333 test_loss: 1.133037 test_acc: 0.440000
Epoch: 560 train_loss: 0.007692 val_loss: 1.100028 val_acc: 0.513333 test_loss: 1.131877 test_acc: 0.440000
Epoch: 570 train_loss: 0.007708 val_loss: 1.080416 val_acc: 0.533333 test_loss: 1.148450 test_acc: 0.446667
Epoch: 580 train_loss: 0.007754 val_loss: 1.083061 val_acc: 0.500000 test_loss: 1.129701 test_acc: 0.446667
Epoch: 590 train_loss: 0.007694 val_loss: 1.095713 val_acc: 0.513333 test_loss: 1.133954 test_acc: 0.453333
Epoch: 600 train_loss: 0.007777 val_loss: 1.087168 val_acc: 0.513333 test_loss: 1.149726 test_acc: 0.440000
Epoch: 610 train_loss: 0.008061 val_loss: 1.069507 val_acc: 0.486667 test_loss: 1.101410 test_acc: 0.433333
Epoch: 620 train_loss: 0.007919 val_loss: 1.070212 val_acc: 0.506667 test_loss: 1.089636 test_acc: 0.426667
Epoch: 630 train_loss: 0.007816 val_loss: 1.057641 val_acc: 0.513333 test_loss: 1.082994 test_acc: 0.453333
Epoch: 640 train_loss: 0.007801 val_loss: 1.065183 val_acc: 0.506667 test_loss: 1.108482 test_acc: 0.453333
Epoch: 650 train_loss: 0.007761 val_loss: 1.070117 val_acc: 0.520000 test_loss: 1.092577 test_acc: 0.440000
Epoch: 660 train_loss: 0.007721 val_loss: 1.062814 val_acc: 0.533333 test_loss: 1.085329 test_acc: 0.473333
Epoch: 670 train_loss: 0.007704 val_loss: 1.070076 val_acc: 0.520000 test_loss: 1.105698 test_acc: 0.466667
Epoch: 680 train_loss: 0.007729 val_loss: 1.068963 val_acc: 0.533333 test_loss: 1.117778 test_acc: 0.453333
Epoch: 690 train_loss: 0.007731 val_loss: 1.081526 val_acc: 0.526667 test_loss: 1.096712 test_acc: 0.480000
Epoch: 700 train_loss: 0.007713 val_loss: 1.073739 val_acc: 0.533333 test_loss: 1.101618 test_acc: 0.480000
Epoch: 710 train_loss: 0.007756 val_loss: 1.081368 val_acc: 0.500000 test_loss: 1.109418 test_acc: 0.453333
Epoch: 720 train_loss: 0.007801 val_loss: 1.069526 val_acc: 0.493333 test_loss: 1.083271 test_acc: 0.446667
Epoch: 730 train_loss: 0.007715 val_loss: 1.072904 val_acc: 0.513333 test_loss: 1.118083 test_acc: 0.426667
Epoch: 740 train_loss: 0.007717 val_loss: 1.092133 val_acc: 0.493333 test_loss: 1.132331 test_acc: 0.446667
Epoch: 750 train_loss: 0.007701 val_loss: 1.077503 val_acc: 0.513333 test_loss: 1.126344 test_acc: 0.433333
Epoch: 760 train_loss: 0.007669 val_loss: 1.078160 val_acc: 0.533333 test_loss: 1.120648 test_acc: 0.440000
Epoch: 770 train_loss: 0.007733 val_loss: 1.079382 val_acc: 0.513333 test_loss: 1.108311 test_acc: 0.446667
Epoch: 780 train_loss: 0.007693 val_loss: 1.082209 val_acc: 0.526667 test_loss: 1.132057 test_acc: 0.453333
Epoch: 790 train_loss: 0.007695 val_loss: 1.082243 val_acc: 0.513333 test_loss: 1.141356 test_acc: 0.440000
------------- 1 val_acc: 0.5533 test_acc: 0.4733 best_test: 0.5333 -----------------
Fold: 2
Epoch: 000 train_loss: 0.009211 val_loss: 1.094901 val_acc: 0.406667 test_loss: 1.100392 test_acc: 0.346667
Epoch: 010 train_loss: 0.008702 val_loss: 1.035022 val_acc: 0.506667 test_loss: 1.064750 test_acc: 0.500000
Epoch: 020 train_loss: 0.008639 val_loss: 1.030443 val_acc: 0.506667 test_loss: 1.082634 test_acc: 0.493333
Epoch: 030 train_loss: 0.008510 val_loss: 1.045175 val_acc: 0.500000 test_loss: 1.058522 test_acc: 0.493333
Epoch: 040 train_loss: 0.008432 val_loss: 1.054886 val_acc: 0.486667 test_loss: 1.060413 test_acc: 0.526667
Epoch: 050 train_loss: 0.008325 val_loss: 1.058294 val_acc: 0.440000 test_loss: 1.036787 test_acc: 0.526667
Epoch: 060 train_loss: 0.008447 val_loss: 1.040920 val_acc: 0.500000 test_loss: 1.050145 test_acc: 0.540000
Epoch: 070 train_loss: 0.008191 val_loss: 1.045192 val_acc: 0.460000 test_loss: 1.036930 test_acc: 0.540000
Epoch: 080 train_loss: 0.008167 val_loss: 1.076392 val_acc: 0.440000 test_loss: 1.053604 test_acc: 0.526667
Epoch: 090 train_loss: 0.008120 val_loss: 1.078104 val_acc: 0.453333 test_loss: 1.030083 test_acc: 0.546667
Epoch: 100 train_loss: 0.008095 val_loss: 1.095574 val_acc: 0.446667 test_loss: 1.049965 test_acc: 0.520000
Epoch: 110 train_loss: 0.008067 val_loss: 1.087142 val_acc: 0.473333 test_loss: 1.042552 test_acc: 0.533333
Epoch: 120 train_loss: 0.008167 val_loss: 1.060303 val_acc: 0.453333 test_loss: 1.044135 test_acc: 0.513333
Epoch: 130 train_loss: 0.008015 val_loss: 1.072565 val_acc: 0.460000 test_loss: 1.048615 test_acc: 0.533333
Epoch: 140 train_loss: 0.007921 val_loss: 1.106611 val_acc: 0.440000 test_loss: 1.041540 test_acc: 0.540000
Epoch: 150 train_loss: 0.007891 val_loss: 1.104340 val_acc: 0.420000 test_loss: 1.087643 test_acc: 0.473333
Epoch: 160 train_loss: 0.008610 val_loss: 1.073568 val_acc: 0.440000 test_loss: 1.046852 test_acc: 0.520000
Epoch: 170 train_loss: 0.007916 val_loss: 1.117153 val_acc: 0.446667 test_loss: 1.040853 test_acc: 0.553333
Epoch: 180 train_loss: 0.007955 val_loss: 1.139686 val_acc: 0.440000 test_loss: 1.045224 test_acc: 0.526667
Epoch: 190 train_loss: 0.007883 val_loss: 1.109158 val_acc: 0.426667 test_loss: 1.041175 test_acc: 0.533333
Epoch: 200 train_loss: 0.008034 val_loss: 1.081385 val_acc: 0.460000 test_loss: 1.092565 test_acc: 0.480000
Epoch: 210 train_loss: 0.007924 val_loss: 1.107761 val_acc: 0.460000 test_loss: 1.043568 test_acc: 0.520000
Epoch: 220 train_loss: 0.007799 val_loss: 1.135374 val_acc: 0.420000 test_loss: 1.070553 test_acc: 0.513333
Epoch: 230 train_loss: 0.007828 val_loss: 1.117019 val_acc: 0.446667 test_loss: 1.076479 test_acc: 0.506667
Epoch: 240 train_loss: 0.007786 val_loss: 1.108446 val_acc: 0.453333 test_loss: 1.068720 test_acc: 0.506667
Epoch: 250 train_loss: 0.007739 val_loss: 1.112337 val_acc: 0.446667 test_loss: 1.083679 test_acc: 0.513333
Epoch: 260 train_loss: 0.007770 val_loss: 1.113954 val_acc: 0.440000 test_loss: 1.078095 test_acc: 0.526667
Epoch: 270 train_loss: 0.007777 val_loss: 1.099894 val_acc: 0.440000 test_loss: 1.052653 test_acc: 0.520000
Epoch: 280 train_loss: 0.007744 val_loss: 1.132296 val_acc: 0.440000 test_loss: 1.064414 test_acc: 0.526667
Epoch: 290 train_loss: 0.007906 val_loss: 1.105500 val_acc: 0.453333 test_loss: 1.076273 test_acc: 0.493333
Epoch: 300 train_loss: 0.007856 val_loss: 1.098782 val_acc: 0.433333 test_loss: 1.066173 test_acc: 0.513333
Epoch: 310 train_loss: 0.007784 val_loss: 1.118230 val_acc: 0.440000 test_loss: 1.058482 test_acc: 0.546667
Epoch: 320 train_loss: 0.007944 val_loss: 1.107316 val_acc: 0.440000 test_loss: 1.041867 test_acc: 0.553333
Epoch: 330 train_loss: 0.008045 val_loss: 1.083788 val_acc: 0.433333 test_loss: 1.064282 test_acc: 0.493333
Epoch: 340 train_loss: 0.007777 val_loss: 1.111413 val_acc: 0.446667 test_loss: 1.071857 test_acc: 0.513333
Epoch: 350 train_loss: 0.007732 val_loss: 1.130722 val_acc: 0.420000 test_loss: 1.079647 test_acc: 0.500000
Epoch: 360 train_loss: 0.007773 val_loss: 1.136667 val_acc: 0.446667 test_loss: 1.073677 test_acc: 0.513333
Epoch: 370 train_loss: 0.007766 val_loss: 1.135468 val_acc: 0.406667 test_loss: 1.073717 test_acc: 0.486667
Epoch: 380 train_loss: 0.007723 val_loss: 1.144838 val_acc: 0.420000 test_loss: 1.072321 test_acc: 0.520000
Epoch: 390 train_loss: 0.007871 val_loss: 1.112079 val_acc: 0.446667 test_loss: 1.069690 test_acc: 0.513333
Epoch: 400 train_loss: 0.007803 val_loss: 1.135743 val_acc: 0.420000 test_loss: 1.074756 test_acc: 0.520000
Epoch: 410 train_loss: 0.007885 val_loss: 1.115262 val_acc: 0.460000 test_loss: 1.066979 test_acc: 0.533333
Epoch: 420 train_loss: 0.007769 val_loss: 1.098190 val_acc: 0.440000 test_loss: 1.081846 test_acc: 0.506667
Epoch: 430 train_loss: 0.007747 val_loss: 1.143767 val_acc: 0.413333 test_loss: 1.108437 test_acc: 0.493333
Epoch: 440 train_loss: 0.007728 val_loss: 1.110387 val_acc: 0.440000 test_loss: 1.075231 test_acc: 0.520000
Epoch: 450 train_loss: 0.007731 val_loss: 1.128478 val_acc: 0.426667 test_loss: 1.082662 test_acc: 0.513333
Epoch: 460 train_loss: 0.008347 val_loss: 1.064538 val_acc: 0.440000 test_loss: 1.067546 test_acc: 0.513333
Epoch: 470 train_loss: 0.007861 val_loss: 1.099458 val_acc: 0.453333 test_loss: 1.085256 test_acc: 0.513333
Epoch: 480 train_loss: 0.008053 val_loss: 1.115049 val_acc: 0.413333 test_loss: 1.093810 test_acc: 0.513333
Epoch: 490 train_loss: 0.007906 val_loss: 1.088841 val_acc: 0.460000 test_loss: 1.074684 test_acc: 0.506667
Epoch: 500 train_loss: 0.007984 val_loss: 1.103700 val_acc: 0.440000 test_loss: 1.086013 test_acc: 0.493333
Epoch: 510 train_loss: 0.007831 val_loss: 1.102134 val_acc: 0.453333 test_loss: 1.090736 test_acc: 0.520000
Epoch: 520 train_loss: 0.007756 val_loss: 1.123431 val_acc: 0.453333 test_loss: 1.085261 test_acc: 0.520000
Epoch: 530 train_loss: 0.007712 val_loss: 1.130428 val_acc: 0.446667 test_loss: 1.087235 test_acc: 0.533333
Epoch: 540 train_loss: 0.007758 val_loss: 1.124513 val_acc: 0.440000 test_loss: 1.094731 test_acc: 0.500000
Epoch: 550 train_loss: 0.007730 val_loss: 1.129680 val_acc: 0.433333 test_loss: 1.095506 test_acc: 0.506667
Epoch: 560 train_loss: 0.007693 val_loss: 1.129886 val_acc: 0.446667 test_loss: 1.090702 test_acc: 0.506667
Epoch: 570 train_loss: 0.007711 val_loss: 1.146190 val_acc: 0.433333 test_loss: 1.093616 test_acc: 0.513333
Epoch: 580 train_loss: 0.007699 val_loss: 1.147679 val_acc: 0.440000 test_loss: 1.091947 test_acc: 0.513333
Epoch: 590 train_loss: 0.007685 val_loss: 1.168716 val_acc: 0.440000 test_loss: 1.110212 test_acc: 0.506667
Epoch: 600 train_loss: 0.007675 val_loss: 1.153685 val_acc: 0.433333 test_loss: 1.098521 test_acc: 0.500000
Epoch: 610 train_loss: 0.007692 val_loss: 1.139438 val_acc: 0.433333 test_loss: 1.075799 test_acc: 0.506667
Epoch: 620 train_loss: 0.008282 val_loss: 1.078508 val_acc: 0.453333 test_loss: 1.082454 test_acc: 0.506667
Epoch: 630 train_loss: 0.007995 val_loss: 1.077105 val_acc: 0.466667 test_loss: 1.092139 test_acc: 0.480000
Epoch: 640 train_loss: 0.007775 val_loss: 1.111978 val_acc: 0.420000 test_loss: 1.073908 test_acc: 0.506667
Epoch: 650 train_loss: 0.007741 val_loss: 1.143680 val_acc: 0.433333 test_loss: 1.120329 test_acc: 0.513333
Epoch: 660 train_loss: 0.007733 val_loss: 1.120386 val_acc: 0.433333 test_loss: 1.090317 test_acc: 0.506667
Epoch: 670 train_loss: 0.007689 val_loss: 1.128862 val_acc: 0.433333 test_loss: 1.103791 test_acc: 0.506667
Epoch: 680 train_loss: 0.007736 val_loss: 1.108520 val_acc: 0.473333 test_loss: 1.092371 test_acc: 0.506667
Epoch: 690 train_loss: 0.007709 val_loss: 1.131842 val_acc: 0.440000 test_loss: 1.089081 test_acc: 0.513333
Epoch: 700 train_loss: 0.007747 val_loss: 1.119063 val_acc: 0.440000 test_loss: 1.096362 test_acc: 0.513333
Epoch: 710 train_loss: 0.007681 val_loss: 1.114886 val_acc: 0.446667 test_loss: 1.089785 test_acc: 0.513333
Epoch: 720 train_loss: 0.007675 val_loss: 1.115975 val_acc: 0.426667 test_loss: 1.089242 test_acc: 0.506667
Epoch: 730 train_loss: 0.007690 val_loss: 1.127146 val_acc: 0.440000 test_loss: 1.117035 test_acc: 0.506667
Epoch: 740 train_loss: 0.007699 val_loss: 1.115345 val_acc: 0.440000 test_loss: 1.095700 test_acc: 0.500000
Epoch: 750 train_loss: 0.007696 val_loss: 1.120860 val_acc: 0.446667 test_loss: 1.095161 test_acc: 0.506667
Epoch: 760 train_loss: 0.007676 val_loss: 1.137079 val_acc: 0.453333 test_loss: 1.133961 test_acc: 0.486667
Epoch: 770 train_loss: 0.007885 val_loss: 1.086180 val_acc: 0.446667 test_loss: 1.077050 test_acc: 0.480000
Epoch: 780 train_loss: 0.007979 val_loss: 1.116670 val_acc: 0.453333 test_loss: 1.072720 test_acc: 0.506667
Epoch: 790 train_loss: 0.007906 val_loss: 1.118827 val_acc: 0.433333 test_loss: 1.088241 test_acc: 0.480000
------------- 2 val_acc: 0.5467 test_acc: 0.5600 best_test: 0.5733 -----------------
Fold: 3
Epoch: 000 train_loss: 0.009271 val_loss: 1.090229 val_acc: 0.433333 test_loss: 1.090220 test_acc: 0.420000
Epoch: 010 train_loss: 0.008744 val_loss: 1.069885 val_acc: 0.480000 test_loss: 1.075479 test_acc: 0.480000
Epoch: 020 train_loss: 0.008525 val_loss: 1.059995 val_acc: 0.520000 test_loss: 1.060540 test_acc: 0.506667
Epoch: 030 train_loss: 0.008463 val_loss: 1.076528 val_acc: 0.493333 test_loss: 1.059379 test_acc: 0.513333
Epoch: 040 train_loss: 0.008385 val_loss: 1.068930 val_acc: 0.546667 test_loss: 1.056004 test_acc: 0.513333
Epoch: 050 train_loss: 0.008329 val_loss: 1.099882 val_acc: 0.500000 test_loss: 1.068904 test_acc: 0.493333
Epoch: 060 train_loss: 0.008304 val_loss: 1.055297 val_acc: 0.533333 test_loss: 1.059247 test_acc: 0.493333
Epoch: 070 train_loss: 0.008145 val_loss: 1.067069 val_acc: 0.526667 test_loss: 1.071844 test_acc: 0.486667
Epoch: 080 train_loss: 0.008263 val_loss: 1.066863 val_acc: 0.520000 test_loss: 1.062291 test_acc: 0.493333
Epoch: 090 train_loss: 0.008166 val_loss: 1.078413 val_acc: 0.540000 test_loss: 1.072902 test_acc: 0.480000
Epoch: 100 train_loss: 0.008154 val_loss: 1.087807 val_acc: 0.520000 test_loss: 1.090884 test_acc: 0.500000
Epoch: 110 train_loss: 0.008190 val_loss: 1.052091 val_acc: 0.546667 test_loss: 1.067355 test_acc: 0.486667
Epoch: 120 train_loss: 0.008040 val_loss: 1.061390 val_acc: 0.526667 test_loss: 1.089897 test_acc: 0.486667
Epoch: 130 train_loss: 0.007938 val_loss: 1.078658 val_acc: 0.560000 test_loss: 1.113852 test_acc: 0.493333
Epoch: 140 train_loss: 0.007991 val_loss: 1.089966 val_acc: 0.520000 test_loss: 1.076942 test_acc: 0.526667
Epoch: 150 train_loss: 0.008033 val_loss: 1.074010 val_acc: 0.533333 test_loss: 1.091321 test_acc: 0.473333
Epoch: 160 train_loss: 0.007939 val_loss: 1.080846 val_acc: 0.520000 test_loss: 1.118103 test_acc: 0.493333
Epoch: 170 train_loss: 0.007914 val_loss: 1.091954 val_acc: 0.513333 test_loss: 1.113467 test_acc: 0.493333
Epoch: 180 train_loss: 0.007904 val_loss: 1.086977 val_acc: 0.520000 test_loss: 1.123793 test_acc: 0.473333
Epoch: 190 train_loss: 0.008035 val_loss: 1.052554 val_acc: 0.533333 test_loss: 1.076213 test_acc: 0.493333
Epoch: 200 train_loss: 0.007934 val_loss: 1.098047 val_acc: 0.500000 test_loss: 1.124212 test_acc: 0.486667
Epoch: 210 train_loss: 0.007815 val_loss: 1.089922 val_acc: 0.540000 test_loss: 1.138161 test_acc: 0.473333
Epoch: 220 train_loss: 0.007782 val_loss: 1.111927 val_acc: 0.513333 test_loss: 1.130493 test_acc: 0.466667
Epoch: 230 train_loss: 0.007784 val_loss: 1.110346 val_acc: 0.486667 test_loss: 1.135208 test_acc: 0.466667
Epoch: 240 train_loss: 0.007787 val_loss: 1.090742 val_acc: 0.526667 test_loss: 1.137776 test_acc: 0.480000
Epoch: 250 train_loss: 0.007851 val_loss: 1.078749 val_acc: 0.566667 test_loss: 1.110613 test_acc: 0.486667
Epoch: 260 train_loss: 0.007815 val_loss: 1.089722 val_acc: 0.513333 test_loss: 1.116869 test_acc: 0.500000
Epoch: 270 train_loss: 0.007744 val_loss: 1.107303 val_acc: 0.513333 test_loss: 1.132378 test_acc: 0.480000
Epoch: 280 train_loss: 0.007751 val_loss: 1.102020 val_acc: 0.493333 test_loss: 1.160636 test_acc: 0.473333
Epoch: 290 train_loss: 0.007739 val_loss: 1.127872 val_acc: 0.513333 test_loss: 1.161625 test_acc: 0.453333
Epoch: 300 train_loss: 0.007848 val_loss: 1.079804 val_acc: 0.486667 test_loss: 1.113554 test_acc: 0.480000
Epoch: 310 train_loss: 0.007805 val_loss: 1.120900 val_acc: 0.486667 test_loss: 1.132435 test_acc: 0.473333
Epoch: 320 train_loss: 0.007720 val_loss: 1.097409 val_acc: 0.513333 test_loss: 1.119598 test_acc: 0.473333
Epoch: 330 train_loss: 0.007747 val_loss: 1.106789 val_acc: 0.540000 test_loss: 1.118257 test_acc: 0.473333
Epoch: 340 train_loss: 0.007768 val_loss: 1.116818 val_acc: 0.500000 test_loss: 1.147112 test_acc: 0.466667
Epoch: 350 train_loss: 0.007723 val_loss: 1.090131 val_acc: 0.506667 test_loss: 1.125249 test_acc: 0.466667
Epoch: 360 train_loss: 0.007727 val_loss: 1.080864 val_acc: 0.513333 test_loss: 1.124123 test_acc: 0.473333
Epoch: 370 train_loss: 0.007760 val_loss: 1.085968 val_acc: 0.526667 test_loss: 1.128079 test_acc: 0.466667
Epoch: 380 train_loss: 0.007742 val_loss: 1.077789 val_acc: 0.506667 test_loss: 1.110526 test_acc: 0.480000
Epoch: 390 train_loss: 0.007744 val_loss: 1.102827 val_acc: 0.493333 test_loss: 1.132674 test_acc: 0.466667
Epoch: 400 train_loss: 0.008139 val_loss: 1.094267 val_acc: 0.506667 test_loss: 1.099883 test_acc: 0.486667
Epoch: 410 train_loss: 0.008028 val_loss: 1.054978 val_acc: 0.553333 test_loss: 1.098848 test_acc: 0.473333
Epoch: 420 train_loss: 0.007909 val_loss: 1.068947 val_acc: 0.520000 test_loss: 1.099288 test_acc: 0.513333
Epoch: 430 train_loss: 0.007794 val_loss: 1.089259 val_acc: 0.500000 test_loss: 1.091018 test_acc: 0.486667
Epoch: 440 train_loss: 0.007781 val_loss: 1.080942 val_acc: 0.500000 test_loss: 1.082391 test_acc: 0.480000
Epoch: 450 train_loss: 0.007757 val_loss: 1.090165 val_acc: 0.513333 test_loss: 1.101078 test_acc: 0.466667
Epoch: 460 train_loss: 0.007739 val_loss: 1.085648 val_acc: 0.513333 test_loss: 1.106569 test_acc: 0.473333
Epoch: 470 train_loss: 0.007754 val_loss: 1.089440 val_acc: 0.506667 test_loss: 1.122107 test_acc: 0.460000
Epoch: 480 train_loss: 0.007720 val_loss: 1.099678 val_acc: 0.526667 test_loss: 1.112462 test_acc: 0.506667
Epoch: 490 train_loss: 0.008116 val_loss: 1.063051 val_acc: 0.546667 test_loss: 1.059613 test_acc: 0.526667
Epoch: 500 train_loss: 0.007764 val_loss: 1.093225 val_acc: 0.513333 test_loss: 1.082644 test_acc: 0.506667
Epoch: 510 train_loss: 0.007758 val_loss: 1.087550 val_acc: 0.533333 test_loss: 1.120470 test_acc: 0.500000
Epoch: 520 train_loss: 0.007759 val_loss: 1.091333 val_acc: 0.513333 test_loss: 1.094744 test_acc: 0.500000
Epoch: 530 train_loss: 0.007753 val_loss: 1.106818 val_acc: 0.513333 test_loss: 1.106447 test_acc: 0.500000
Epoch: 540 train_loss: 0.007684 val_loss: 1.109499 val_acc: 0.500000 test_loss: 1.135145 test_acc: 0.480000
Epoch: 550 train_loss: 0.007722 val_loss: 1.107896 val_acc: 0.506667 test_loss: 1.137384 test_acc: 0.486667
Epoch: 560 train_loss: 0.007732 val_loss: 1.105961 val_acc: 0.500000 test_loss: 1.108313 test_acc: 0.493333
Epoch: 570 train_loss: 0.007740 val_loss: 1.086483 val_acc: 0.513333 test_loss: 1.113017 test_acc: 0.493333
Epoch: 580 train_loss: 0.007832 val_loss: 1.081390 val_acc: 0.500000 test_loss: 1.099290 test_acc: 0.486667
Epoch: 590 train_loss: 0.007838 val_loss: 1.090464 val_acc: 0.513333 test_loss: 1.112002 test_acc: 0.473333
Epoch: 600 train_loss: 0.007793 val_loss: 1.090055 val_acc: 0.513333 test_loss: 1.129301 test_acc: 0.500000
Epoch: 610 train_loss: 0.007747 val_loss: 1.095401 val_acc: 0.506667 test_loss: 1.119346 test_acc: 0.480000
Epoch: 620 train_loss: 0.007717 val_loss: 1.094614 val_acc: 0.493333 test_loss: 1.114971 test_acc: 0.493333
Epoch: 630 train_loss: 0.007760 val_loss: 1.091919 val_acc: 0.486667 test_loss: 1.096134 test_acc: 0.500000
Epoch: 640 train_loss: 0.007716 val_loss: 1.104480 val_acc: 0.506667 test_loss: 1.142646 test_acc: 0.473333
Epoch: 650 train_loss: 0.007735 val_loss: 1.102105 val_acc: 0.513333 test_loss: 1.134607 test_acc: 0.473333
