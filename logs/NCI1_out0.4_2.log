/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Namespace(device='cuda:1', dataset='NCI1', in_size=37, num_classes=2, model_name='fusion', fusion_type='early', gcn_channels=3, gcn_hidden=128, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=128, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=6, eta=0.5, ffn_dim=128, lam1=0.2, lam2=0.2, theta1=0.15, theta2=0.4, theta3=0.4, gamma=0.7, loss_log=2, folds=10, lr=1e-05, weight_decay=5e-05, batch_size=64, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.008236 val_loss: 0.479465 val_acc: 0.603406 test_loss: 0.479665 test_acc: 0.647202
Epoch: 010 train_loss: 0.007184 val_loss: 0.466389 val_acc: 0.583942 test_loss: 0.470778 test_acc: 0.579075
Epoch: 020 train_loss: 0.006930 val_loss: 0.452932 val_acc: 0.618005 test_loss: 0.451883 test_acc: 0.613139
Epoch: 030 train_loss: 0.006864 val_loss: 0.439640 val_acc: 0.671533 test_loss: 0.445456 test_acc: 0.656934
Epoch: 040 train_loss: 0.006631 val_loss: 0.436725 val_acc: 0.690998 test_loss: 0.440555 test_acc: 0.678832
Epoch: 050 train_loss: 0.006388 val_loss: 0.439512 val_acc: 0.688564 test_loss: 0.442492 test_acc: 0.693431
Epoch: 060 train_loss: 0.006320 val_loss: 0.429421 val_acc: 0.669100 test_loss: 0.431892 test_acc: 0.683698
Epoch: 070 train_loss: 0.006226 val_loss: 0.431979 val_acc: 0.700730 test_loss: 0.439414 test_acc: 0.710462
Epoch: 080 train_loss: 0.006001 val_loss: 0.432996 val_acc: 0.725061 test_loss: 0.438696 test_acc: 0.715328
Epoch: 090 train_loss: 0.005919 val_loss: 0.447628 val_acc: 0.652068 test_loss: 0.462736 test_acc: 0.676399
Epoch: 100 train_loss: 0.005882 val_loss: 0.431901 val_acc: 0.722628 test_loss: 0.449118 test_acc: 0.717762
Epoch: 110 train_loss: 0.005724 val_loss: 0.432489 val_acc: 0.722628 test_loss: 0.456499 test_acc: 0.725061
Epoch: 120 train_loss: 0.005586 val_loss: 0.427391 val_acc: 0.727494 test_loss: 0.451286 test_acc: 0.720195
Epoch: 130 train_loss: 0.005432 val_loss: 0.437971 val_acc: 0.739659 test_loss: 0.452853 test_acc: 0.737226
Epoch: 140 train_loss: 0.005397 val_loss: 0.439931 val_acc: 0.737226 test_loss: 0.459725 test_acc: 0.720195
Epoch: 150 train_loss: 0.005291 val_loss: 0.437484 val_acc: 0.742092 test_loss: 0.454745 test_acc: 0.722628
Epoch: 160 train_loss: 0.005129 val_loss: 0.439262 val_acc: 0.727494 test_loss: 0.499967 test_acc: 0.690998
Epoch: 170 train_loss: 0.005029 val_loss: 0.448255 val_acc: 0.712895 test_loss: 0.474685 test_acc: 0.705596
Epoch: 180 train_loss: 0.004975 val_loss: 0.436574 val_acc: 0.729927 test_loss: 0.483869 test_acc: 0.729927
Epoch: 190 train_loss: 0.004927 val_loss: 0.441848 val_acc: 0.720195 test_loss: 0.464950 test_acc: 0.729927
Epoch: 200 train_loss: 0.004859 val_loss: 0.439851 val_acc: 0.729927 test_loss: 0.487456 test_acc: 0.729927
Epoch: 210 train_loss: 0.004728 val_loss: 0.448018 val_acc: 0.739659 test_loss: 0.483968 test_acc: 0.729927
Epoch: 220 train_loss: 0.004961 val_loss: 0.444215 val_acc: 0.737226 test_loss: 0.478320 test_acc: 0.703163
Epoch: 230 train_loss: 0.004680 val_loss: 0.457389 val_acc: 0.734793 test_loss: 0.501475 test_acc: 0.727494
Epoch: 240 train_loss: 0.004661 val_loss: 0.457237 val_acc: 0.746959 test_loss: 0.510182 test_acc: 0.749392
Epoch: 250 train_loss: 0.004551 val_loss: 0.468087 val_acc: 0.725061 test_loss: 0.505609 test_acc: 0.717762
Epoch: 260 train_loss: 0.004567 val_loss: 0.478570 val_acc: 0.715328 test_loss: 0.524407 test_acc: 0.686131
Epoch: 270 train_loss: 0.004518 val_loss: 0.472366 val_acc: 0.715328 test_loss: 0.508418 test_acc: 0.712895
Epoch: 280 train_loss: 0.004499 val_loss: 0.465659 val_acc: 0.732360 test_loss: 0.506219 test_acc: 0.722628
Epoch: 290 train_loss: 0.004438 val_loss: 0.465698 val_acc: 0.729927 test_loss: 0.502713 test_acc: 0.727494
Epoch: 300 train_loss: 0.004332 val_loss: 0.471558 val_acc: 0.720195 test_loss: 0.509775 test_acc: 0.722628
Epoch: 310 train_loss: 0.004357 val_loss: 0.479711 val_acc: 0.708029 test_loss: 0.517174 test_acc: 0.710462
Epoch: 320 train_loss: 0.004323 val_loss: 0.480091 val_acc: 0.717762 test_loss: 0.508530 test_acc: 0.734793
Epoch: 330 train_loss: 0.004297 val_loss: 0.476579 val_acc: 0.720195 test_loss: 0.500087 test_acc: 0.715328
Epoch: 340 train_loss: 0.004232 val_loss: 0.479050 val_acc: 0.725061 test_loss: 0.516741 test_acc: 0.734793
Epoch: 350 train_loss: 0.004282 val_loss: 0.471442 val_acc: 0.725061 test_loss: 0.517079 test_acc: 0.722628
Epoch: 360 train_loss: 0.004148 val_loss: 0.487551 val_acc: 0.708029 test_loss: 0.539495 test_acc: 0.712895
Epoch: 370 train_loss: 0.004206 val_loss: 0.478773 val_acc: 0.722628 test_loss: 0.523693 test_acc: 0.708029
Epoch: 380 train_loss: 0.004122 val_loss: 0.492990 val_acc: 0.710462 test_loss: 0.529255 test_acc: 0.717762
Epoch: 390 train_loss: 0.004079 val_loss: 0.487363 val_acc: 0.715328 test_loss: 0.528803 test_acc: 0.710462
Epoch: 400 train_loss: 0.004105 val_loss: 0.481637 val_acc: 0.720195 test_loss: 0.517684 test_acc: 0.712895
Epoch: 410 train_loss: 0.004087 val_loss: 0.500429 val_acc: 0.698297 test_loss: 0.515932 test_acc: 0.708029
Epoch: 420 train_loss: 0.004130 val_loss: 0.501453 val_acc: 0.705596 test_loss: 0.543613 test_acc: 0.700730
Epoch: 430 train_loss: 0.004038 val_loss: 0.490051 val_acc: 0.708029 test_loss: 0.536004 test_acc: 0.698297
Epoch: 440 train_loss: 0.004082 val_loss: 0.493808 val_acc: 0.715328 test_loss: 0.513829 test_acc: 0.737226
Epoch: 450 train_loss: 0.004038 val_loss: 0.499338 val_acc: 0.705596 test_loss: 0.536199 test_acc: 0.715328
Epoch: 460 train_loss: 0.004021 val_loss: 0.495952 val_acc: 0.708029 test_loss: 0.534497 test_acc: 0.710462
Epoch: 470 train_loss: 0.004013 val_loss: 0.492790 val_acc: 0.727494 test_loss: 0.538577 test_acc: 0.708029
Epoch: 480 train_loss: 0.003986 val_loss: 0.491575 val_acc: 0.710462 test_loss: 0.534939 test_acc: 0.727494
Epoch: 490 train_loss: 0.003954 val_loss: 0.499262 val_acc: 0.712895 test_loss: 0.537493 test_acc: 0.715328
Epoch: 500 train_loss: 0.004024 val_loss: 0.495942 val_acc: 0.703163 test_loss: 0.532895 test_acc: 0.715328
Epoch: 510 train_loss: 0.004007 val_loss: 0.490782 val_acc: 0.717762 test_loss: 0.533074 test_acc: 0.720195
Epoch: 520 train_loss: 0.003939 val_loss: 0.498838 val_acc: 0.708029 test_loss: 0.541731 test_acc: 0.705596
Epoch: 530 train_loss: 0.003912 val_loss: 0.505059 val_acc: 0.693431 test_loss: 0.544161 test_acc: 0.700730
Epoch: 540 train_loss: 0.003938 val_loss: 0.498230 val_acc: 0.715328 test_loss: 0.533548 test_acc: 0.722628
Epoch: 550 train_loss: 0.003891 val_loss: 0.497705 val_acc: 0.710462 test_loss: 0.545790 test_acc: 0.710462
Epoch: 560 train_loss: 0.003939 val_loss: 0.499175 val_acc: 0.727494 test_loss: 0.546910 test_acc: 0.695864
Epoch: 570 train_loss: 0.003942 val_loss: 0.498744 val_acc: 0.717762 test_loss: 0.541490 test_acc: 0.722628
Epoch: 580 train_loss: 0.003886 val_loss: 0.499777 val_acc: 0.717762 test_loss: 0.549054 test_acc: 0.705596
Epoch: 590 train_loss: 0.003944 val_loss: 0.506091 val_acc: 0.700730 test_loss: 0.545566 test_acc: 0.715328
Epoch: 600 train_loss: 0.003876 val_loss: 0.506856 val_acc: 0.725061 test_loss: 0.545570 test_acc: 0.710462
Epoch: 610 train_loss: 0.003865 val_loss: 0.512484 val_acc: 0.710462 test_loss: 0.552582 test_acc: 0.698297
Epoch: 620 train_loss: 0.003858 val_loss: 0.507660 val_acc: 0.720195 test_loss: 0.544847 test_acc: 0.708029
Epoch: 630 train_loss: 0.003830 val_loss: 0.509141 val_acc: 0.710462 test_loss: 0.552739 test_acc: 0.705596
Epoch: 640 train_loss: 0.003844 val_loss: 0.507219 val_acc: 0.717762 test_loss: 0.552645 test_acc: 0.708029
Epoch: 650 train_loss: 0.003887 val_loss: 0.509935 val_acc: 0.712895 test_loss: 0.548522 test_acc: 0.708029
Epoch: 660 train_loss: 0.003866 val_loss: 0.511287 val_acc: 0.710462 test_loss: 0.561476 test_acc: 0.700730
Epoch: 670 train_loss: 0.003873 val_loss: 0.512951 val_acc: 0.729927 test_loss: 0.550863 test_acc: 0.705596
Epoch: 680 train_loss: 0.003873 val_loss: 0.509167 val_acc: 0.710462 test_loss: 0.551106 test_acc: 0.708029
Epoch: 690 train_loss: 0.003843 val_loss: 0.507794 val_acc: 0.715328 test_loss: 0.547927 test_acc: 0.703163
Epoch: 700 train_loss: 0.003799 val_loss: 0.507670 val_acc: 0.712895 test_loss: 0.555058 test_acc: 0.710462
Epoch: 710 train_loss: 0.003855 val_loss: 0.510578 val_acc: 0.712895 test_loss: 0.553481 test_acc: 0.703163
