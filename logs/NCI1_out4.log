Namespace(device='cuda:1', dataset='NCI1', in_size=37, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=1, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=500, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.006460 val_loss: 0.679934 val_acc: 0.501217 test_loss: 0.690382 test_acc: 0.496350
Epoch: 001 train_loss: 0.005378 val_loss: 0.672513 val_acc: 0.498783 test_loss: 0.688770 test_acc: 0.532847
Epoch: 002 train_loss: 0.005355 val_loss: 0.670888 val_acc: 0.569343 test_loss: 0.685571 test_acc: 0.547445
Epoch: 003 train_loss: 0.005352 val_loss: 0.667147 val_acc: 0.620438 test_loss: 0.683552 test_acc: 0.579075
Epoch: 004 train_loss: 0.005328 val_loss: 0.664023 val_acc: 0.644769 test_loss: 0.680634 test_acc: 0.581509
Epoch: 005 train_loss: 0.005324 val_loss: 0.660510 val_acc: 0.549878 test_loss: 0.681610 test_acc: 0.549878
Epoch: 006 train_loss: 0.005290 val_loss: 0.659020 val_acc: 0.686131 test_loss: 0.677165 test_acc: 0.620438
Epoch: 007 train_loss: 0.005273 val_loss: 0.658570 val_acc: 0.564477 test_loss: 0.684974 test_acc: 0.569343
Epoch: 008 train_loss: 0.005385 val_loss: 0.664526 val_acc: 0.686131 test_loss: 0.684253 test_acc: 0.635036
Epoch: 009 train_loss: 0.005257 val_loss: 0.650629 val_acc: 0.581509 test_loss: 0.676798 test_acc: 0.547445
Epoch: 010 train_loss: 0.005241 val_loss: 0.654517 val_acc: 0.698297 test_loss: 0.675550 test_acc: 0.647202
Epoch: 011 train_loss: 0.005198 val_loss: 0.638586 val_acc: 0.637470 test_loss: 0.663160 test_acc: 0.605839
Epoch: 012 train_loss: 0.005175 val_loss: 0.636990 val_acc: 0.625304 test_loss: 0.663590 test_acc: 0.593674
Epoch: 013 train_loss: 0.005138 val_loss: 0.645093 val_acc: 0.608273 test_loss: 0.672831 test_acc: 0.576642
Epoch: 014 train_loss: 0.005150 val_loss: 0.651814 val_acc: 0.591241 test_loss: 0.684812 test_acc: 0.559611
Epoch: 015 train_loss: 0.005222 val_loss: 0.639935 val_acc: 0.615572 test_loss: 0.665722 test_acc: 0.581509
Epoch: 016 train_loss: 0.005145 val_loss: 0.635610 val_acc: 0.673966 test_loss: 0.656387 test_acc: 0.649635
Epoch: 017 train_loss: 0.005172 val_loss: 0.625919 val_acc: 0.671533 test_loss: 0.652549 test_acc: 0.637470
Epoch: 018 train_loss: 0.005094 val_loss: 0.629425 val_acc: 0.715328 test_loss: 0.651802 test_acc: 0.695864
Epoch: 019 train_loss: 0.005114 val_loss: 0.622537 val_acc: 0.695864 test_loss: 0.648812 test_acc: 0.669100
Epoch: 020 train_loss: 0.005061 val_loss: 0.623101 val_acc: 0.676399 test_loss: 0.647911 test_acc: 0.666667
Epoch: 021 train_loss: 0.005063 val_loss: 0.619033 val_acc: 0.698297 test_loss: 0.645895 test_acc: 0.654501
Epoch: 022 train_loss: 0.005141 val_loss: 0.619474 val_acc: 0.729927 test_loss: 0.647900 test_acc: 0.678832
Epoch: 023 train_loss: 0.005096 val_loss: 0.622507 val_acc: 0.669100 test_loss: 0.651126 test_acc: 0.639903
Epoch: 024 train_loss: 0.005127 val_loss: 0.620842 val_acc: 0.708029 test_loss: 0.646998 test_acc: 0.664234
Epoch: 025 train_loss: 0.005047 val_loss: 0.620770 val_acc: 0.678832 test_loss: 0.653299 test_acc: 0.622871
Epoch: 026 train_loss: 0.005106 val_loss: 0.633528 val_acc: 0.635036 test_loss: 0.664770 test_acc: 0.613139
Epoch: 027 train_loss: 0.005056 val_loss: 0.628888 val_acc: 0.632603 test_loss: 0.664310 test_acc: 0.600973
Epoch: 028 train_loss: 0.005104 val_loss: 0.642942 val_acc: 0.715328 test_loss: 0.659827 test_acc: 0.693431
Epoch: 029 train_loss: 0.005043 val_loss: 0.617705 val_acc: 0.700730 test_loss: 0.643252 test_acc: 0.661800
Epoch: 030 train_loss: 0.005016 val_loss: 0.627781 val_acc: 0.635036 test_loss: 0.660209 test_acc: 0.583942
Epoch: 031 train_loss: 0.005029 val_loss: 0.618729 val_acc: 0.656934 test_loss: 0.651414 test_acc: 0.642336
Epoch: 032 train_loss: 0.004970 val_loss: 0.612372 val_acc: 0.698297 test_loss: 0.641777 test_acc: 0.652068
Epoch: 033 train_loss: 0.004953 val_loss: 0.613752 val_acc: 0.712895 test_loss: 0.640515 test_acc: 0.681265
Epoch: 034 train_loss: 0.004965 val_loss: 0.618003 val_acc: 0.656934 test_loss: 0.653884 test_acc: 0.635036
Epoch: 035 train_loss: 0.004956 val_loss: 0.617472 val_acc: 0.656934 test_loss: 0.647064 test_acc: 0.630170
Epoch: 036 train_loss: 0.004950 val_loss: 0.619418 val_acc: 0.700730 test_loss: 0.641113 test_acc: 0.690998
Epoch: 037 train_loss: 0.004953 val_loss: 0.609111 val_acc: 0.686131 test_loss: 0.638777 test_acc: 0.664234
Epoch: 038 train_loss: 0.004914 val_loss: 0.609586 val_acc: 0.673966 test_loss: 0.648019 test_acc: 0.639903
Epoch: 039 train_loss: 0.004901 val_loss: 0.617733 val_acc: 0.669100 test_loss: 0.655450 test_acc: 0.622871
Epoch: 040 train_loss: 0.004878 val_loss: 0.610274 val_acc: 0.710462 test_loss: 0.636959 test_acc: 0.671533
Epoch: 041 train_loss: 0.004875 val_loss: 0.614631 val_acc: 0.708029 test_loss: 0.640408 test_acc: 0.673966
Epoch: 042 train_loss: 0.004904 val_loss: 0.614865 val_acc: 0.744526 test_loss: 0.644371 test_acc: 0.686131
Epoch: 043 train_loss: 0.004915 val_loss: 0.616487 val_acc: 0.693431 test_loss: 0.650339 test_acc: 0.639903
Epoch: 044 train_loss: 0.004928 val_loss: 0.609911 val_acc: 0.717762 test_loss: 0.640774 test_acc: 0.683698
