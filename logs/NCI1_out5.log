Namespace(device='cuda:0', dataset='NCI1', in_size=37, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.5, num_fusion_layers=4, folds=10, loss_log=0, lr=0.0001, weight_decay=1e-06, batch_size=128, epoches=500, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.005540 val_loss: 0.672113 val_acc: 0.498783 test_loss: 0.690233 test_acc: 0.481752
Epoch: 001 train_loss: 0.005396 val_loss: 0.672795 val_acc: 0.520681 test_loss: 0.684991 test_acc: 0.489051
Epoch: 002 train_loss: 0.005372 val_loss: 0.671788 val_acc: 0.520681 test_loss: 0.684595 test_acc: 0.496350
Epoch: 003 train_loss: 0.005359 val_loss: 0.672427 val_acc: 0.510949 test_loss: 0.686947 test_acc: 0.479319
Epoch: 004 train_loss: 0.005365 val_loss: 0.672699 val_acc: 0.574209 test_loss: 0.682610 test_acc: 0.535280
Epoch: 005 train_loss: 0.005347 val_loss: 0.668381 val_acc: 0.513382 test_loss: 0.684163 test_acc: 0.491484
Epoch: 006 train_loss: 0.005337 val_loss: 0.666318 val_acc: 0.503650 test_loss: 0.684487 test_acc: 0.498783
Epoch: 007 train_loss: 0.005278 val_loss: 0.643362 val_acc: 0.644769 test_loss: 0.660720 test_acc: 0.618005
Epoch: 008 train_loss: 0.005213 val_loss: 0.643847 val_acc: 0.615572 test_loss: 0.666338 test_acc: 0.554745
Epoch: 009 train_loss: 0.005191 val_loss: 0.673939 val_acc: 0.503650 test_loss: 0.699529 test_acc: 0.447689
Epoch: 010 train_loss: 0.005300 val_loss: 0.632600 val_acc: 0.661800 test_loss: 0.668201 test_acc: 0.588808
Epoch: 011 train_loss: 0.005172 val_loss: 0.632782 val_acc: 0.659367 test_loss: 0.657651 test_acc: 0.622871
Epoch: 012 train_loss: 0.005154 val_loss: 0.632191 val_acc: 0.625304 test_loss: 0.667547 test_acc: 0.564477
Epoch: 013 train_loss: 0.005085 val_loss: 0.605522 val_acc: 0.693431 test_loss: 0.649037 test_acc: 0.622871
Epoch: 014 train_loss: 0.005106 val_loss: 0.629911 val_acc: 0.620438 test_loss: 0.661018 test_acc: 0.581509
Epoch: 015 train_loss: 0.005046 val_loss: 0.604848 val_acc: 0.686131 test_loss: 0.651198 test_acc: 0.600973
Epoch: 016 train_loss: 0.005013 val_loss: 0.618781 val_acc: 0.703163 test_loss: 0.642276 test_acc: 0.654501
Epoch: 017 train_loss: 0.005000 val_loss: 0.608503 val_acc: 0.676399 test_loss: 0.644121 test_acc: 0.608273
Epoch: 018 train_loss: 0.004955 val_loss: 0.604646 val_acc: 0.686131 test_loss: 0.644021 test_acc: 0.625304
Epoch: 019 train_loss: 0.004874 val_loss: 0.594976 val_acc: 0.661800 test_loss: 0.638793 test_acc: 0.625304
Epoch: 020 train_loss: 0.004769 val_loss: 0.598339 val_acc: 0.656934 test_loss: 0.641446 test_acc: 0.639903
Epoch: 021 train_loss: 0.004916 val_loss: 0.600073 val_acc: 0.729927 test_loss: 0.635085 test_acc: 0.652068
Epoch: 022 train_loss: 0.004799 val_loss: 0.561759 val_acc: 0.698297 test_loss: 0.631278 test_acc: 0.654501
Epoch: 023 train_loss: 0.004743 val_loss: 0.572848 val_acc: 0.708029 test_loss: 0.613144 test_acc: 0.678832
Epoch: 024 train_loss: 0.004637 val_loss: 0.573370 val_acc: 0.715328 test_loss: 0.621077 test_acc: 0.664234
Epoch: 025 train_loss: 0.004795 val_loss: 0.575602 val_acc: 0.698297 test_loss: 0.617439 test_acc: 0.676399
Epoch: 026 train_loss: 0.004690 val_loss: 0.601099 val_acc: 0.625304 test_loss: 0.640154 test_acc: 0.613139
Epoch: 027 train_loss: 0.004714 val_loss: 0.594649 val_acc: 0.664234 test_loss: 0.636867 test_acc: 0.610706
Epoch: 028 train_loss: 0.004675 val_loss: 0.586493 val_acc: 0.693431 test_loss: 0.630153 test_acc: 0.652068
Epoch: 029 train_loss: 0.004535 val_loss: 0.580881 val_acc: 0.722628 test_loss: 0.613034 test_acc: 0.664234
Epoch: 030 train_loss: 0.004500 val_loss: 0.566445 val_acc: 0.715328 test_loss: 0.615102 test_acc: 0.656934
Epoch: 031 train_loss: 0.004497 val_loss: 0.580902 val_acc: 0.673966 test_loss: 0.614255 test_acc: 0.652068
Epoch: 032 train_loss: 0.004495 val_loss: 0.564384 val_acc: 0.715328 test_loss: 0.614771 test_acc: 0.661800
Epoch: 033 train_loss: 0.004458 val_loss: 0.561549 val_acc: 0.710462 test_loss: 0.613541 test_acc: 0.659367
Epoch: 034 train_loss: 0.004473 val_loss: 0.595856 val_acc: 0.637470 test_loss: 0.627873 test_acc: 0.635036
Epoch: 035 train_loss: 0.004470 val_loss: 0.553924 val_acc: 0.727494 test_loss: 0.596980 test_acc: 0.690998
Epoch: 036 train_loss: 0.004383 val_loss: 0.568686 val_acc: 0.722628 test_loss: 0.595720 test_acc: 0.681265
Epoch: 037 train_loss: 0.004383 val_loss: 0.573459 val_acc: 0.695864 test_loss: 0.634865 test_acc: 0.664234
Epoch: 038 train_loss: 0.004333 val_loss: 0.582408 val_acc: 0.700730 test_loss: 0.631750 test_acc: 0.673966
Epoch: 039 train_loss: 0.004207 val_loss: 0.605460 val_acc: 0.661800 test_loss: 0.653718 test_acc: 0.644769
Epoch: 040 train_loss: 0.004301 val_loss: 0.571427 val_acc: 0.676399 test_loss: 0.608132 test_acc: 0.666667
Epoch: 041 train_loss: 0.004232 val_loss: 0.577102 val_acc: 0.708029 test_loss: 0.609062 test_acc: 0.678832
Epoch: 042 train_loss: 0.004200 val_loss: 0.590523 val_acc: 0.654501 test_loss: 0.617457 test_acc: 0.664234
Epoch: 043 train_loss: 0.004193 val_loss: 0.566765 val_acc: 0.746959 test_loss: 0.610596 test_acc: 0.673966
Epoch: 044 train_loss: 0.004148 val_loss: 0.597528 val_acc: 0.727494 test_loss: 0.585451 test_acc: 0.698297
Epoch: 045 train_loss: 0.004148 val_loss: 0.574687 val_acc: 0.725061 test_loss: 0.584157 test_acc: 0.693431
Epoch: 046 train_loss: 0.004021 val_loss: 0.572526 val_acc: 0.703163 test_loss: 0.594374 test_acc: 0.695864
Epoch: 047 train_loss: 0.004057 val_loss: 0.560155 val_acc: 0.737226 test_loss: 0.604985 test_acc: 0.695864
