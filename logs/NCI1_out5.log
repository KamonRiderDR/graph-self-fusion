Namespace(device='cuda:0', dataset='NCI1', in_size=37, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.5, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=0, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=500, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.006496 val_loss: 0.680283 val_acc: 0.489051 test_loss: 0.692302 test_acc: 0.476886
Epoch: 001 train_loss: 0.005404 val_loss: 0.677951 val_acc: 0.491484 test_loss: 0.690695 test_acc: 0.476886
Epoch: 002 train_loss: 0.005378 val_loss: 0.677299 val_acc: 0.501217 test_loss: 0.688904 test_acc: 0.491484
Epoch: 003 train_loss: 0.005366 val_loss: 0.673054 val_acc: 0.496350 test_loss: 0.690110 test_acc: 0.481752
Epoch: 004 train_loss: 0.005343 val_loss: 0.672088 val_acc: 0.571776 test_loss: 0.686358 test_acc: 0.554745
Epoch: 005 train_loss: 0.005340 val_loss: 0.662468 val_acc: 0.564477 test_loss: 0.681697 test_acc: 0.559611
Epoch: 006 train_loss: 0.005307 val_loss: 0.659916 val_acc: 0.598540 test_loss: 0.678739 test_acc: 0.569343
Epoch: 007 train_loss: 0.005310 val_loss: 0.657741 val_acc: 0.537713 test_loss: 0.682613 test_acc: 0.554745
Epoch: 008 train_loss: 0.005298 val_loss: 0.675130 val_acc: 0.659367 test_loss: 0.688733 test_acc: 0.625304
Epoch: 009 train_loss: 0.005274 val_loss: 0.647009 val_acc: 0.613139 test_loss: 0.672048 test_acc: 0.576642
Epoch: 010 train_loss: 0.005218 val_loss: 0.655955 val_acc: 0.700730 test_loss: 0.675803 test_acc: 0.644769
Epoch: 011 train_loss: 0.005199 val_loss: 0.650976 val_acc: 0.569343 test_loss: 0.674836 test_acc: 0.574209
Epoch: 012 train_loss: 0.005179 val_loss: 0.643853 val_acc: 0.591241 test_loss: 0.667366 test_acc: 0.598540
Epoch: 013 train_loss: 0.005135 val_loss: 0.642315 val_acc: 0.608273 test_loss: 0.667029 test_acc: 0.593674
Epoch: 014 train_loss: 0.005118 val_loss: 0.655404 val_acc: 0.579075 test_loss: 0.686565 test_acc: 0.557178
Epoch: 015 train_loss: 0.005182 val_loss: 0.635690 val_acc: 0.712895 test_loss: 0.658456 test_acc: 0.669100
Epoch: 016 train_loss: 0.005130 val_loss: 0.633834 val_acc: 0.700730 test_loss: 0.655260 test_acc: 0.644769
Epoch: 017 train_loss: 0.005141 val_loss: 0.632594 val_acc: 0.737226 test_loss: 0.653914 test_acc: 0.678832
Epoch: 018 train_loss: 0.005061 val_loss: 0.618157 val_acc: 0.720195 test_loss: 0.648456 test_acc: 0.683698
Epoch: 019 train_loss: 0.005085 val_loss: 0.626270 val_acc: 0.725061 test_loss: 0.655225 test_acc: 0.683698
Epoch: 020 train_loss: 0.005044 val_loss: 0.618907 val_acc: 0.683698 test_loss: 0.649461 test_acc: 0.635036
Epoch: 021 train_loss: 0.005025 val_loss: 0.614314 val_acc: 0.712895 test_loss: 0.644939 test_acc: 0.652068
Epoch: 022 train_loss: 0.005098 val_loss: 0.622029 val_acc: 0.737226 test_loss: 0.653251 test_acc: 0.683698
Epoch: 023 train_loss: 0.005065 val_loss: 0.622520 val_acc: 0.676399 test_loss: 0.654287 test_acc: 0.618005
Epoch: 024 train_loss: 0.005078 val_loss: 0.614413 val_acc: 0.705596 test_loss: 0.649121 test_acc: 0.659367
Epoch: 025 train_loss: 0.005019 val_loss: 0.621263 val_acc: 0.676399 test_loss: 0.655656 test_acc: 0.620438
Epoch: 026 train_loss: 0.005101 val_loss: 0.644203 val_acc: 0.605839 test_loss: 0.678930 test_acc: 0.564477
Epoch: 027 train_loss: 0.005047 val_loss: 0.608607 val_acc: 0.720195 test_loss: 0.647805 test_acc: 0.666667
Epoch: 028 train_loss: 0.005052 val_loss: 0.648445 val_acc: 0.695864 test_loss: 0.666701 test_acc: 0.627737
Epoch: 029 train_loss: 0.005046 val_loss: 0.614249 val_acc: 0.712895 test_loss: 0.643548 test_acc: 0.664234
Epoch: 030 train_loss: 0.004988 val_loss: 0.634445 val_acc: 0.632603 test_loss: 0.669379 test_acc: 0.571776
Epoch: 031 train_loss: 0.004971 val_loss: 0.616029 val_acc: 0.683698 test_loss: 0.645517 test_acc: 0.647202
Epoch: 032 train_loss: 0.004929 val_loss: 0.608385 val_acc: 0.717762 test_loss: 0.639967 test_acc: 0.671533
Epoch: 033 train_loss: 0.004916 val_loss: 0.607599 val_acc: 0.708029 test_loss: 0.641792 test_acc: 0.654501
Epoch: 034 train_loss: 0.004904 val_loss: 0.610633 val_acc: 0.700730 test_loss: 0.637523 test_acc: 0.673966
Epoch: 035 train_loss: 0.004940 val_loss: 0.611038 val_acc: 0.708029 test_loss: 0.641968 test_acc: 0.647202
Epoch: 036 train_loss: 0.004889 val_loss: 0.609698 val_acc: 0.715328 test_loss: 0.642709 test_acc: 0.673966
Epoch: 037 train_loss: 0.004924 val_loss: 0.617294 val_acc: 0.749392 test_loss: 0.642091 test_acc: 0.700730
Epoch: 038 train_loss: 0.004874 val_loss: 0.608600 val_acc: 0.693431 test_loss: 0.646004 test_acc: 0.666667
Epoch: 039 train_loss: 0.004862 val_loss: 0.607204 val_acc: 0.705596 test_loss: 0.641736 test_acc: 0.683698
Epoch: 040 train_loss: 0.004853 val_loss: 0.606977 val_acc: 0.710462 test_loss: 0.644905 test_acc: 0.654501
Epoch: 041 train_loss: 0.004817 val_loss: 0.612225 val_acc: 0.712895 test_loss: 0.638161 test_acc: 0.671533
Epoch: 042 train_loss: 0.004824 val_loss: 0.607855 val_acc: 0.708029 test_loss: 0.636668 test_acc: 0.669100
Epoch: 043 train_loss: 0.004838 val_loss: 0.611814 val_acc: 0.737226 test_loss: 0.642175 test_acc: 0.678832
Epoch: 044 train_loss: 0.004815 val_loss: 0.615545 val_acc: 0.722628 test_loss: 0.643917 test_acc: 0.698297
Epoch: 045 train_loss: 0.004830 val_loss: 0.607553 val_acc: 0.700730 test_loss: 0.648542 test_acc: 0.666667
