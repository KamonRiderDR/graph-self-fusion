Namespace(device='cuda:2', dataset='NCI1', in_size=37, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.6, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=500, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.006407 val_loss: 0.684374 val_acc: 0.491484 test_loss: 0.693841 test_acc: 0.476886
Epoch: 001 train_loss: 0.005410 val_loss: 0.679345 val_acc: 0.491484 test_loss: 0.692178 test_acc: 0.476886
Epoch: 002 train_loss: 0.005385 val_loss: 0.677973 val_acc: 0.489051 test_loss: 0.689686 test_acc: 0.476886
Epoch: 003 train_loss: 0.005368 val_loss: 0.673151 val_acc: 0.489051 test_loss: 0.688541 test_acc: 0.476886
Epoch: 004 train_loss: 0.005339 val_loss: 0.671591 val_acc: 0.562044 test_loss: 0.685676 test_acc: 0.566910
Epoch: 005 train_loss: 0.005332 val_loss: 0.662387 val_acc: 0.557178 test_loss: 0.679562 test_acc: 0.564477
Epoch: 006 train_loss: 0.005296 val_loss: 0.659008 val_acc: 0.574209 test_loss: 0.678129 test_acc: 0.574209
Epoch: 007 train_loss: 0.005284 val_loss: 0.655287 val_acc: 0.571776 test_loss: 0.677167 test_acc: 0.557178
Epoch: 008 train_loss: 0.005268 val_loss: 0.670110 val_acc: 0.686131 test_loss: 0.684978 test_acc: 0.639903
Epoch: 009 train_loss: 0.005257 val_loss: 0.642606 val_acc: 0.635036 test_loss: 0.666427 test_acc: 0.605839
Epoch: 010 train_loss: 0.005192 val_loss: 0.662264 val_acc: 0.690998 test_loss: 0.679247 test_acc: 0.642336
Epoch: 011 train_loss: 0.005193 val_loss: 0.642000 val_acc: 0.610706 test_loss: 0.665925 test_acc: 0.586375
Epoch: 012 train_loss: 0.005181 val_loss: 0.653804 val_acc: 0.588808 test_loss: 0.682661 test_acc: 0.554745
Epoch: 013 train_loss: 0.005106 val_loss: 0.644839 val_acc: 0.596107 test_loss: 0.675186 test_acc: 0.547445
Epoch: 014 train_loss: 0.005122 val_loss: 0.637173 val_acc: 0.627737 test_loss: 0.667162 test_acc: 0.593674
Epoch: 015 train_loss: 0.005117 val_loss: 0.631843 val_acc: 0.700730 test_loss: 0.652691 test_acc: 0.671533
Epoch: 016 train_loss: 0.005084 val_loss: 0.622825 val_acc: 0.681265 test_loss: 0.650460 test_acc: 0.652068
Epoch: 017 train_loss: 0.005083 val_loss: 0.619441 val_acc: 0.700730 test_loss: 0.649585 test_acc: 0.661800
Epoch: 018 train_loss: 0.005069 val_loss: 0.623180 val_acc: 0.705596 test_loss: 0.653071 test_acc: 0.664234
Epoch: 019 train_loss: 0.005067 val_loss: 0.624780 val_acc: 0.676399 test_loss: 0.655388 test_acc: 0.627737
Epoch: 020 train_loss: 0.005017 val_loss: 0.618102 val_acc: 0.671533 test_loss: 0.645916 test_acc: 0.644769
Epoch: 021 train_loss: 0.005015 val_loss: 0.615977 val_acc: 0.688564 test_loss: 0.644844 test_acc: 0.661800
Epoch: 022 train_loss: 0.005062 val_loss: 0.622034 val_acc: 0.722628 test_loss: 0.648127 test_acc: 0.676399
Epoch: 023 train_loss: 0.005044 val_loss: 0.620833 val_acc: 0.669100 test_loss: 0.650170 test_acc: 0.642336
Epoch: 024 train_loss: 0.005034 val_loss: 0.613580 val_acc: 0.683698 test_loss: 0.647706 test_acc: 0.639903
Epoch: 025 train_loss: 0.004993 val_loss: 0.628807 val_acc: 0.618005 test_loss: 0.662846 test_acc: 0.605839
Epoch: 026 train_loss: 0.005020 val_loss: 0.615853 val_acc: 0.673966 test_loss: 0.649972 test_acc: 0.632603
Epoch: 027 train_loss: 0.005004 val_loss: 0.630714 val_acc: 0.639903 test_loss: 0.667787 test_acc: 0.610706
Epoch: 028 train_loss: 0.005039 val_loss: 0.618780 val_acc: 0.710462 test_loss: 0.648722 test_acc: 0.666667
Epoch: 029 train_loss: 0.004952 val_loss: 0.610307 val_acc: 0.742092 test_loss: 0.639783 test_acc: 0.698297
Epoch: 030 train_loss: 0.004918 val_loss: 0.612503 val_acc: 0.681265 test_loss: 0.647814 test_acc: 0.656934
Epoch: 031 train_loss: 0.004895 val_loss: 0.614040 val_acc: 0.686131 test_loss: 0.649388 test_acc: 0.644769
Epoch: 032 train_loss: 0.004904 val_loss: 0.606495 val_acc: 0.705596 test_loss: 0.638990 test_acc: 0.673966
Epoch: 033 train_loss: 0.004866 val_loss: 0.611046 val_acc: 0.708029 test_loss: 0.641078 test_acc: 0.676399
Epoch: 034 train_loss: 0.004865 val_loss: 0.610053 val_acc: 0.708029 test_loss: 0.636047 test_acc: 0.678832
Epoch: 035 train_loss: 0.004872 val_loss: 0.612437 val_acc: 0.673966 test_loss: 0.643406 test_acc: 0.644769
Epoch: 036 train_loss: 0.004854 val_loss: 0.616430 val_acc: 0.722628 test_loss: 0.638841 test_acc: 0.703163
Epoch: 037 train_loss: 0.004881 val_loss: 0.607655 val_acc: 0.737226 test_loss: 0.633357 test_acc: 0.729927
Epoch: 038 train_loss: 0.004846 val_loss: 0.610625 val_acc: 0.681265 test_loss: 0.638203 test_acc: 0.661800
Epoch: 039 train_loss: 0.004825 val_loss: 0.607345 val_acc: 0.727494 test_loss: 0.638911 test_acc: 0.676399
Epoch: 040 train_loss: 0.004820 val_loss: 0.611636 val_acc: 0.686131 test_loss: 0.640933 test_acc: 0.666667
Epoch: 041 train_loss: 0.004779 val_loss: 0.613132 val_acc: 0.727494 test_loss: 0.631046 test_acc: 0.681265
Epoch: 042 train_loss: 0.004792 val_loss: 0.606071 val_acc: 0.703163 test_loss: 0.635230 test_acc: 0.683698
Epoch: 043 train_loss: 0.004849 val_loss: 0.609443 val_acc: 0.749392 test_loss: 0.629023 test_acc: 0.703163
Epoch: 044 train_loss: 0.004758 val_loss: 0.606280 val_acc: 0.734793 test_loss: 0.631839 test_acc: 0.708029
