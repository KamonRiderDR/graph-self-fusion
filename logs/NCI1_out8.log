Namespace(device='cuda:3', dataset='NCI1', in_size=37, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=37, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=37, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.8, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=3, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=500, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.006827 val_loss: 0.684768 val_acc: 0.491484 test_loss: 0.692572 test_acc: 0.476886
Epoch: 001 train_loss: 0.005402 val_loss: 0.676572 val_acc: 0.489051 test_loss: 0.689142 test_acc: 0.476886
Epoch: 002 train_loss: 0.005366 val_loss: 0.675373 val_acc: 0.503650 test_loss: 0.687224 test_acc: 0.486618
Epoch: 003 train_loss: 0.005353 val_loss: 0.670982 val_acc: 0.508516 test_loss: 0.685792 test_acc: 0.496350
Epoch: 004 train_loss: 0.005344 val_loss: 0.669313 val_acc: 0.525547 test_loss: 0.686465 test_acc: 0.503650
Epoch: 005 train_loss: 0.005321 val_loss: 0.665334 val_acc: 0.532847 test_loss: 0.681832 test_acc: 0.547445
Epoch: 006 train_loss: 0.005292 val_loss: 0.656450 val_acc: 0.571776 test_loss: 0.675302 test_acc: 0.564477
Epoch: 007 train_loss: 0.005274 val_loss: 0.653887 val_acc: 0.566910 test_loss: 0.675243 test_acc: 0.564477
Epoch: 008 train_loss: 0.005272 val_loss: 0.656553 val_acc: 0.693431 test_loss: 0.674283 test_acc: 0.639903
Epoch: 009 train_loss: 0.005262 val_loss: 0.648065 val_acc: 0.627737 test_loss: 0.669324 test_acc: 0.596107
Epoch: 010 train_loss: 0.005208 val_loss: 0.662940 val_acc: 0.683698 test_loss: 0.681351 test_acc: 0.630170
Epoch: 011 train_loss: 0.005202 val_loss: 0.642369 val_acc: 0.622871 test_loss: 0.668334 test_acc: 0.576642
Epoch: 012 train_loss: 0.005154 val_loss: 0.644237 val_acc: 0.596107 test_loss: 0.671970 test_acc: 0.562044
Epoch: 013 train_loss: 0.005102 val_loss: 0.633587 val_acc: 0.615572 test_loss: 0.665251 test_acc: 0.596107
Epoch: 014 train_loss: 0.005099 val_loss: 0.651717 val_acc: 0.591241 test_loss: 0.686480 test_acc: 0.549878
Epoch: 015 train_loss: 0.005144 val_loss: 0.635343 val_acc: 0.618005 test_loss: 0.664888 test_acc: 0.593674
Epoch: 016 train_loss: 0.005082 val_loss: 0.629842 val_acc: 0.742092 test_loss: 0.658496 test_acc: 0.673966
Epoch: 017 train_loss: 0.005115 val_loss: 0.638617 val_acc: 0.742092 test_loss: 0.663300 test_acc: 0.666667
Epoch: 018 train_loss: 0.005135 val_loss: 0.628817 val_acc: 0.632603 test_loss: 0.660088 test_acc: 0.618005
Epoch: 019 train_loss: 0.005079 val_loss: 0.635245 val_acc: 0.720195 test_loss: 0.662864 test_acc: 0.666667
Epoch: 020 train_loss: 0.005054 val_loss: 0.619649 val_acc: 0.666667 test_loss: 0.653187 test_acc: 0.622871
Epoch: 021 train_loss: 0.005039 val_loss: 0.621955 val_acc: 0.639903 test_loss: 0.655853 test_acc: 0.610706
Epoch: 022 train_loss: 0.004999 val_loss: 0.619362 val_acc: 0.676399 test_loss: 0.651246 test_acc: 0.654501
Epoch: 023 train_loss: 0.005005 val_loss: 0.618102 val_acc: 0.683698 test_loss: 0.653740 test_acc: 0.637470
Epoch: 024 train_loss: 0.005022 val_loss: 0.615797 val_acc: 0.739659 test_loss: 0.654991 test_acc: 0.666667
Epoch: 025 train_loss: 0.004979 val_loss: 0.616501 val_acc: 0.671533 test_loss: 0.655801 test_acc: 0.644769
Epoch: 026 train_loss: 0.005006 val_loss: 0.620787 val_acc: 0.666667 test_loss: 0.654309 test_acc: 0.630170
Epoch: 027 train_loss: 0.004984 val_loss: 0.619334 val_acc: 0.632603 test_loss: 0.659749 test_acc: 0.613139
Epoch: 028 train_loss: 0.005018 val_loss: 0.620876 val_acc: 0.727494 test_loss: 0.650083 test_acc: 0.669100
Epoch: 029 train_loss: 0.004951 val_loss: 0.613524 val_acc: 0.712895 test_loss: 0.648758 test_acc: 0.664234
Epoch: 030 train_loss: 0.004917 val_loss: 0.615387 val_acc: 0.656934 test_loss: 0.656523 test_acc: 0.635036
Epoch: 031 train_loss: 0.004909 val_loss: 0.610280 val_acc: 0.708029 test_loss: 0.647303 test_acc: 0.654501
Epoch: 032 train_loss: 0.004892 val_loss: 0.614435 val_acc: 0.705596 test_loss: 0.646078 test_acc: 0.642336
Epoch: 033 train_loss: 0.004882 val_loss: 0.608452 val_acc: 0.717762 test_loss: 0.640401 test_acc: 0.659367
Epoch: 034 train_loss: 0.004881 val_loss: 0.613470 val_acc: 0.734793 test_loss: 0.643818 test_acc: 0.693431
Epoch: 035 train_loss: 0.004960 val_loss: 0.622933 val_acc: 0.746959 test_loss: 0.650962 test_acc: 0.683698
Epoch: 036 train_loss: 0.004865 val_loss: 0.622960 val_acc: 0.749392 test_loss: 0.653694 test_acc: 0.693431
Epoch: 037 train_loss: 0.004915 val_loss: 0.614324 val_acc: 0.729927 test_loss: 0.641329 test_acc: 0.683698
Epoch: 038 train_loss: 0.004843 val_loss: 0.609949 val_acc: 0.681265 test_loss: 0.642044 test_acc: 0.642336
Epoch: 039 train_loss: 0.004825 val_loss: 0.606204 val_acc: 0.727494 test_loss: 0.647858 test_acc: 0.666667
Epoch: 040 train_loss: 0.004862 val_loss: 0.609980 val_acc: 0.710462 test_loss: 0.653313 test_acc: 0.637470
Epoch: 041 train_loss: 0.004842 val_loss: 0.609730 val_acc: 0.708029 test_loss: 0.644069 test_acc: 0.656934
Epoch: 042 train_loss: 0.004797 val_loss: 0.604058 val_acc: 0.722628 test_loss: 0.638356 test_acc: 0.664234
Epoch: 043 train_loss: 0.004804 val_loss: 0.603934 val_acc: 0.732360 test_loss: 0.639480 test_acc: 0.683698
Epoch: 044 train_loss: 0.004777 val_loss: 0.601846 val_acc: 0.729927 test_loss: 0.635295 test_acc: 0.690998
Epoch: 045 train_loss: 0.004795 val_loss: 0.602568 val_acc: 0.742092 test_loss: 0.639486 test_acc: 0.683698
