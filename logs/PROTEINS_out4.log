Namespace(device='cuda:1', dataset='PROTEINS', in_size=3, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=3, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=3, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.4, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=1, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.007779 val_loss: 0.685203 val_acc: 0.585586 test_loss: 0.675343 test_acc: 0.598214
Epoch: 001 train_loss: 0.005297 val_loss: 0.653857 val_acc: 0.594595 test_loss: 0.655550 test_acc: 0.589286
Epoch: 002 train_loss: 0.005040 val_loss: 0.641796 val_acc: 0.603604 test_loss: 0.650441 test_acc: 0.589286
Epoch: 003 train_loss: 0.004952 val_loss: 0.629388 val_acc: 0.594595 test_loss: 0.641827 test_acc: 0.589286
Epoch: 004 train_loss: 0.004952 val_loss: 0.626720 val_acc: 0.594595 test_loss: 0.641572 test_acc: 0.589286
Epoch: 005 train_loss: 0.004935 val_loss: 0.626088 val_acc: 0.594595 test_loss: 0.639262 test_acc: 0.589286
Epoch: 006 train_loss: 0.004875 val_loss: 0.627508 val_acc: 0.594595 test_loss: 0.641751 test_acc: 0.589286
Epoch: 007 train_loss: 0.004859 val_loss: 0.621325 val_acc: 0.594595 test_loss: 0.640895 test_acc: 0.589286
Epoch: 008 train_loss: 0.004867 val_loss: 0.616400 val_acc: 0.594595 test_loss: 0.638058 test_acc: 0.589286
Epoch: 009 train_loss: 0.004872 val_loss: 0.624949 val_acc: 0.594595 test_loss: 0.642316 test_acc: 0.589286
Epoch: 010 train_loss: 0.004836 val_loss: 0.618579 val_acc: 0.594595 test_loss: 0.636353 test_acc: 0.589286
Epoch: 011 train_loss: 0.004805 val_loss: 0.611705 val_acc: 0.594595 test_loss: 0.633535 test_acc: 0.589286
Epoch: 012 train_loss: 0.004814 val_loss: 0.617511 val_acc: 0.594595 test_loss: 0.638800 test_acc: 0.589286
Epoch: 013 train_loss: 0.004790 val_loss: 0.611383 val_acc: 0.612613 test_loss: 0.634565 test_acc: 0.607143
Epoch: 014 train_loss: 0.004810 val_loss: 0.619545 val_acc: 0.594595 test_loss: 0.641787 test_acc: 0.589286
Epoch: 015 train_loss: 0.004782 val_loss: 0.607036 val_acc: 0.639640 test_loss: 0.632270 test_acc: 0.607143
Epoch: 016 train_loss: 0.004861 val_loss: 0.626357 val_acc: 0.594595 test_loss: 0.648322 test_acc: 0.589286
Epoch: 017 train_loss: 0.004754 val_loss: 0.605856 val_acc: 0.639640 test_loss: 0.633213 test_acc: 0.607143
Epoch: 018 train_loss: 0.004770 val_loss: 0.613026 val_acc: 0.612613 test_loss: 0.641352 test_acc: 0.607143
Epoch: 019 train_loss: 0.004771 val_loss: 0.603382 val_acc: 0.639640 test_loss: 0.631576 test_acc: 0.616071
Epoch: 020 train_loss: 0.004747 val_loss: 0.602293 val_acc: 0.639640 test_loss: 0.631931 test_acc: 0.607143
Epoch: 021 train_loss: 0.004730 val_loss: 0.597469 val_acc: 0.639640 test_loss: 0.627977 test_acc: 0.598214
Epoch: 022 train_loss: 0.004738 val_loss: 0.600074 val_acc: 0.630631 test_loss: 0.629892 test_acc: 0.607143
Epoch: 023 train_loss: 0.004708 val_loss: 0.592977 val_acc: 0.639640 test_loss: 0.627332 test_acc: 0.607143
Epoch: 024 train_loss: 0.004723 val_loss: 0.596406 val_acc: 0.639640 test_loss: 0.627266 test_acc: 0.607143
Epoch: 025 train_loss: 0.004706 val_loss: 0.592916 val_acc: 0.639640 test_loss: 0.626883 test_acc: 0.607143
Epoch: 026 train_loss: 0.004716 val_loss: 0.590173 val_acc: 0.639640 test_loss: 0.625972 test_acc: 0.607143
Epoch: 027 train_loss: 0.004712 val_loss: 0.602048 val_acc: 0.639640 test_loss: 0.637238 test_acc: 0.616071
Epoch: 028 train_loss: 0.004717 val_loss: 0.590926 val_acc: 0.639640 test_loss: 0.625568 test_acc: 0.607143
Epoch: 029 train_loss: 0.004711 val_loss: 0.598667 val_acc: 0.630631 test_loss: 0.634650 test_acc: 0.598214
Epoch: 030 train_loss: 0.004697 val_loss: 0.591838 val_acc: 0.639640 test_loss: 0.626367 test_acc: 0.607143
Epoch: 031 train_loss: 0.004693 val_loss: 0.597960 val_acc: 0.630631 test_loss: 0.636940 test_acc: 0.607143
Epoch: 032 train_loss: 0.004703 val_loss: 0.587592 val_acc: 0.711712 test_loss: 0.630188 test_acc: 0.678571
Epoch: 033 train_loss: 0.004695 val_loss: 0.597210 val_acc: 0.630631 test_loss: 0.635015 test_acc: 0.607143
Epoch: 034 train_loss: 0.004697 val_loss: 0.586880 val_acc: 0.702703 test_loss: 0.623406 test_acc: 0.678571
Epoch: 035 train_loss: 0.004675 val_loss: 0.585984 val_acc: 0.639640 test_loss: 0.624942 test_acc: 0.607143
Epoch: 036 train_loss: 0.004691 val_loss: 0.587772 val_acc: 0.693694 test_loss: 0.622858 test_acc: 0.660714
Epoch: 037 train_loss: 0.004676 val_loss: 0.588131 val_acc: 0.639640 test_loss: 0.625689 test_acc: 0.607143
Epoch: 038 train_loss: 0.004666 val_loss: 0.583937 val_acc: 0.693694 test_loss: 0.623375 test_acc: 0.660714
Epoch: 039 train_loss: 0.004682 val_loss: 0.592610 val_acc: 0.711712 test_loss: 0.631144 test_acc: 0.669643
Epoch: 040 train_loss: 0.004705 val_loss: 0.585434 val_acc: 0.693694 test_loss: 0.623097 test_acc: 0.678571
Epoch: 041 train_loss: 0.004685 val_loss: 0.593093 val_acc: 0.684685 test_loss: 0.632035 test_acc: 0.651786
Epoch: 042 train_loss: 0.004714 val_loss: 0.583089 val_acc: 0.720721 test_loss: 0.623363 test_acc: 0.687500
Epoch: 043 train_loss: 0.004718 val_loss: 0.594111 val_acc: 0.693694 test_loss: 0.634203 test_acc: 0.660714
Epoch: 044 train_loss: 0.004678 val_loss: 0.587979 val_acc: 0.693694 test_loss: 0.627001 test_acc: 0.669643
Epoch: 045 train_loss: 0.004670 val_loss: 0.587756 val_acc: 0.702703 test_loss: 0.625934 test_acc: 0.669643
Epoch: 046 train_loss: 0.004647 val_loss: 0.576556 val_acc: 0.747748 test_loss: 0.622083 test_acc: 0.687500
Epoch: 047 train_loss: 0.004664 val_loss: 0.585919 val_acc: 0.693694 test_loss: 0.623811 test_acc: 0.669643
Epoch: 048 train_loss: 0.004645 val_loss: 0.581587 val_acc: 0.729730 test_loss: 0.620039 test_acc: 0.678571
Epoch: 049 train_loss: 0.004649 val_loss: 0.587055 val_acc: 0.693694 test_loss: 0.627864 test_acc: 0.669643
Epoch: 050 train_loss: 0.004654 val_loss: 0.584127 val_acc: 0.711712 test_loss: 0.621625 test_acc: 0.678571
Epoch: 051 train_loss: 0.004668 val_loss: 0.592051 val_acc: 0.693694 test_loss: 0.633932 test_acc: 0.651786
Epoch: 052 train_loss: 0.004661 val_loss: 0.580415 val_acc: 0.738739 test_loss: 0.620632 test_acc: 0.687500
Epoch: 053 train_loss: 0.004660 val_loss: 0.586376 val_acc: 0.702703 test_loss: 0.626844 test_acc: 0.669643
Epoch: 054 train_loss: 0.004704 val_loss: 0.595177 val_acc: 0.702703 test_loss: 0.630345 test_acc: 0.678571
Epoch: 055 train_loss: 0.004648 val_loss: 0.583391 val_acc: 0.711712 test_loss: 0.621010 test_acc: 0.678571
Epoch: 056 train_loss: 0.004642 val_loss: 0.581782 val_acc: 0.711712 test_loss: 0.622069 test_acc: 0.678571
Epoch: 057 train_loss: 0.004653 val_loss: 0.575987 val_acc: 0.756757 test_loss: 0.621545 test_acc: 0.696429
Epoch: 058 train_loss: 0.004642 val_loss: 0.589479 val_acc: 0.693694 test_loss: 0.624115 test_acc: 0.678571
Epoch: 059 train_loss: 0.004628 val_loss: 0.581838 val_acc: 0.729730 test_loss: 0.619031 test_acc: 0.687500
Epoch: 060 train_loss: 0.004634 val_loss: 0.580028 val_acc: 0.729730 test_loss: 0.619880 test_acc: 0.687500
Epoch: 061 train_loss: 0.004683 val_loss: 0.590427 val_acc: 0.720721 test_loss: 0.625754 test_acc: 0.687500
Epoch: 062 train_loss: 0.004642 val_loss: 0.580821 val_acc: 0.729730 test_loss: 0.620260 test_acc: 0.678571
Epoch: 063 train_loss: 0.004632 val_loss: 0.584992 val_acc: 0.720721 test_loss: 0.625402 test_acc: 0.678571
Epoch: 064 train_loss: 0.004646 val_loss: 0.581232 val_acc: 0.738739 test_loss: 0.623414 test_acc: 0.687500
Epoch: 065 train_loss: 0.004665 val_loss: 0.589799 val_acc: 0.720721 test_loss: 0.631398 test_acc: 0.678571
Epoch: 066 train_loss: 0.004662 val_loss: 0.588013 val_acc: 0.747748 test_loss: 0.625743 test_acc: 0.696429
Epoch: 067 train_loss: 0.004651 val_loss: 0.578621 val_acc: 0.711712 test_loss: 0.621594 test_acc: 0.678571
Epoch: 068 train_loss: 0.004637 val_loss: 0.581280 val_acc: 0.729730 test_loss: 0.622009 test_acc: 0.678571
Epoch: 069 train_loss: 0.004647 val_loss: 0.587278 val_acc: 0.747748 test_loss: 0.624736 test_acc: 0.687500
Epoch: 070 train_loss: 0.004621 val_loss: 0.576135 val_acc: 0.729730 test_loss: 0.622654 test_acc: 0.696429
Epoch: 071 train_loss: 0.004621 val_loss: 0.578957 val_acc: 0.729730 test_loss: 0.622038 test_acc: 0.678571
Epoch: 072 train_loss: 0.004605 val_loss: 0.581929 val_acc: 0.747748 test_loss: 0.623932 test_acc: 0.705357
Epoch: 073 train_loss: 0.004638 val_loss: 0.580393 val_acc: 0.729730 test_loss: 0.625390 test_acc: 0.705357
Epoch: 074 train_loss: 0.004662 val_loss: 0.585408 val_acc: 0.729730 test_loss: 0.629158 test_acc: 0.678571
Epoch: 075 train_loss: 0.004657 val_loss: 0.581636 val_acc: 0.747748 test_loss: 0.625193 test_acc: 0.714286
Epoch: 076 train_loss: 0.004646 val_loss: 0.588739 val_acc: 0.729730 test_loss: 0.629412 test_acc: 0.669643
Epoch: 077 train_loss: 0.004603 val_loss: 0.576234 val_acc: 0.756757 test_loss: 0.618492 test_acc: 0.714286
Epoch: 078 train_loss: 0.004597 val_loss: 0.577969 val_acc: 0.738739 test_loss: 0.619182 test_acc: 0.687500
Epoch: 079 train_loss: 0.004597 val_loss: 0.577168 val_acc: 0.738739 test_loss: 0.616994 test_acc: 0.696429
Epoch: 080 train_loss: 0.004586 val_loss: 0.578168 val_acc: 0.738739 test_loss: 0.618711 test_acc: 0.696429
Epoch: 081 train_loss: 0.004580 val_loss: 0.576769 val_acc: 0.738739 test_loss: 0.617964 test_acc: 0.714286
Epoch: 082 train_loss: 0.004582 val_loss: 0.576964 val_acc: 0.738739 test_loss: 0.616426 test_acc: 0.714286
Epoch: 083 train_loss: 0.004591 val_loss: 0.578694 val_acc: 0.738739 test_loss: 0.616250 test_acc: 0.705357
Epoch: 084 train_loss: 0.004587 val_loss: 0.576142 val_acc: 0.738739 test_loss: 0.617611 test_acc: 0.714286
Epoch: 085 train_loss: 0.004585 val_loss: 0.581108 val_acc: 0.738739 test_loss: 0.617963 test_acc: 0.687500
Epoch: 086 train_loss: 0.004581 val_loss: 0.574591 val_acc: 0.738739 test_loss: 0.617041 test_acc: 0.714286
Epoch: 087 train_loss: 0.004580 val_loss: 0.575060 val_acc: 0.738739 test_loss: 0.616170 test_acc: 0.705357
Epoch: 088 train_loss: 0.004586 val_loss: 0.581115 val_acc: 0.738739 test_loss: 0.618415 test_acc: 0.696429
Epoch: 089 train_loss: 0.004571 val_loss: 0.578101 val_acc: 0.738739 test_loss: 0.618295 test_acc: 0.714286
