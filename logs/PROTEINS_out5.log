Namespace(device='cuda:0', dataset='PROTEINS', in_size=3, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=3, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=3, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.5, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=0, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.009451 val_loss: 0.717518 val_acc: 0.576577 test_loss: 0.725584 test_acc: 0.607143
Epoch: 001 train_loss: 0.005465 val_loss: 0.667042 val_acc: 0.594595 test_loss: 0.674010 test_acc: 0.589286
Epoch: 002 train_loss: 0.005291 val_loss: 0.683978 val_acc: 0.594595 test_loss: 0.692962 test_acc: 0.589286
Epoch: 003 train_loss: 0.005188 val_loss: 0.651355 val_acc: 0.594595 test_loss: 0.658696 test_acc: 0.589286
Epoch: 004 train_loss: 0.005046 val_loss: 0.649235 val_acc: 0.594595 test_loss: 0.656749 test_acc: 0.589286
Epoch: 005 train_loss: 0.004951 val_loss: 0.639894 val_acc: 0.594595 test_loss: 0.650573 test_acc: 0.589286
Epoch: 006 train_loss: 0.004901 val_loss: 0.639752 val_acc: 0.594595 test_loss: 0.649284 test_acc: 0.589286
Epoch: 007 train_loss: 0.004900 val_loss: 0.638790 val_acc: 0.594595 test_loss: 0.649531 test_acc: 0.589286
Epoch: 008 train_loss: 0.004900 val_loss: 0.636105 val_acc: 0.594595 test_loss: 0.650119 test_acc: 0.589286
Epoch: 009 train_loss: 0.004870 val_loss: 0.636596 val_acc: 0.594595 test_loss: 0.649129 test_acc: 0.589286
Epoch: 010 train_loss: 0.004872 val_loss: 0.637678 val_acc: 0.594595 test_loss: 0.653794 test_acc: 0.589286
Epoch: 011 train_loss: 0.004879 val_loss: 0.638747 val_acc: 0.594595 test_loss: 0.655589 test_acc: 0.589286
Epoch: 012 train_loss: 0.004889 val_loss: 0.635824 val_acc: 0.594595 test_loss: 0.653208 test_acc: 0.589286
Epoch: 013 train_loss: 0.004860 val_loss: 0.634506 val_acc: 0.594595 test_loss: 0.649436 test_acc: 0.589286
Epoch: 014 train_loss: 0.004840 val_loss: 0.623377 val_acc: 0.594595 test_loss: 0.640209 test_acc: 0.589286
Epoch: 015 train_loss: 0.004798 val_loss: 0.620017 val_acc: 0.594595 test_loss: 0.636466 test_acc: 0.589286
Epoch: 016 train_loss: 0.004789 val_loss: 0.617237 val_acc: 0.594595 test_loss: 0.635772 test_acc: 0.589286
Epoch: 017 train_loss: 0.004774 val_loss: 0.615560 val_acc: 0.594595 test_loss: 0.633894 test_acc: 0.598214
Epoch: 018 train_loss: 0.004769 val_loss: 0.614091 val_acc: 0.594595 test_loss: 0.636478 test_acc: 0.598214
Epoch: 019 train_loss: 0.004759 val_loss: 0.609310 val_acc: 0.603604 test_loss: 0.630834 test_acc: 0.607143
Epoch: 020 train_loss: 0.004749 val_loss: 0.608744 val_acc: 0.612613 test_loss: 0.633358 test_acc: 0.607143
Epoch: 021 train_loss: 0.004741 val_loss: 0.604576 val_acc: 0.603604 test_loss: 0.630317 test_acc: 0.607143
Epoch: 022 train_loss: 0.004733 val_loss: 0.601516 val_acc: 0.612613 test_loss: 0.629850 test_acc: 0.625000
Epoch: 023 train_loss: 0.004725 val_loss: 0.603561 val_acc: 0.657658 test_loss: 0.631034 test_acc: 0.669643
Epoch: 024 train_loss: 0.004736 val_loss: 0.599986 val_acc: 0.648649 test_loss: 0.626705 test_acc: 0.669643
Epoch: 025 train_loss: 0.004746 val_loss: 0.609182 val_acc: 0.612613 test_loss: 0.642744 test_acc: 0.616071
Epoch: 026 train_loss: 0.004733 val_loss: 0.599897 val_acc: 0.693694 test_loss: 0.629105 test_acc: 0.678571
Epoch: 027 train_loss: 0.004748 val_loss: 0.600680 val_acc: 0.639640 test_loss: 0.633625 test_acc: 0.651786
Epoch: 028 train_loss: 0.004700 val_loss: 0.594651 val_acc: 0.666667 test_loss: 0.626930 test_acc: 0.660714
Epoch: 029 train_loss: 0.004687 val_loss: 0.595259 val_acc: 0.657658 test_loss: 0.629967 test_acc: 0.669643
Epoch: 030 train_loss: 0.004692 val_loss: 0.594236 val_acc: 0.702703 test_loss: 0.627895 test_acc: 0.687500
Epoch: 031 train_loss: 0.004716 val_loss: 0.593545 val_acc: 0.702703 test_loss: 0.627009 test_acc: 0.687500
Epoch: 032 train_loss: 0.004707 val_loss: 0.594709 val_acc: 0.693694 test_loss: 0.630560 test_acc: 0.678571
Epoch: 033 train_loss: 0.004678 val_loss: 0.591267 val_acc: 0.693694 test_loss: 0.628105 test_acc: 0.678571
Epoch: 034 train_loss: 0.004676 val_loss: 0.586088 val_acc: 0.729730 test_loss: 0.622806 test_acc: 0.696429
Epoch: 035 train_loss: 0.004670 val_loss: 0.587182 val_acc: 0.693694 test_loss: 0.625118 test_acc: 0.678571
Epoch: 036 train_loss: 0.004680 val_loss: 0.587460 val_acc: 0.720721 test_loss: 0.624470 test_acc: 0.696429
Epoch: 037 train_loss: 0.004675 val_loss: 0.591727 val_acc: 0.693694 test_loss: 0.631368 test_acc: 0.669643
Epoch: 038 train_loss: 0.004679 val_loss: 0.593066 val_acc: 0.684685 test_loss: 0.633429 test_acc: 0.669643
Epoch: 039 train_loss: 0.004691 val_loss: 0.592169 val_acc: 0.729730 test_loss: 0.626571 test_acc: 0.687500
Epoch: 040 train_loss: 0.004675 val_loss: 0.589427 val_acc: 0.711712 test_loss: 0.630291 test_acc: 0.678571
Epoch: 041 train_loss: 0.004666 val_loss: 0.584562 val_acc: 0.738739 test_loss: 0.625606 test_acc: 0.696429
Epoch: 042 train_loss: 0.004692 val_loss: 0.590130 val_acc: 0.738739 test_loss: 0.628101 test_acc: 0.678571
Epoch: 043 train_loss: 0.004670 val_loss: 0.586627 val_acc: 0.729730 test_loss: 0.627982 test_acc: 0.678571
Epoch: 044 train_loss: 0.004659 val_loss: 0.584796 val_acc: 0.738739 test_loss: 0.626620 test_acc: 0.687500
Epoch: 045 train_loss: 0.004666 val_loss: 0.591105 val_acc: 0.738739 test_loss: 0.631191 test_acc: 0.678571
Epoch: 046 train_loss: 0.004688 val_loss: 0.582912 val_acc: 0.738739 test_loss: 0.621990 test_acc: 0.696429
Epoch: 047 train_loss: 0.004678 val_loss: 0.590967 val_acc: 0.684685 test_loss: 0.630037 test_acc: 0.678571
Epoch: 048 train_loss: 0.004643 val_loss: 0.583453 val_acc: 0.738739 test_loss: 0.622894 test_acc: 0.678571
Epoch: 049 train_loss: 0.004635 val_loss: 0.589069 val_acc: 0.738739 test_loss: 0.630412 test_acc: 0.678571
Epoch: 050 train_loss: 0.004645 val_loss: 0.583790 val_acc: 0.729730 test_loss: 0.623295 test_acc: 0.705357
Epoch: 051 train_loss: 0.004666 val_loss: 0.592390 val_acc: 0.720721 test_loss: 0.636767 test_acc: 0.678571
Epoch: 052 train_loss: 0.004643 val_loss: 0.583203 val_acc: 0.729730 test_loss: 0.620538 test_acc: 0.705357
Epoch: 053 train_loss: 0.004614 val_loss: 0.586638 val_acc: 0.729730 test_loss: 0.623003 test_acc: 0.687500
Epoch: 054 train_loss: 0.004624 val_loss: 0.588111 val_acc: 0.738739 test_loss: 0.624702 test_acc: 0.669643
Epoch: 055 train_loss: 0.004614 val_loss: 0.580266 val_acc: 0.747748 test_loss: 0.621614 test_acc: 0.705357
Epoch: 056 train_loss: 0.004660 val_loss: 0.602705 val_acc: 0.684685 test_loss: 0.645250 test_acc: 0.705357
Epoch: 057 train_loss: 0.004684 val_loss: 0.587633 val_acc: 0.729730 test_loss: 0.622558 test_acc: 0.705357
Epoch: 058 train_loss: 0.004677 val_loss: 0.588568 val_acc: 0.738739 test_loss: 0.630169 test_acc: 0.669643
Epoch: 059 train_loss: 0.004619 val_loss: 0.578142 val_acc: 0.729730 test_loss: 0.620367 test_acc: 0.696429
Epoch: 060 train_loss: 0.004626 val_loss: 0.581244 val_acc: 0.738739 test_loss: 0.621384 test_acc: 0.678571
Epoch: 061 train_loss: 0.004650 val_loss: 0.588423 val_acc: 0.729730 test_loss: 0.623549 test_acc: 0.678571
Epoch: 062 train_loss: 0.004644 val_loss: 0.585011 val_acc: 0.738739 test_loss: 0.625990 test_acc: 0.678571
Epoch: 063 train_loss: 0.004636 val_loss: 0.596511 val_acc: 0.729730 test_loss: 0.639938 test_acc: 0.669643
Epoch: 064 train_loss: 0.004677 val_loss: 0.597075 val_acc: 0.729730 test_loss: 0.628885 test_acc: 0.696429
Epoch: 065 train_loss: 0.004652 val_loss: 0.584549 val_acc: 0.729730 test_loss: 0.626674 test_acc: 0.705357
Epoch: 066 train_loss: 0.004626 val_loss: 0.580559 val_acc: 0.747748 test_loss: 0.619909 test_acc: 0.705357
Epoch: 067 train_loss: 0.004623 val_loss: 0.577941 val_acc: 0.738739 test_loss: 0.621505 test_acc: 0.696429
Epoch: 068 train_loss: 0.004617 val_loss: 0.580162 val_acc: 0.738739 test_loss: 0.623062 test_acc: 0.687500
Epoch: 069 train_loss: 0.004611 val_loss: 0.584216 val_acc: 0.729730 test_loss: 0.619337 test_acc: 0.696429
Epoch: 070 train_loss: 0.004594 val_loss: 0.581138 val_acc: 0.729730 test_loss: 0.619194 test_acc: 0.696429
Epoch: 071 train_loss: 0.004584 val_loss: 0.581087 val_acc: 0.738739 test_loss: 0.620926 test_acc: 0.687500
Epoch: 072 train_loss: 0.004581 val_loss: 0.582498 val_acc: 0.729730 test_loss: 0.622584 test_acc: 0.687500
Epoch: 073 train_loss: 0.004620 val_loss: 0.584408 val_acc: 0.720721 test_loss: 0.623801 test_acc: 0.687500
Epoch: 074 train_loss: 0.004647 val_loss: 0.589641 val_acc: 0.738739 test_loss: 0.627636 test_acc: 0.678571
Epoch: 075 train_loss: 0.004646 val_loss: 0.586653 val_acc: 0.729730 test_loss: 0.625386 test_acc: 0.696429
Epoch: 076 train_loss: 0.004640 val_loss: 0.595578 val_acc: 0.729730 test_loss: 0.627213 test_acc: 0.678571
Epoch: 077 train_loss: 0.004619 val_loss: 0.582033 val_acc: 0.729730 test_loss: 0.619060 test_acc: 0.696429
Epoch: 078 train_loss: 0.004605 val_loss: 0.578027 val_acc: 0.738739 test_loss: 0.617412 test_acc: 0.687500
Epoch: 079 train_loss: 0.004581 val_loss: 0.583283 val_acc: 0.729730 test_loss: 0.620782 test_acc: 0.687500
Epoch: 080 train_loss: 0.004585 val_loss: 0.580897 val_acc: 0.720721 test_loss: 0.619462 test_acc: 0.687500
Epoch: 081 train_loss: 0.004580 val_loss: 0.579242 val_acc: 0.729730 test_loss: 0.619537 test_acc: 0.714286
Epoch: 082 train_loss: 0.004614 val_loss: 0.584900 val_acc: 0.729730 test_loss: 0.621633 test_acc: 0.687500
Epoch: 083 train_loss: 0.004630 val_loss: 0.584033 val_acc: 0.729730 test_loss: 0.619528 test_acc: 0.696429
Epoch: 084 train_loss: 0.004599 val_loss: 0.584934 val_acc: 0.729730 test_loss: 0.627020 test_acc: 0.669643
Epoch: 085 train_loss: 0.004570 val_loss: 0.580979 val_acc: 0.729730 test_loss: 0.619814 test_acc: 0.696429
Epoch: 086 train_loss: 0.004596 val_loss: 0.585735 val_acc: 0.729730 test_loss: 0.627027 test_acc: 0.669643
Epoch: 087 train_loss: 0.004582 val_loss: 0.578301 val_acc: 0.747748 test_loss: 0.618386 test_acc: 0.696429
Epoch: 088 train_loss: 0.004620 val_loss: 0.585443 val_acc: 0.738739 test_loss: 0.621008 test_acc: 0.696429
Epoch: 089 train_loss: 0.004596 val_loss: 0.586647 val_acc: 0.720721 test_loss: 0.621423 test_acc: 0.696429
Epoch: 090 train_loss: 0.004584 val_loss: 0.574102 val_acc: 0.747748 test_loss: 0.618443 test_acc: 0.723214
Epoch: 091 train_loss: 0.004643 val_loss: 0.605232 val_acc: 0.684685 test_loss: 0.641493 test_acc: 0.705357
