Namespace(device='cuda:2', dataset='PROTEINS', in_size=3, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=3, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=3, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.6, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=2, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.007907 val_loss: 0.689124 val_acc: 0.621622 test_loss: 0.695625 test_acc: 0.607143
Epoch: 001 train_loss: 0.005299 val_loss: 0.659428 val_acc: 0.594595 test_loss: 0.664371 test_acc: 0.616071
Epoch: 002 train_loss: 0.005154 val_loss: 0.673351 val_acc: 0.594595 test_loss: 0.681597 test_acc: 0.607143
Epoch: 003 train_loss: 0.005095 val_loss: 0.641483 val_acc: 0.594595 test_loss: 0.650882 test_acc: 0.607143
Epoch: 004 train_loss: 0.005042 val_loss: 0.638295 val_acc: 0.594595 test_loss: 0.648237 test_acc: 0.607143
Epoch: 005 train_loss: 0.004940 val_loss: 0.636954 val_acc: 0.594595 test_loss: 0.648924 test_acc: 0.598214
Epoch: 006 train_loss: 0.004888 val_loss: 0.630809 val_acc: 0.594595 test_loss: 0.644445 test_acc: 0.598214
Epoch: 007 train_loss: 0.004859 val_loss: 0.634364 val_acc: 0.594595 test_loss: 0.648261 test_acc: 0.607143
Epoch: 008 train_loss: 0.004868 val_loss: 0.628009 val_acc: 0.594595 test_loss: 0.642298 test_acc: 0.589286
Epoch: 009 train_loss: 0.004833 val_loss: 0.622179 val_acc: 0.594595 test_loss: 0.638052 test_acc: 0.607143
Epoch: 010 train_loss: 0.004805 val_loss: 0.630467 val_acc: 0.594595 test_loss: 0.649527 test_acc: 0.607143
Epoch: 011 train_loss: 0.004819 val_loss: 0.616224 val_acc: 0.603604 test_loss: 0.634440 test_acc: 0.607143
Epoch: 012 train_loss: 0.004807 val_loss: 0.622174 val_acc: 0.594595 test_loss: 0.643145 test_acc: 0.598214
Epoch: 013 train_loss: 0.004830 val_loss: 0.618969 val_acc: 0.603604 test_loss: 0.640673 test_acc: 0.616071
Epoch: 014 train_loss: 0.004825 val_loss: 0.631309 val_acc: 0.603604 test_loss: 0.652948 test_acc: 0.607143
Epoch: 015 train_loss: 0.004819 val_loss: 0.620286 val_acc: 0.612613 test_loss: 0.640503 test_acc: 0.625000
Epoch: 016 train_loss: 0.004801 val_loss: 0.612567 val_acc: 0.603604 test_loss: 0.636105 test_acc: 0.616071
Epoch: 017 train_loss: 0.004802 val_loss: 0.606373 val_acc: 0.612613 test_loss: 0.630309 test_acc: 0.625000
Epoch: 018 train_loss: 0.004791 val_loss: 0.617893 val_acc: 0.612613 test_loss: 0.643041 test_acc: 0.625000
Epoch: 019 train_loss: 0.004774 val_loss: 0.611269 val_acc: 0.612613 test_loss: 0.635851 test_acc: 0.633929
Epoch: 020 train_loss: 0.004756 val_loss: 0.609337 val_acc: 0.612613 test_loss: 0.636454 test_acc: 0.625000
Epoch: 021 train_loss: 0.004801 val_loss: 0.610919 val_acc: 0.612613 test_loss: 0.638661 test_acc: 0.633929
Epoch: 022 train_loss: 0.004787 val_loss: 0.601730 val_acc: 0.612613 test_loss: 0.632662 test_acc: 0.642857
Epoch: 023 train_loss: 0.004745 val_loss: 0.598845 val_acc: 0.711712 test_loss: 0.635576 test_acc: 0.651786
Epoch: 024 train_loss: 0.004805 val_loss: 0.610942 val_acc: 0.648649 test_loss: 0.637481 test_acc: 0.669643
Epoch: 025 train_loss: 0.004769 val_loss: 0.599825 val_acc: 0.639640 test_loss: 0.630353 test_acc: 0.660714
Epoch: 026 train_loss: 0.004729 val_loss: 0.595572 val_acc: 0.666667 test_loss: 0.633287 test_acc: 0.651786
Epoch: 027 train_loss: 0.004700 val_loss: 0.593962 val_acc: 0.657658 test_loss: 0.627499 test_acc: 0.642857
Epoch: 028 train_loss: 0.004688 val_loss: 0.594870 val_acc: 0.666667 test_loss: 0.625414 test_acc: 0.651786
Epoch: 029 train_loss: 0.004689 val_loss: 0.594866 val_acc: 0.657658 test_loss: 0.629527 test_acc: 0.651786
Epoch: 030 train_loss: 0.004676 val_loss: 0.593442 val_acc: 0.702703 test_loss: 0.628424 test_acc: 0.669643
Epoch: 031 train_loss: 0.004692 val_loss: 0.594518 val_acc: 0.693694 test_loss: 0.629299 test_acc: 0.678571
Epoch: 032 train_loss: 0.004688 val_loss: 0.586277 val_acc: 0.711712 test_loss: 0.626804 test_acc: 0.669643
Epoch: 033 train_loss: 0.004657 val_loss: 0.593118 val_acc: 0.693694 test_loss: 0.630244 test_acc: 0.678571
Epoch: 034 train_loss: 0.004667 val_loss: 0.586261 val_acc: 0.711712 test_loss: 0.624709 test_acc: 0.669643
Epoch: 035 train_loss: 0.004664 val_loss: 0.585956 val_acc: 0.711712 test_loss: 0.623921 test_acc: 0.660714
Epoch: 036 train_loss: 0.004664 val_loss: 0.588828 val_acc: 0.711712 test_loss: 0.624422 test_acc: 0.669643
Epoch: 037 train_loss: 0.004670 val_loss: 0.593009 val_acc: 0.702703 test_loss: 0.630347 test_acc: 0.669643
Epoch: 038 train_loss: 0.004672 val_loss: 0.597738 val_acc: 0.702703 test_loss: 0.634422 test_acc: 0.678571
Epoch: 039 train_loss: 0.004680 val_loss: 0.589666 val_acc: 0.729730 test_loss: 0.625328 test_acc: 0.678571
Epoch: 040 train_loss: 0.004660 val_loss: 0.587292 val_acc: 0.702703 test_loss: 0.626996 test_acc: 0.669643
Epoch: 041 train_loss: 0.004654 val_loss: 0.580818 val_acc: 0.711712 test_loss: 0.622488 test_acc: 0.678571
Epoch: 042 train_loss: 0.004662 val_loss: 0.589117 val_acc: 0.729730 test_loss: 0.623462 test_acc: 0.669643
Epoch: 043 train_loss: 0.004643 val_loss: 0.584967 val_acc: 0.711712 test_loss: 0.623840 test_acc: 0.678571
Epoch: 044 train_loss: 0.004651 val_loss: 0.581099 val_acc: 0.720721 test_loss: 0.622146 test_acc: 0.678571
Epoch: 045 train_loss: 0.004644 val_loss: 0.586322 val_acc: 0.729730 test_loss: 0.622375 test_acc: 0.669643
Epoch: 046 train_loss: 0.004655 val_loss: 0.577415 val_acc: 0.720721 test_loss: 0.631295 test_acc: 0.696429
Epoch: 047 train_loss: 0.004703 val_loss: 0.593131 val_acc: 0.711712 test_loss: 0.628651 test_acc: 0.669643
Epoch: 048 train_loss: 0.004665 val_loss: 0.588425 val_acc: 0.738739 test_loss: 0.621889 test_acc: 0.669643
Epoch: 049 train_loss: 0.004638 val_loss: 0.595029 val_acc: 0.711712 test_loss: 0.636238 test_acc: 0.669643
Epoch: 050 train_loss: 0.004672 val_loss: 0.583003 val_acc: 0.729730 test_loss: 0.622354 test_acc: 0.687500
Epoch: 051 train_loss: 0.004704 val_loss: 0.613017 val_acc: 0.666667 test_loss: 0.651236 test_acc: 0.678571
Epoch: 052 train_loss: 0.004723 val_loss: 0.600370 val_acc: 0.702703 test_loss: 0.635335 test_acc: 0.687500
Epoch: 053 train_loss: 0.004708 val_loss: 0.593940 val_acc: 0.738739 test_loss: 0.632022 test_acc: 0.678571
Epoch: 054 train_loss: 0.004707 val_loss: 0.582828 val_acc: 0.738739 test_loss: 0.621475 test_acc: 0.687500
Epoch: 055 train_loss: 0.004625 val_loss: 0.579034 val_acc: 0.747748 test_loss: 0.622673 test_acc: 0.705357
Epoch: 056 train_loss: 0.004623 val_loss: 0.583603 val_acc: 0.711712 test_loss: 0.624392 test_acc: 0.669643
Epoch: 057 train_loss: 0.004619 val_loss: 0.578579 val_acc: 0.756757 test_loss: 0.620752 test_acc: 0.714286
Epoch: 058 train_loss: 0.004625 val_loss: 0.586584 val_acc: 0.738739 test_loss: 0.625554 test_acc: 0.669643
Epoch: 059 train_loss: 0.004604 val_loss: 0.576112 val_acc: 0.738739 test_loss: 0.618911 test_acc: 0.705357
Epoch: 060 train_loss: 0.004611 val_loss: 0.580830 val_acc: 0.738739 test_loss: 0.622110 test_acc: 0.687500
Epoch: 061 train_loss: 0.004694 val_loss: 0.593107 val_acc: 0.738739 test_loss: 0.626636 test_acc: 0.678571
Epoch: 062 train_loss: 0.004641 val_loss: 0.580972 val_acc: 0.729730 test_loss: 0.622423 test_acc: 0.687500
Epoch: 063 train_loss: 0.004630 val_loss: 0.577398 val_acc: 0.738739 test_loss: 0.619147 test_acc: 0.705357
Epoch: 064 train_loss: 0.004631 val_loss: 0.585510 val_acc: 0.747748 test_loss: 0.624225 test_acc: 0.705357
Epoch: 065 train_loss: 0.004642 val_loss: 0.586888 val_acc: 0.729730 test_loss: 0.632017 test_acc: 0.696429
Epoch: 066 train_loss: 0.004629 val_loss: 0.578058 val_acc: 0.738739 test_loss: 0.617159 test_acc: 0.714286
Epoch: 067 train_loss: 0.004589 val_loss: 0.573238 val_acc: 0.738739 test_loss: 0.618601 test_acc: 0.714286
Epoch: 068 train_loss: 0.004596 val_loss: 0.574735 val_acc: 0.738739 test_loss: 0.617412 test_acc: 0.714286
Epoch: 069 train_loss: 0.004592 val_loss: 0.578005 val_acc: 0.747748 test_loss: 0.616545 test_acc: 0.705357
Epoch: 070 train_loss: 0.004587 val_loss: 0.574672 val_acc: 0.738739 test_loss: 0.619159 test_acc: 0.714286
Epoch: 071 train_loss: 0.004577 val_loss: 0.577720 val_acc: 0.738739 test_loss: 0.618387 test_acc: 0.696429
Epoch: 072 train_loss: 0.004567 val_loss: 0.575128 val_acc: 0.747748 test_loss: 0.616728 test_acc: 0.714286
Epoch: 073 train_loss: 0.004593 val_loss: 0.576909 val_acc: 0.738739 test_loss: 0.619689 test_acc: 0.705357
Epoch: 074 train_loss: 0.004595 val_loss: 0.583201 val_acc: 0.738739 test_loss: 0.621605 test_acc: 0.696429
Epoch: 075 train_loss: 0.004599 val_loss: 0.585981 val_acc: 0.729730 test_loss: 0.626644 test_acc: 0.696429
Epoch: 076 train_loss: 0.004646 val_loss: 0.590739 val_acc: 0.738739 test_loss: 0.628139 test_acc: 0.687500
Epoch: 077 train_loss: 0.004598 val_loss: 0.577176 val_acc: 0.738739 test_loss: 0.615614 test_acc: 0.705357
Epoch: 078 train_loss: 0.004589 val_loss: 0.575512 val_acc: 0.738739 test_loss: 0.615119 test_acc: 0.705357
Epoch: 079 train_loss: 0.004581 val_loss: 0.579990 val_acc: 0.738739 test_loss: 0.616186 test_acc: 0.714286
Epoch: 080 train_loss: 0.004576 val_loss: 0.578710 val_acc: 0.738739 test_loss: 0.619991 test_acc: 0.705357
Epoch: 081 train_loss: 0.004571 val_loss: 0.574852 val_acc: 0.783784 test_loss: 0.617162 test_acc: 0.714286
Epoch: 082 train_loss: 0.004560 val_loss: 0.575913 val_acc: 0.738739 test_loss: 0.614534 test_acc: 0.696429
Epoch: 083 train_loss: 0.004554 val_loss: 0.575656 val_acc: 0.756757 test_loss: 0.616706 test_acc: 0.714286
Epoch: 084 train_loss: 0.004591 val_loss: 0.579230 val_acc: 0.738739 test_loss: 0.622644 test_acc: 0.705357
Epoch: 085 train_loss: 0.004571 val_loss: 0.575333 val_acc: 0.747748 test_loss: 0.615770 test_acc: 0.696429
Epoch: 086 train_loss: 0.004565 val_loss: 0.574471 val_acc: 0.747748 test_loss: 0.619625 test_acc: 0.696429
Epoch: 087 train_loss: 0.004567 val_loss: 0.576972 val_acc: 0.756757 test_loss: 0.618724 test_acc: 0.705357
Epoch: 088 train_loss: 0.004667 val_loss: 0.591743 val_acc: 0.738739 test_loss: 0.630133 test_acc: 0.714286
Epoch: 089 train_loss: 0.004629 val_loss: 0.577712 val_acc: 0.747748 test_loss: 0.619001 test_acc: 0.696429
Epoch: 090 train_loss: 0.004590 val_loss: 0.578201 val_acc: 0.738739 test_loss: 0.622187 test_acc: 0.705357
Epoch: 091 train_loss: 0.004584 val_loss: 0.581867 val_acc: 0.738739 test_loss: 0.621522 test_acc: 0.687500
