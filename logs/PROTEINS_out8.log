Namespace(device='cuda:3', dataset='PROTEINS', in_size=3, num_classes=2, fusion_type='early', gcn_channels=3, gcn_hidden=64, gcn_layers=4, gcn_dropout=0.1, trans_num_layers=4, input_node_dim=3, hidden_node_dim=32, input_edge_dim=0, hidden_edge_dim=32, ouput_dim=None, n_heads=4, max_in_degree=5, max_out_degree=5, max_path_distance=5, hidden_dim=64, num_layers=4, num_features=3, num_heads=8, dropout=0.1, pos_encoding='gcn', att_dropout=0.1, d_k=64, d_v=64, pos_embed_type='s', alpha=0.8, num_fusion_layers=4, eta=0.4, lam1=0.2, lam2=0.2, theta1=0.1, theta2=0.4, theta3=0.4, loss_log=3, folds=10, lr=0.0001, weight_decay=0.0005, batch_size=128, epoches=800, output_dim=2)
/home/dongrui/anaconda3/envs/dr/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
Fold: 0
Epoch: 000 train_loss: 0.008679 val_loss: 0.692508 val_acc: 0.594595 test_loss: 0.694656 test_acc: 0.589286
Epoch: 001 train_loss: 0.005432 val_loss: 0.672944 val_acc: 0.594595 test_loss: 0.676828 test_acc: 0.589286
Epoch: 002 train_loss: 0.005217 val_loss: 0.668524 val_acc: 0.594595 test_loss: 0.674612 test_acc: 0.589286
Epoch: 003 train_loss: 0.005100 val_loss: 0.648578 val_acc: 0.594595 test_loss: 0.653993 test_acc: 0.589286
Epoch: 004 train_loss: 0.005006 val_loss: 0.641099 val_acc: 0.594595 test_loss: 0.648437 test_acc: 0.589286
Epoch: 005 train_loss: 0.004947 val_loss: 0.640494 val_acc: 0.594595 test_loss: 0.649228 test_acc: 0.589286
Epoch: 006 train_loss: 0.004912 val_loss: 0.642047 val_acc: 0.594595 test_loss: 0.651415 test_acc: 0.589286
Epoch: 007 train_loss: 0.004897 val_loss: 0.638018 val_acc: 0.594595 test_loss: 0.648017 test_acc: 0.589286
Epoch: 008 train_loss: 0.004891 val_loss: 0.632874 val_acc: 0.594595 test_loss: 0.643820 test_acc: 0.589286
Epoch: 009 train_loss: 0.004873 val_loss: 0.632301 val_acc: 0.594595 test_loss: 0.643717 test_acc: 0.589286
Epoch: 010 train_loss: 0.004847 val_loss: 0.629778 val_acc: 0.594595 test_loss: 0.641687 test_acc: 0.589286
Epoch: 011 train_loss: 0.004843 val_loss: 0.629018 val_acc: 0.594595 test_loss: 0.640981 test_acc: 0.589286
Epoch: 012 train_loss: 0.004838 val_loss: 0.630474 val_acc: 0.594595 test_loss: 0.642012 test_acc: 0.589286
Epoch: 013 train_loss: 0.004840 val_loss: 0.630284 val_acc: 0.594595 test_loss: 0.642599 test_acc: 0.589286
Epoch: 014 train_loss: 0.004838 val_loss: 0.634853 val_acc: 0.594595 test_loss: 0.647354 test_acc: 0.589286
Epoch: 015 train_loss: 0.004820 val_loss: 0.627144 val_acc: 0.594595 test_loss: 0.639894 test_acc: 0.589286
Epoch: 016 train_loss: 0.004828 val_loss: 0.626505 val_acc: 0.594595 test_loss: 0.638411 test_acc: 0.589286
Epoch: 017 train_loss: 0.004794 val_loss: 0.621778 val_acc: 0.594595 test_loss: 0.635410 test_acc: 0.589286
Epoch: 018 train_loss: 0.004792 val_loss: 0.623415 val_acc: 0.594595 test_loss: 0.637045 test_acc: 0.589286
Epoch: 019 train_loss: 0.004805 val_loss: 0.621161 val_acc: 0.594595 test_loss: 0.635813 test_acc: 0.589286
Epoch: 020 train_loss: 0.004783 val_loss: 0.621496 val_acc: 0.594595 test_loss: 0.635988 test_acc: 0.589286
Epoch: 021 train_loss: 0.004782 val_loss: 0.617725 val_acc: 0.594595 test_loss: 0.633706 test_acc: 0.589286
Epoch: 022 train_loss: 0.004771 val_loss: 0.615215 val_acc: 0.594595 test_loss: 0.632508 test_acc: 0.589286
Epoch: 023 train_loss: 0.004756 val_loss: 0.612254 val_acc: 0.594595 test_loss: 0.629271 test_acc: 0.589286
Epoch: 024 train_loss: 0.004757 val_loss: 0.611285 val_acc: 0.594595 test_loss: 0.628213 test_acc: 0.607143
Epoch: 025 train_loss: 0.004794 val_loss: 0.624515 val_acc: 0.594595 test_loss: 0.646127 test_acc: 0.589286
Epoch: 026 train_loss: 0.004767 val_loss: 0.612283 val_acc: 0.603604 test_loss: 0.629581 test_acc: 0.607143
Epoch: 027 train_loss: 0.004758 val_loss: 0.612513 val_acc: 0.594595 test_loss: 0.631456 test_acc: 0.607143
Epoch: 028 train_loss: 0.004736 val_loss: 0.603956 val_acc: 0.603604 test_loss: 0.626235 test_acc: 0.607143
Epoch: 029 train_loss: 0.004738 val_loss: 0.612362 val_acc: 0.603604 test_loss: 0.633838 test_acc: 0.598214
Epoch: 030 train_loss: 0.004731 val_loss: 0.605674 val_acc: 0.603604 test_loss: 0.626684 test_acc: 0.616071
Epoch: 031 train_loss: 0.004715 val_loss: 0.606583 val_acc: 0.612613 test_loss: 0.630076 test_acc: 0.616071
Epoch: 032 train_loss: 0.004709 val_loss: 0.597807 val_acc: 0.630631 test_loss: 0.623905 test_acc: 0.616071
Epoch: 033 train_loss: 0.004721 val_loss: 0.609613 val_acc: 0.612613 test_loss: 0.638588 test_acc: 0.607143
Epoch: 034 train_loss: 0.004728 val_loss: 0.596806 val_acc: 0.630631 test_loss: 0.626858 test_acc: 0.642857
Epoch: 035 train_loss: 0.004688 val_loss: 0.593707 val_acc: 0.621622 test_loss: 0.623971 test_acc: 0.633929
Epoch: 036 train_loss: 0.004677 val_loss: 0.589191 val_acc: 0.657658 test_loss: 0.622448 test_acc: 0.625000
Epoch: 037 train_loss: 0.004693 val_loss: 0.594427 val_acc: 0.621622 test_loss: 0.624359 test_acc: 0.633929
Epoch: 038 train_loss: 0.004679 val_loss: 0.587537 val_acc: 0.684685 test_loss: 0.621925 test_acc: 0.642857
Epoch: 039 train_loss: 0.004718 val_loss: 0.606369 val_acc: 0.639640 test_loss: 0.642121 test_acc: 0.642857
Epoch: 040 train_loss: 0.004728 val_loss: 0.592455 val_acc: 0.729730 test_loss: 0.623920 test_acc: 0.678571
Epoch: 041 train_loss: 0.004690 val_loss: 0.589538 val_acc: 0.693694 test_loss: 0.623832 test_acc: 0.660714
Epoch: 042 train_loss: 0.004703 val_loss: 0.589595 val_acc: 0.720721 test_loss: 0.622923 test_acc: 0.687500
Epoch: 043 train_loss: 0.004762 val_loss: 0.615959 val_acc: 0.675676 test_loss: 0.644236 test_acc: 0.669643
Epoch: 044 train_loss: 0.004732 val_loss: 0.598691 val_acc: 0.729730 test_loss: 0.631429 test_acc: 0.678571
Epoch: 045 train_loss: 0.004679 val_loss: 0.588834 val_acc: 0.711712 test_loss: 0.623097 test_acc: 0.660714
Epoch: 046 train_loss: 0.004676 val_loss: 0.585878 val_acc: 0.747748 test_loss: 0.621642 test_acc: 0.687500
Epoch: 047 train_loss: 0.004673 val_loss: 0.588866 val_acc: 0.729730 test_loss: 0.627563 test_acc: 0.678571
Epoch: 048 train_loss: 0.004642 val_loss: 0.583550 val_acc: 0.729730 test_loss: 0.617967 test_acc: 0.705357
Epoch: 049 train_loss: 0.004640 val_loss: 0.590529 val_acc: 0.711712 test_loss: 0.627450 test_acc: 0.669643
Epoch: 050 train_loss: 0.004644 val_loss: 0.583779 val_acc: 0.729730 test_loss: 0.619038 test_acc: 0.705357
Epoch: 051 train_loss: 0.004654 val_loss: 0.591801 val_acc: 0.702703 test_loss: 0.629938 test_acc: 0.669643
Epoch: 052 train_loss: 0.004646 val_loss: 0.583253 val_acc: 0.738739 test_loss: 0.618610 test_acc: 0.696429
Epoch: 053 train_loss: 0.004658 val_loss: 0.584465 val_acc: 0.711712 test_loss: 0.621691 test_acc: 0.678571
Epoch: 054 train_loss: 0.004680 val_loss: 0.587695 val_acc: 0.720721 test_loss: 0.623151 test_acc: 0.669643
Epoch: 055 train_loss: 0.004633 val_loss: 0.580554 val_acc: 0.747748 test_loss: 0.620170 test_acc: 0.705357
Epoch: 056 train_loss: 0.004632 val_loss: 0.588075 val_acc: 0.738739 test_loss: 0.626329 test_acc: 0.678571
Epoch: 057 train_loss: 0.004634 val_loss: 0.581896 val_acc: 0.738739 test_loss: 0.618649 test_acc: 0.714286
Epoch: 058 train_loss: 0.004629 val_loss: 0.587194 val_acc: 0.738739 test_loss: 0.623383 test_acc: 0.696429
Epoch: 059 train_loss: 0.004623 val_loss: 0.580318 val_acc: 0.738739 test_loss: 0.618347 test_acc: 0.714286
Epoch: 060 train_loss: 0.004617 val_loss: 0.580456 val_acc: 0.738739 test_loss: 0.619191 test_acc: 0.696429
Epoch: 061 train_loss: 0.004667 val_loss: 0.588712 val_acc: 0.738739 test_loss: 0.622634 test_acc: 0.678571
Epoch: 062 train_loss: 0.004622 val_loss: 0.580904 val_acc: 0.738739 test_loss: 0.618145 test_acc: 0.705357
Epoch: 063 train_loss: 0.004609 val_loss: 0.583070 val_acc: 0.720721 test_loss: 0.619548 test_acc: 0.696429
Epoch: 064 train_loss: 0.004605 val_loss: 0.584619 val_acc: 0.729730 test_loss: 0.618993 test_acc: 0.696429
Epoch: 065 train_loss: 0.004622 val_loss: 0.581405 val_acc: 0.729730 test_loss: 0.617391 test_acc: 0.705357
Epoch: 066 train_loss: 0.004628 val_loss: 0.582782 val_acc: 0.729730 test_loss: 0.616757 test_acc: 0.714286
Epoch: 067 train_loss: 0.004612 val_loss: 0.581471 val_acc: 0.738739 test_loss: 0.619072 test_acc: 0.714286
Epoch: 068 train_loss: 0.004599 val_loss: 0.584944 val_acc: 0.738739 test_loss: 0.619430 test_acc: 0.705357
Epoch: 069 train_loss: 0.004604 val_loss: 0.587376 val_acc: 0.729730 test_loss: 0.620983 test_acc: 0.714286
Epoch: 070 train_loss: 0.004624 val_loss: 0.583815 val_acc: 0.738739 test_loss: 0.620809 test_acc: 0.705357
Epoch: 071 train_loss: 0.004611 val_loss: 0.589114 val_acc: 0.738739 test_loss: 0.623059 test_acc: 0.687500
Epoch: 072 train_loss: 0.004592 val_loss: 0.585383 val_acc: 0.738739 test_loss: 0.619048 test_acc: 0.705357
Epoch: 073 train_loss: 0.004621 val_loss: 0.585753 val_acc: 0.729730 test_loss: 0.619867 test_acc: 0.705357
Epoch: 074 train_loss: 0.004618 val_loss: 0.587826 val_acc: 0.729730 test_loss: 0.621653 test_acc: 0.696429
Epoch: 075 train_loss: 0.004602 val_loss: 0.583598 val_acc: 0.729730 test_loss: 0.616222 test_acc: 0.705357
Epoch: 076 train_loss: 0.004591 val_loss: 0.587306 val_acc: 0.729730 test_loss: 0.619834 test_acc: 0.705357
Epoch: 077 train_loss: 0.004588 val_loss: 0.583546 val_acc: 0.729730 test_loss: 0.615264 test_acc: 0.714286
Epoch: 078 train_loss: 0.004579 val_loss: 0.582998 val_acc: 0.729730 test_loss: 0.615678 test_acc: 0.714286
Epoch: 079 train_loss: 0.004579 val_loss: 0.583618 val_acc: 0.729730 test_loss: 0.616670 test_acc: 0.714286
Epoch: 080 train_loss: 0.004571 val_loss: 0.582490 val_acc: 0.738739 test_loss: 0.615798 test_acc: 0.705357
Epoch: 081 train_loss: 0.004566 val_loss: 0.579915 val_acc: 0.756757 test_loss: 0.615312 test_acc: 0.732143
Epoch: 082 train_loss: 0.004568 val_loss: 0.582997 val_acc: 0.738739 test_loss: 0.613847 test_acc: 0.705357
Epoch: 083 train_loss: 0.004576 val_loss: 0.582742 val_acc: 0.738739 test_loss: 0.615028 test_acc: 0.714286
Epoch: 084 train_loss: 0.004601 val_loss: 0.585330 val_acc: 0.720721 test_loss: 0.620130 test_acc: 0.696429
Epoch: 085 train_loss: 0.004583 val_loss: 0.583349 val_acc: 0.738739 test_loss: 0.616159 test_acc: 0.714286
Epoch: 086 train_loss: 0.004597 val_loss: 0.584566 val_acc: 0.738739 test_loss: 0.618173 test_acc: 0.705357
Epoch: 087 train_loss: 0.004574 val_loss: 0.581186 val_acc: 0.738739 test_loss: 0.614375 test_acc: 0.714286
Epoch: 088 train_loss: 0.004606 val_loss: 0.595233 val_acc: 0.729730 test_loss: 0.624703 test_acc: 0.678571
Epoch: 089 train_loss: 0.004563 val_loss: 0.585158 val_acc: 0.747748 test_loss: 0.614609 test_acc: 0.723214
